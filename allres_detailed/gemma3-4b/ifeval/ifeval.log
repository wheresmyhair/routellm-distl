INFO 04-10 08:03:33 [__init__.py:239] Automatically detected platform cuda.
INFO:lm_eval.__main__:Selected Tasks: ['ifeval']
INFO:lm_eval.evaluator:Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
INFO:lm_eval.evaluator:Initializing hf model, with arguments: {'pretrained': 'google/gemma-3-4b-it'}
INFO:lm_eval.models.huggingface:Using device 'cuda'
INFO:lm_eval.models.huggingface:Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda'}
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.39s/it]
INFO:lm_eval.models.huggingface:Model type is 'gemma3', part of the Gemma family--a BOS token will be used as Gemma underperforms without it.
INFO:lm_eval.evaluator:ifeval: Using gen_kwargs: {'until': [], 'do_sample': False, 'temperature': 0.0, 'max_gen_toks': 1280}
INFO:lm_eval.evaluator:num_fewshot has been set to 0 for ifeval in its config. Manual configuration will be ignored.
INFO:lm_eval.api.task:Building contexts for ifeval on rank 0...
  0%|          | 0/541 [00:00<?, ?it/s]100%|██████████| 541/541 [00:00<00:00, 142041.84it/s]
INFO:lm_eval.evaluator:Running generate_until requests
Running generate_until requests:   0%|          | 0/541 [00:00<?, ?it/s]/home/yizhenjia/anaconda3/envs/lmeval/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/home/yizhenjia/anaconda3/envs/lmeval/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `64` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.
  warnings.warn(
Running generate_until requests:   0%|          | 1/541 [02:55<26:22:59, 175.89s/it]Running generate_until requests:  24%|██▍       | 129/541 [05:49<15:51,  2.31s/it]  Running generate_until requests:  48%|████▊     | 257/541 [08:43<08:17,  1.75s/it]Running generate_until requests:  71%|███████   | 385/541 [11:37<04:05,  1.57s/it]Running generate_until requests:  95%|█████████▍| 513/541 [12:34<00:31,  1.13s/it]Running generate_until requests: 100%|██████████| 541/541 [12:34<00:00,  1.39s/it]
INFO:lm_eval.loggers.evaluation_tracker:Saving results aggregated
INFO:lm_eval.loggers.evaluation_tracker:Saving per-sample results for: ifeval
hf (pretrained=google/gemma-3-4b-it), gen_kwargs: (None), limit: None, num_fewshot: 0, batch_size: 128
|Tasks |Version|Filter|n-shot|        Metric         |   |Value |   |Stderr|
|------|------:|------|-----:|-----------------------|---|-----:|---|------|
|ifeval|      4|none  |     0|inst_level_loose_acc   |↑  |0.8118|±  |   N/A|
|      |       |none  |     0|inst_level_strict_acc  |↑  |0.7890|±  |   N/A|
|      |       |none  |     0|prompt_level_loose_acc |↑  |0.7375|±  |0.0189|
|      |       |none  |     0|prompt_level_strict_acc|↑  |0.7043|±  |0.0196|

