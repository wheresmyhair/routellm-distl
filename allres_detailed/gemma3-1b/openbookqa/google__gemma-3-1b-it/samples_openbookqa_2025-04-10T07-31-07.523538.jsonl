{"doc_id": 0, "doc": {"id": "8-343", "question_stem": "A person wants to start saving money so that they can afford a nice vacation at the end of the year. After looking over their budget and expenses, they decide the best way to save money is to", "choices": {"text": ["make more phone calls", "quit eating lunch out", "buy less with monopoly money", "have lunch with friends"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "A person wants to start saving money so that they can afford a nice vacation at the end of the year. After looking over their budget and expenses, they decide the best way to save money is to", "arg_1": " make more phone calls"}, "gen_args_1": {"arg_0": "A person wants to start saving money so that they can afford a nice vacation at the end of the year. After looking over their budget and expenses, they decide the best way to save money is to", "arg_1": " quit eating lunch out"}, "gen_args_2": {"arg_0": "A person wants to start saving money so that they can afford a nice vacation at the end of the year. After looking over their budget and expenses, they decide the best way to save money is to", "arg_1": " buy less with monopoly money"}, "gen_args_3": {"arg_0": "A person wants to start saving money so that they can afford a nice vacation at the end of the year. After looking over their budget and expenses, they decide the best way to save money is to", "arg_1": " have lunch with friends"}}, "resps": [[["-22.25", "False"]], [["-22.5", "False"]], [["-37.75", "False"]], [["-18.75", "False"]]], "filtered_resps": [["-22.25", "False"], ["-22.5", "False"], ["-37.75", "False"], ["-18.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4c9fc70efbb1d461aa025fe71d375e360013eed565f6fdde72d0b6709106f461", "prompt_hash": "6cb709a6a1558e21bc5ae6848f22bf5e1760dd0059a1313c7f5f8383d17f16d6", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 1, "doc": {"id": "1129", "question_stem": "There is most likely going to be fog around:", "choices": {"text": ["a marsh", "a tundra", "the plains", "a desert"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "There is most likely going to be fog around:", "arg_1": " a marsh"}, "gen_args_1": {"arg_0": "There is most likely going to be fog around:", "arg_1": " a tundra"}, "gen_args_2": {"arg_0": "There is most likely going to be fog around:", "arg_1": " the plains"}, "gen_args_3": {"arg_0": "There is most likely going to be fog around:", "arg_1": " a desert"}}, "resps": [[["-17.625", "False"]], [["-21.75", "False"]], [["-15.375", "False"]], [["-16.875", "False"]]], "filtered_resps": [["-17.625", "False"], ["-21.75", "False"], ["-15.375", "False"], ["-16.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "9a2647c2c2842f84c11f012f8a2624ea1b981714f7a5621e71702c26b846f016", "prompt_hash": "3be7e0b59f57f7f169d9c7382bf9cf7ec652619fb078867977d7fd65a5e7dda6", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 2, "doc": {"id": "880", "question_stem": "Predators eat", "choices": {"text": ["lions", "humans", "bunnies", "grass"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Predators eat", "arg_1": " lions"}, "gen_args_1": {"arg_0": "Predators eat", "arg_1": " humans"}, "gen_args_2": {"arg_0": "Predators eat", "arg_1": " bunnies"}, "gen_args_3": {"arg_0": "Predators eat", "arg_1": " grass"}}, "resps": [[["-15.5", "False"]], [["-10.0", "False"]], [["-12.0", "False"]], [["-11.1875", "False"]]], "filtered_resps": [["-15.5", "False"], ["-10.0", "False"], ["-12.0", "False"], ["-11.1875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "66de709bcb7da2ec35e2b0258d2b97191e3ce98dd06e2c0f484caf392d0769c7", "prompt_hash": "d0144f20063dd5f2d92a4f6250b5a1344e3393232729b1e7da9eb9d5c5784607", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 3, "doc": {"id": "7-999", "question_stem": "Oak tree seeds are planted and a sidewalk is paved right next to that spot, until eventually, the tree is tall and the roots must extend past the sidewalk, which means", "choices": {"text": ["roots may be split", "roots may begin to die", "parts may break the concrete", "roots may fall apart"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Oak tree seeds are planted and a sidewalk is paved right next to that spot, until eventually, the tree is tall and the roots must extend past the sidewalk, which means", "arg_1": " roots may be split"}, "gen_args_1": {"arg_0": "Oak tree seeds are planted and a sidewalk is paved right next to that spot, until eventually, the tree is tall and the roots must extend past the sidewalk, which means", "arg_1": " roots may begin to die"}, "gen_args_2": {"arg_0": "Oak tree seeds are planted and a sidewalk is paved right next to that spot, until eventually, the tree is tall and the roots must extend past the sidewalk, which means", "arg_1": " parts may break the concrete"}, "gen_args_3": {"arg_0": "Oak tree seeds are planted and a sidewalk is paved right next to that spot, until eventually, the tree is tall and the roots must extend past the sidewalk, which means", "arg_1": " roots may fall apart"}}, "resps": [[["-22.75", "False"]], [["-29.75", "False"]], [["-35.5", "False"]], [["-30.875", "False"]]], "filtered_resps": [["-22.75", "False"], ["-29.75", "False"], ["-35.5", "False"], ["-30.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2e02d7a396eed9e093d6c40476cf41f803d08cce96ad59c9f1cff2c64ed995a4", "prompt_hash": "5f985a19a69d83e5bf883ee72c1cbbb98508c3156fb6edfb49d42108a70a4650", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 4, "doc": {"id": "8-464", "question_stem": "An electric car runs on electricity via", "choices": {"text": ["gasoline", "a power station", "electrical conductors", "fuel"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "An electric car runs on electricity via", "arg_1": " gasoline"}, "gen_args_1": {"arg_0": "An electric car runs on electricity via", "arg_1": " a power station"}, "gen_args_2": {"arg_0": "An electric car runs on electricity via", "arg_1": " electrical conductors"}, "gen_args_3": {"arg_0": "An electric car runs on electricity via", "arg_1": " fuel"}}, "resps": [[["-12.6875", "False"]], [["-15.0625", "False"]], [["-15.375", "False"]], [["-11.5", "False"]]], "filtered_resps": [["-12.6875", "False"], ["-15.0625", "False"], ["-15.375", "False"], ["-11.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e001c5baeda203a0a706baa6488b435f941037c9f0b2bce2a9146d2d2f585655", "prompt_hash": "ee536da028e9e43d39fdca8c316ff319e091c7b5a84568236b9de4b5daba0019", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 5, "doc": {"id": "9-794", "question_stem": "As the rain forest is deforested the atmosphere will increase with", "choices": {"text": ["oxygen", "nitrogen", "carbon", "rain"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "As the rain forest is deforested the atmosphere will increase with", "arg_1": " oxygen"}, "gen_args_1": {"arg_0": "As the rain forest is deforested the atmosphere will increase with", "arg_1": " nitrogen"}, "gen_args_2": {"arg_0": "As the rain forest is deforested the atmosphere will increase with", "arg_1": " carbon"}, "gen_args_3": {"arg_0": "As the rain forest is deforested the atmosphere will increase with", "arg_1": " rain"}}, "resps": [[["-5.6875", "False"]], [["-6.75", "False"]], [["-0.734375", "True"]], [["-8.125", "False"]]], "filtered_resps": [["-5.6875", "False"], ["-6.75", "False"], ["-0.734375", "True"], ["-8.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "c4712aa21393cc168b74e4418095b487427ada6cbfe09c340591d1466d041273", "prompt_hash": "b0e3386f87e0782d703af254dc5f990ade60c7051278e191123b9e162cbb1279", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 6, "doc": {"id": "9-1163", "question_stem": "an electric car contains a motor that runs on", "choices": {"text": ["gas", "hydrogen", "ions", "plutonium"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "an electric car contains a motor that runs on", "arg_1": " gas"}, "gen_args_1": {"arg_0": "an electric car contains a motor that runs on", "arg_1": " hydrogen"}, "gen_args_2": {"arg_0": "an electric car contains a motor that runs on", "arg_1": " ions"}, "gen_args_3": {"arg_0": "an electric car contains a motor that runs on", "arg_1": " plutonium"}}, "resps": [[["-9.0", "False"]], [["-7.875", "False"]], [["-14.0", "False"]], [["-13.0", "False"]]], "filtered_resps": [["-9.0", "False"], ["-7.875", "False"], ["-14.0", "False"], ["-13.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2e99b58e6c23918f08cf0842afeaf8c426563e6530933a07c8647cb06c1545a7", "prompt_hash": "837617ae6234c0f2d3fc4728393c38c6b26757699f393213212444bd09b6978c", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 7, "doc": {"id": "9-322", "question_stem": "The middle of the day usually involves the bright star nearest to the earth to be straight overhead why?", "choices": {"text": ["moons gravity", "human planet rotation", "global warming", "moon rotation"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "The middle of the day usually involves the bright star nearest to the earth to be straight overhead why?", "arg_1": " moons gravity"}, "gen_args_1": {"arg_0": "The middle of the day usually involves the bright star nearest to the earth to be straight overhead why?", "arg_1": " human planet rotation"}, "gen_args_2": {"arg_0": "The middle of the day usually involves the bright star nearest to the earth to be straight overhead why?", "arg_1": " global warming"}, "gen_args_3": {"arg_0": "The middle of the day usually involves the bright star nearest to the earth to be straight overhead why?", "arg_1": " moon rotation"}}, "resps": [[["-23.0", "False"]], [["-27.25", "False"]], [["-14.75", "False"]], [["-24.0", "False"]]], "filtered_resps": [["-23.0", "False"], ["-27.25", "False"], ["-14.75", "False"], ["-24.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a04d4329b4c4d0a615c07a56e5abea75382fb69fd5af0a1090342bead65ba8ff", "prompt_hash": "c4f7cf2b4f54bd52e449ecf3cb0499864c2250e185a6b01be19c7e64cbb3f02e", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 8, "doc": {"id": "7-1140", "question_stem": "The summer solstice in the northern hemisphere is four months before", "choices": {"text": ["May", "July", "April", "October"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "The summer solstice in the northern hemisphere is four months before", "arg_1": " May"}, "gen_args_1": {"arg_0": "The summer solstice in the northern hemisphere is four months before", "arg_1": " July"}, "gen_args_2": {"arg_0": "The summer solstice in the northern hemisphere is four months before", "arg_1": " April"}, "gen_args_3": {"arg_0": "The summer solstice in the northern hemisphere is four months before", "arg_1": " October"}}, "resps": [[["-8.0", "False"]], [["-5.21875", "False"]], [["-8.375", "False"]], [["-8.4375", "False"]]], "filtered_resps": [["-8.0", "False"], ["-5.21875", "False"], ["-8.375", "False"], ["-8.4375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "721314bf5a37cf991980da990d597b6275094d723ca4e95ba073f79b8b6f855b", "prompt_hash": "e8d2b7227a141f2c717d8b87f02442398dd0846b2deb740713ff405a3d476dd0", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 9, "doc": {"id": "7-903", "question_stem": "The main component in dirt is", "choices": {"text": ["microorganisms", "broken stones", "pollution", "bacteria"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "The main component in dirt is", "arg_1": " microorganisms"}, "gen_args_1": {"arg_0": "The main component in dirt is", "arg_1": " broken stones"}, "gen_args_2": {"arg_0": "The main component in dirt is", "arg_1": " pollution"}, "gen_args_3": {"arg_0": "The main component in dirt is", "arg_1": " bacteria"}}, "resps": [[["-5.78125", "False"]], [["-13.25", "False"]], [["-11.125", "False"]], [["-5.09375", "False"]]], "filtered_resps": [["-5.78125", "False"], ["-13.25", "False"], ["-11.125", "False"], ["-5.09375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4912151418d21ffae50599cbe80597551828988d0e31edde9d582d94fc2154c0", "prompt_hash": "2fd93f4a374d2ace55237eb5e27b7c41704366847c51858d90cf8b44746dd06d", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 10, "doc": {"id": "7-511", "question_stem": "It's easier for human's to survive in:", "choices": {"text": ["a cave", "the ocean.", "a town", "alone"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "It's easier for human's to survive in:", "arg_1": " a cave"}, "gen_args_1": {"arg_0": "It's easier for human's to survive in:", "arg_1": " the ocean."}, "gen_args_2": {"arg_0": "It's easier for human's to survive in:", "arg_1": " a town"}, "gen_args_3": {"arg_0": "It's easier for human's to survive in:", "arg_1": " alone"}}, "resps": [[["-10.1875", "False"]], [["-10.875", "False"]], [["-15.25", "False"]], [["-14.375", "False"]]], "filtered_resps": [["-10.1875", "False"], ["-10.875", "False"], ["-15.25", "False"], ["-14.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "0d8e4769613ca6e0c381e972055ff8ebe0af49352a987a71d2ed3bdce564ed34", "prompt_hash": "7916d3827eb19fc6864fcd17c80cbfa7f8fad0d80be0b82fc244b7e4b129c9c9", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 11, "doc": {"id": "9-937", "question_stem": "A cactus stem is used to store", "choices": {"text": ["fruit", "liquid", "food", "spines"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "A cactus stem is used to store", "arg_1": " fruit"}, "gen_args_1": {"arg_0": "A cactus stem is used to store", "arg_1": " liquid"}, "gen_args_2": {"arg_0": "A cactus stem is used to store", "arg_1": " food"}, "gen_args_3": {"arg_0": "A cactus stem is used to store", "arg_1": " spines"}}, "resps": [[["-9.375", "False"]], [["-5.6875", "False"]], [["-5.5", "False"]], [["-11.5", "False"]]], "filtered_resps": [["-9.375", "False"], ["-5.6875", "False"], ["-5.5", "False"], ["-11.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "082b181c99f868042e79566b575bcf6e8167f57bcca91c4f7c26b2ef0057bcee", "prompt_hash": "c18ac7b9e9e6346afeb3e6c58ebe33c5847d53c77756fe371e59b59edceec985", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 12, "doc": {"id": "8-201", "question_stem": "A red-tailed hawk is searching for prey. It is most likely to swoop down on", "choices": {"text": ["an eagle", "a cow", "a gecko", "a deer"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A red-tailed hawk is searching for prey. It is most likely to swoop down on", "arg_1": " an eagle"}, "gen_args_1": {"arg_0": "A red-tailed hawk is searching for prey. It is most likely to swoop down on", "arg_1": " a cow"}, "gen_args_2": {"arg_0": "A red-tailed hawk is searching for prey. It is most likely to swoop down on", "arg_1": " a gecko"}, "gen_args_3": {"arg_0": "A red-tailed hawk is searching for prey. It is most likely to swoop down on", "arg_1": " a deer"}}, "resps": [[["-8.8125", "False"]], [["-9.125", "False"]], [["-11.125", "False"]], [["-7.34375", "False"]]], "filtered_resps": [["-8.8125", "False"], ["-9.125", "False"], ["-11.125", "False"], ["-7.34375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "fda594043662da1091c6ff4da8034d49f4c601908d0685051e717d65c0dec208", "prompt_hash": "024b22eb94819a20faadba0189b85b98c8c14020e185398648ad813808fcad47", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 13, "doc": {"id": "1618", "question_stem": "The chance of wildfires is increased by", "choices": {"text": ["parched foliage", "torrential rain", "lush foliage", "careful fire maintenance"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "The chance of wildfires is increased by", "arg_1": " parched foliage"}, "gen_args_1": {"arg_0": "The chance of wildfires is increased by", "arg_1": " torrential rain"}, "gen_args_2": {"arg_0": "The chance of wildfires is increased by", "arg_1": " lush foliage"}, "gen_args_3": {"arg_0": "The chance of wildfires is increased by", "arg_1": " careful fire maintenance"}}, "resps": [[["-21.875", "False"]], [["-13.75", "False"]], [["-15.9375", "False"]], [["-21.25", "False"]]], "filtered_resps": [["-21.875", "False"], ["-13.75", "False"], ["-15.9375", "False"], ["-21.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "ed4894036a89f53368030986e0ca733b9095245ea0e640adf19aa94ec95b6b0a", "prompt_hash": "e6c942d9615ef5e7a5d9729cfdd0dcfc17764d076170e669374acb4c6ac1157d", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 14, "doc": {"id": "758", "question_stem": "A positive effect of burning biofuel is", "choices": {"text": ["shortage of crops for the food supply", "an increase in air pollution", "powering the lights in a home", "deforestation in the amazon to make room for crops"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A positive effect of burning biofuel is", "arg_1": " shortage of crops for the food supply"}, "gen_args_1": {"arg_0": "A positive effect of burning biofuel is", "arg_1": " an increase in air pollution"}, "gen_args_2": {"arg_0": "A positive effect of burning biofuel is", "arg_1": " powering the lights in a home"}, "gen_args_3": {"arg_0": "A positive effect of burning biofuel is", "arg_1": " deforestation in the amazon to make room for crops"}}, "resps": [[["-38.75", "False"]], [["-15.875", "False"]], [["-26.375", "False"]], [["-35.5", "False"]]], "filtered_resps": [["-38.75", "False"], ["-15.875", "False"], ["-26.375", "False"], ["-35.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "1ceca20139ab573033cdcca6dff004499954ecdcd5041cb0acf780f01c2aa7f4", "prompt_hash": "3fa8877188f30bd2a05325641d847519c009423851b200e219fe3503fc0143ee", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 15, "doc": {"id": "7-414", "question_stem": "As gasoline costs rise, alternative fuels are being used, which means that", "choices": {"text": ["wind power will be expensive", "gas costs will rise", "oil costs will be maintained", "gasoline will be needed less"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "As gasoline costs rise, alternative fuels are being used, which means that", "arg_1": " wind power will be expensive"}, "gen_args_1": {"arg_0": "As gasoline costs rise, alternative fuels are being used, which means that", "arg_1": " gas costs will rise"}, "gen_args_2": {"arg_0": "As gasoline costs rise, alternative fuels are being used, which means that", "arg_1": " oil costs will be maintained"}, "gen_args_3": {"arg_0": "As gasoline costs rise, alternative fuels are being used, which means that", "arg_1": " gasoline will be needed less"}}, "resps": [[["-22.0", "False"]], [["-18.875", "False"]], [["-27.5", "False"]], [["-17.375", "False"]]], "filtered_resps": [["-22.0", "False"], ["-18.875", "False"], ["-27.5", "False"], ["-17.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "5af56fef80c8d6765c23809513b692bb66c6f71d208d4292696029d6e69f00c0", "prompt_hash": "56176503f6096f8fea652701afe1e6b8f824c3d3da8b8c3128eb4314defc1a2a", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 16, "doc": {"id": "9-675", "question_stem": "A person wants to be able to have more natural power in their home. They choose to cease using a traditional electric company to source this electricity, and so decide to install", "choices": {"text": ["sun grafts", "sunlight shields", "panels collecting sunlight", "solar bees"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A person wants to be able to have more natural power in their home. They choose to cease using a traditional electric company to source this electricity, and so decide to install", "arg_1": " sun grafts"}, "gen_args_1": {"arg_0": "A person wants to be able to have more natural power in their home. They choose to cease using a traditional electric company to source this electricity, and so decide to install", "arg_1": " sunlight shields"}, "gen_args_2": {"arg_0": "A person wants to be able to have more natural power in their home. They choose to cease using a traditional electric company to source this electricity, and so decide to install", "arg_1": " panels collecting sunlight"}, "gen_args_3": {"arg_0": "A person wants to be able to have more natural power in their home. They choose to cease using a traditional electric company to source this electricity, and so decide to install", "arg_1": " solar bees"}}, "resps": [[["-25.0", "False"]], [["-23.75", "False"]], [["-19.375", "False"]], [["-25.125", "False"]]], "filtered_resps": [["-25.0", "False"], ["-23.75", "False"], ["-19.375", "False"], ["-25.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "5725dc5e88197727eb5f52226802552a48ca56edb8947ec8fbbffcbc66a3f873", "prompt_hash": "553b480dca817919ee73d66bf36ccf45b3736d9a9dd9f5a1ee1649360ef58d61", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 17, "doc": {"id": "9-163", "question_stem": "A Mola Mola might live where?", "choices": {"text": ["Lake Michigan", "The Mississippi River", "Bay of Bengal", "Lake Eerie"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A Mola Mola might live where?", "arg_1": " Lake Michigan"}, "gen_args_1": {"arg_0": "A Mola Mola might live where?", "arg_1": " The Mississippi River"}, "gen_args_2": {"arg_0": "A Mola Mola might live where?", "arg_1": " Bay of Bengal"}, "gen_args_3": {"arg_0": "A Mola Mola might live where?", "arg_1": " Lake Eerie"}}, "resps": [[["-15.0625", "False"]], [["-18.75", "False"]], [["-17.0", "False"]], [["-28.75", "False"]]], "filtered_resps": [["-15.0625", "False"], ["-18.75", "False"], ["-17.0", "False"], ["-28.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "cdf27f3e904931e744eb4fe7c1608f3057d9bf8e929fa8ae7afeae87ce3ca71a", "prompt_hash": "44a5eb1e4cc93b11e5a99e08dd0f36309d2c011e8f8995f44461589d9a99497f", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 18, "doc": {"id": "1032", "question_stem": "Which requires energy to move?", "choices": {"text": ["weasel", "willow", "mango", "poison ivy"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Which requires energy to move?", "arg_1": " weasel"}, "gen_args_1": {"arg_0": "Which requires energy to move?", "arg_1": " willow"}, "gen_args_2": {"arg_0": "Which requires energy to move?", "arg_1": " mango"}, "gen_args_3": {"arg_0": "Which requires energy to move?", "arg_1": " poison ivy"}}, "resps": [[["-20.0", "False"]], [["-19.875", "False"]], [["-21.125", "False"]], [["-22.75", "False"]]], "filtered_resps": [["-20.0", "False"], ["-19.875", "False"], ["-21.125", "False"], ["-22.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "72712908863192bec7e322c1fc3b1187233d37955032e83488df0723c9e0fbf8", "prompt_hash": "b0ea91b8d6b9cb54a2db788e7138e3c60690b4ac902550e07e2542486859b5b8", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 19, "doc": {"id": "889", "question_stem": "An animal that only eats plants is a", "choices": {"text": ["rat", "moth", "chimpanzee", "pig"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "An animal that only eats plants is a", "arg_1": " rat"}, "gen_args_1": {"arg_0": "An animal that only eats plants is a", "arg_1": " moth"}, "gen_args_2": {"arg_0": "An animal that only eats plants is a", "arg_1": " chimpanzee"}, "gen_args_3": {"arg_0": "An animal that only eats plants is a", "arg_1": " pig"}}, "resps": [[["-13.0", "False"]], [["-11.125", "False"]], [["-14.1875", "False"]], [["-9.75", "False"]]], "filtered_resps": [["-13.0", "False"], ["-11.125", "False"], ["-14.1875", "False"], ["-9.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "ad3d0d18c6c853fd13b5d9ea241240b3c6fe61f614bc07165711d0048cce5bb2", "prompt_hash": "58296ac8e913621b379802560fe01658a082187bf4ede3918310974674f5376d", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 20, "doc": {"id": "1160", "question_stem": "There was a lot more water vapor in the air when we went on a trip to", "choices": {"text": ["Hanoi", "Athens", "Baghdad", "Phoenix"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "There was a lot more water vapor in the air when we went on a trip to", "arg_1": " Hanoi"}, "gen_args_1": {"arg_0": "There was a lot more water vapor in the air when we went on a trip to", "arg_1": " Athens"}, "gen_args_2": {"arg_0": "There was a lot more water vapor in the air when we went on a trip to", "arg_1": " Baghdad"}, "gen_args_3": {"arg_0": "There was a lot more water vapor in the air when we went on a trip to", "arg_1": " Phoenix"}}, "resps": [[["-16.375", "False"]], [["-14.5625", "False"]], [["-19.0", "False"]], [["-10.875", "False"]]], "filtered_resps": [["-16.375", "False"], ["-14.5625", "False"], ["-19.0", "False"], ["-10.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4c44a77759a383edbcf814cb85e75da140e580c14e8cec22a9f185b1db4e87f1", "prompt_hash": "c0c2a2f64f52105f692898eafbd0fb7f733c1ccad63a1670cfdce8cf6a44c1af", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 21, "doc": {"id": "9-298", "question_stem": "An example of conservation is avoiding the use of", "choices": {"text": ["gasoline", "air", "snow", "clothes"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "An example of conservation is avoiding the use of", "arg_1": " gasoline"}, "gen_args_1": {"arg_0": "An example of conservation is avoiding the use of", "arg_1": " air"}, "gen_args_2": {"arg_0": "An example of conservation is avoiding the use of", "arg_1": " snow"}, "gen_args_3": {"arg_0": "An example of conservation is avoiding the use of", "arg_1": " clothes"}}, "resps": [[["-4.96875", "False"]], [["-7.71875", "False"]], [["-8.5625", "False"]], [["-9.75", "False"]]], "filtered_resps": [["-4.96875", "False"], ["-7.71875", "False"], ["-8.5625", "False"], ["-9.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e3ad4e416da5bd886f98bae3d9aa3c1adbb194cda3ce2a8538fadd86ea3396b7", "prompt_hash": "6f51d758e862ca9e68acb965d6542a49ccd0c5281b451f9d6e6322832fbce1f6", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 22, "doc": {"id": "1189", "question_stem": "What can feathers on Spheniscidae be used for?", "choices": {"text": ["keeping warm", "flying", "sleeping", "eating"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "What can feathers on Spheniscidae be used for?", "arg_1": " keeping warm"}, "gen_args_1": {"arg_0": "What can feathers on Spheniscidae be used for?", "arg_1": " flying"}, "gen_args_2": {"arg_0": "What can feathers on Spheniscidae be used for?", "arg_1": " sleeping"}, "gen_args_3": {"arg_0": "What can feathers on Spheniscidae be used for?", "arg_1": " eating"}}, "resps": [[["-17.125", "False"]], [["-17.625", "False"]], [["-23.75", "False"]], [["-20.625", "False"]]], "filtered_resps": [["-17.125", "False"], ["-17.625", "False"], ["-23.75", "False"], ["-20.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d83976940bf16fa05cc9ee8aa69c39434fa31808e9d52e3a1cb47bda8c7096b9", "prompt_hash": "14a72408dd955caf1c7fd29dae2201de09f04591a032839ab6667180a5012e6c", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 23, "doc": {"id": "8-395", "question_stem": "Overpopulation can cause", "choices": {"text": ["More fresh water for people to drink", "Lower Life Expectancy in Countries", "More food for more people", "More space for places to people to live"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Overpopulation can cause", "arg_1": " More fresh water for people to drink"}, "gen_args_1": {"arg_0": "Overpopulation can cause", "arg_1": " Lower Life Expectancy in Countries"}, "gen_args_2": {"arg_0": "Overpopulation can cause", "arg_1": " More food for more people"}, "gen_args_3": {"arg_0": "Overpopulation can cause", "arg_1": " More space for places to people to live"}}, "resps": [[["-45.75", "False"]], [["-32.75", "False"]], [["-35.0", "False"]], [["-51.0", "False"]]], "filtered_resps": [["-45.75", "False"], ["-32.75", "False"], ["-35.0", "False"], ["-51.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "6bc153b0b92b97138b8eaf28b82c30a97b3759d3a638027dfb662c78bf2c1cfb", "prompt_hash": "28a16ef26f56f2aae1cf227bb28e1cbbb5a7be635a7974678b56d0dceb8f9980", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 24, "doc": {"id": "7-238", "question_stem": "Shining a light through a diamond can", "choices": {"text": ["make a lot of bright lights shine", "summon a brilliant wave of color", "heat up a room", "make a lot of money"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Shining a light through a diamond can", "arg_1": " make a lot of bright lights shine"}, "gen_args_1": {"arg_0": "Shining a light through a diamond can", "arg_1": " summon a brilliant wave of color"}, "gen_args_2": {"arg_0": "Shining a light through a diamond can", "arg_1": " heat up a room"}, "gen_args_3": {"arg_0": "Shining a light through a diamond can", "arg_1": " make a lot of money"}}, "resps": [[["-39.0", "False"]], [["-26.125", "False"]], [["-18.75", "False"]], [["-19.5", "False"]]], "filtered_resps": [["-39.0", "False"], ["-26.125", "False"], ["-18.75", "False"], ["-19.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "b66f527848fc627f05d9e0f8663050220f25af6f71a41c3f96badea6b59e874d", "prompt_hash": "60e9ca51d052d3a6ba27aad9cbcc1aa7472dafb1ce5e265c1c18fc66c1142df0", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 25, "doc": {"id": "7-372", "question_stem": "If you were attacked by a shark and had to punch it sharply where it pulls in air from, you'd use your hand to make contact with", "choices": {"text": ["its snout", "its gills", "its nose", "its belly"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "If you were attacked by a shark and had to punch it sharply where it pulls in air from, you'd use your hand to make contact with", "arg_1": " its snout"}, "gen_args_1": {"arg_0": "If you were attacked by a shark and had to punch it sharply where it pulls in air from, you'd use your hand to make contact with", "arg_1": " its gills"}, "gen_args_2": {"arg_0": "If you were attacked by a shark and had to punch it sharply where it pulls in air from, you'd use your hand to make contact with", "arg_1": " its nose"}, "gen_args_3": {"arg_0": "If you were attacked by a shark and had to punch it sharply where it pulls in air from, you'd use your hand to make contact with", "arg_1": " its belly"}}, "resps": [[["-2.421875", "False"]], [["-5.25", "False"]], [["-6.0", "False"]], [["-8.625", "False"]]], "filtered_resps": [["-2.421875", "False"], ["-5.25", "False"], ["-6.0", "False"], ["-8.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "9df50e81ceb52055065fc40ac4353729feae15536aeb80cbb6366e6491b0c1bd", "prompt_hash": "a46b0aad8837ae94d3d4eaf4bb00f7214493fa8219fb5d2a15479a20fa10d488", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 26, "doc": {"id": "8-35", "question_stem": "which of these would stop a car quicker?", "choices": {"text": ["a wheel with wet brake pads", "a wheel without brake pads", "a wheel with worn brake pads", "a wheel with dry brake pads"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "which of these would stop a car quicker?", "arg_1": " a wheel with wet brake pads"}, "gen_args_1": {"arg_0": "which of these would stop a car quicker?", "arg_1": " a wheel without brake pads"}, "gen_args_2": {"arg_0": "which of these would stop a car quicker?", "arg_1": " a wheel with worn brake pads"}, "gen_args_3": {"arg_0": "which of these would stop a car quicker?", "arg_1": " a wheel with dry brake pads"}}, "resps": [[["-31.5", "False"]], [["-31.375", "False"]], [["-31.125", "False"]], [["-31.125", "False"]]], "filtered_resps": [["-31.5", "False"], ["-31.375", "False"], ["-31.125", "False"], ["-31.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "128f564b674301554319a149b06a3a87ec0df22a43c1365067641cbcadfb620e", "prompt_hash": "987e4e9bcd17d49488a877978f97765cf0e7492a18438aa4105a8209d7f00977", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 27, "doc": {"id": "9-271", "question_stem": "what system is needed for a body to get its needed supply of the gas humans breathe in?", "choices": {"text": ["the circulatory system", "the digestive system", "the school system", "central nervous system"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "what system is needed for a body to get its needed supply of the gas humans breathe in?", "arg_1": " the circulatory system"}, "gen_args_1": {"arg_0": "what system is needed for a body to get its needed supply of the gas humans breathe in?", "arg_1": " the digestive system"}, "gen_args_2": {"arg_0": "what system is needed for a body to get its needed supply of the gas humans breathe in?", "arg_1": " the school system"}, "gen_args_3": {"arg_0": "what system is needed for a body to get its needed supply of the gas humans breathe in?", "arg_1": " central nervous system"}}, "resps": [[["-14.875", "False"]], [["-16.75", "False"]], [["-25.25", "False"]], [["-17.625", "False"]]], "filtered_resps": [["-14.875", "False"], ["-16.75", "False"], ["-25.25", "False"], ["-17.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "7d4089dc1fc4719fe82466f8ceac8ab2256e994a609229b6e9d6555da861488c", "prompt_hash": "30cbb9fc7cf95ec116fe93b088ff644a71825516d6342fa2f51c4ebdc9e90128", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 28, "doc": {"id": "9-409", "question_stem": "Every evening a child can look into the night sky and see that the moon is", "choices": {"text": ["gone", "breaking", "falling", "moving upwards"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Every evening a child can look into the night sky and see that the moon is", "arg_1": " gone"}, "gen_args_1": {"arg_0": "Every evening a child can look into the night sky and see that the moon is", "arg_1": " breaking"}, "gen_args_2": {"arg_0": "Every evening a child can look into the night sky and see that the moon is", "arg_1": " falling"}, "gen_args_3": {"arg_0": "Every evening a child can look into the night sky and see that the moon is", "arg_1": " moving upwards"}}, "resps": [[["-6.90625", "False"]], [["-9.4375", "False"]], [["-5.53125", "False"]], [["-11.125", "False"]]], "filtered_resps": [["-6.90625", "False"], ["-9.4375", "False"], ["-5.53125", "False"], ["-11.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "8ee58de0c198bec925d6c048417d80f412bce4a492fa932e25c3be49553ee296", "prompt_hash": "60e93b79e1850f23207f415771a436cd3736245e80c7e13bd09161848ab7e5f2", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 29, "doc": {"id": "530", "question_stem": "When it's flying, a plane has no friction with the", "choices": {"text": ["wings", "ground", "air", "clouds"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "When it's flying, a plane has no friction with the", "arg_1": " wings"}, "gen_args_1": {"arg_0": "When it's flying, a plane has no friction with the", "arg_1": " ground"}, "gen_args_2": {"arg_0": "When it's flying, a plane has no friction with the", "arg_1": " air"}, "gen_args_3": {"arg_0": "When it's flying, a plane has no friction with the", "arg_1": " clouds"}}, "resps": [[["-10.375", "False"]], [["-0.2578125", "True"]], [["-1.7578125", "False"]], [["-9.75", "False"]]], "filtered_resps": [["-10.375", "False"], ["-0.2578125", "True"], ["-1.7578125", "False"], ["-9.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "68138bd10bd7400d58e59e10542444f836172cb3a5cb0c368c08c0497a4acfdc", "prompt_hash": "39005b832246704b540db9ca609ec39ad90d029eb26a9e6100058b3fb1d2bd26", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 30, "doc": {"id": "1426", "question_stem": "To grow plants require", "choices": {"text": ["acid rain", "pesticides", "shafts of sunlight", "moonbeam rays"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "To grow plants require", "arg_1": " acid rain"}, "gen_args_1": {"arg_0": "To grow plants require", "arg_1": " pesticides"}, "gen_args_2": {"arg_0": "To grow plants require", "arg_1": " shafts of sunlight"}, "gen_args_3": {"arg_0": "To grow plants require", "arg_1": " moonbeam rays"}}, "resps": [[["-12.9375", "False"]], [["-12.125", "False"]], [["-18.375", "False"]], [["-27.875", "False"]]], "filtered_resps": [["-12.9375", "False"], ["-12.125", "False"], ["-18.375", "False"], ["-27.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "b86d2867b52e96d187dad546e62da2031ac65122f5a079ef9dbfa22d8d735e17", "prompt_hash": "e74bc13ed7b0db1f43580ac8efcf4b441a10f53eb06db26e6649d956936407b2", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 31, "doc": {"id": "8-466", "question_stem": "What is the best way to guess a babies eye color?", "choices": {"text": ["The surroundings they are born in.", "Their parents usual diet.", "Just take a random guess.", "The genealogy records of their family."], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "What is the best way to guess a babies eye color?", "arg_1": " The surroundings they are born in."}, "gen_args_1": {"arg_0": "What is the best way to guess a babies eye color?", "arg_1": " Their parents usual diet."}, "gen_args_2": {"arg_0": "What is the best way to guess a babies eye color?", "arg_1": " Just take a random guess."}, "gen_args_3": {"arg_0": "What is the best way to guess a babies eye color?", "arg_1": " The genealogy records of their family."}}, "resps": [[["-37.0", "False"]], [["-44.75", "False"]], [["-26.375", "False"]], [["-44.5", "False"]]], "filtered_resps": [["-37.0", "False"], ["-44.75", "False"], ["-26.375", "False"], ["-44.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f0d788dafbec625151f89d78843de393f7cb8c613239c39a526c833930ce1012", "prompt_hash": "26fc8292b82300a38ec61684a14054e6ee76abc26b0c7cbe07e78ce2792ebf55", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 32, "doc": {"id": "1577", "question_stem": "What animal eats plants?", "choices": {"text": ["eagles", "robins", "owls", "leopards"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "What animal eats plants?", "arg_1": " eagles"}, "gen_args_1": {"arg_0": "What animal eats plants?", "arg_1": " robins"}, "gen_args_2": {"arg_0": "What animal eats plants?", "arg_1": " owls"}, "gen_args_3": {"arg_0": "What animal eats plants?", "arg_1": " leopards"}}, "resps": [[["-20.5", "False"]], [["-19.75", "False"]], [["-18.625", "False"]], [["-23.5", "False"]]], "filtered_resps": [["-20.5", "False"], ["-19.75", "False"], ["-18.625", "False"], ["-23.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "bb8a8432cc784fdda488eb08631c3536ef39ee6e36c8a482a1eb1c37234852e0", "prompt_hash": "e0ab23edc4ee20e13aea2f751a17df63df2a592e241311ac1611c3e6fa8bb927", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 33, "doc": {"id": "8-257", "question_stem": "Which of these is a hypothesis?", "choices": {"text": ["The ice caps will completely melt if global warming continues", "The earth is round", "The earth revolves around the sun", "Gravity causes objects to fall"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Which of these is a hypothesis?", "arg_1": " The ice caps will completely melt if global warming continues"}, "gen_args_1": {"arg_0": "Which of these is a hypothesis?", "arg_1": " The earth is round"}, "gen_args_2": {"arg_0": "Which of these is a hypothesis?", "arg_1": " The earth revolves around the sun"}, "gen_args_3": {"arg_0": "Which of these is a hypothesis?", "arg_1": " Gravity causes objects to fall"}}, "resps": [[["-59.25", "False"]], [["-26.875", "False"]], [["-30.75", "False"]], [["-28.875", "False"]]], "filtered_resps": [["-59.25", "False"], ["-26.875", "False"], ["-30.75", "False"], ["-28.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e47a2c2272dd87257c7cb9aed7b154312bc035ed9228b1283f1447ad9a2df923", "prompt_hash": "39680859b68c1d102a6607bd3742949f88a2311aea5bcbfb269a01637f87b9de", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 34, "doc": {"id": "378", "question_stem": "What explains the characteristic lunar formations?", "choices": {"text": ["remains of ancient ponds", "many collisions that have occured", "volcanic explosions over millions of years", "sink holes due to the moons porous nature"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "What explains the characteristic lunar formations?", "arg_1": " remains of ancient ponds"}, "gen_args_1": {"arg_0": "What explains the characteristic lunar formations?", "arg_1": " many collisions that have occured"}, "gen_args_2": {"arg_0": "What explains the characteristic lunar formations?", "arg_1": " volcanic explosions over millions of years"}, "gen_args_3": {"arg_0": "What explains the characteristic lunar formations?", "arg_1": " sink holes due to the moons porous nature"}}, "resps": [[["-36.75", "False"]], [["-37.75", "False"]], [["-35.5", "False"]], [["-67.0", "False"]]], "filtered_resps": [["-36.75", "False"], ["-37.75", "False"], ["-35.5", "False"], ["-67.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "34d17cc3da666b1b6c521802dac6924c22100f3e98a4c8002253eb2034aeb4c7", "prompt_hash": "424f82bda19bff0fc3a051675ff8f8e437e630189ca7a046f0c6a48b00cbd5c1", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 35, "doc": {"id": "8-41", "question_stem": "Tadpoles start their lives as", "choices": {"text": ["Water animals", "Frogs", "Ants", "College Students"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Tadpoles start their lives as", "arg_1": " Water animals"}, "gen_args_1": {"arg_0": "Tadpoles start their lives as", "arg_1": " Frogs"}, "gen_args_2": {"arg_0": "Tadpoles start their lives as", "arg_1": " Ants"}, "gen_args_3": {"arg_0": "Tadpoles start their lives as", "arg_1": " College Students"}}, "resps": [[["-27.0", "False"]], [["-13.1875", "False"]], [["-19.875", "False"]], [["-23.0", "False"]]], "filtered_resps": [["-27.0", "False"], ["-13.1875", "False"], ["-19.875", "False"], ["-23.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2b846f1d009a3d9de490f6d4e23f9642e3f079f73975fa8d3f6b25c2834b344a", "prompt_hash": "e6937d25c8a8ea2561e218417d8c4c7988f88ff9e5f6d50c62716b8ff822c17c", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 36, "doc": {"id": "9-540", "question_stem": "If a person puts out four apples around their home on the same day, the molecules in which apple would be moving the most rapidly?", "choices": {"text": ["the apple sitting on a sunny sidewalk", "the apple in the freezer", "the apple sitting on the shaded stoop", "the apple in a closet"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "If a person puts out four apples around their home on the same day, the molecules in which apple would be moving the most rapidly?", "arg_1": " the apple sitting on a sunny sidewalk"}, "gen_args_1": {"arg_0": "If a person puts out four apples around their home on the same day, the molecules in which apple would be moving the most rapidly?", "arg_1": " the apple in the freezer"}, "gen_args_2": {"arg_0": "If a person puts out four apples around their home on the same day, the molecules in which apple would be moving the most rapidly?", "arg_1": " the apple sitting on the shaded stoop"}, "gen_args_3": {"arg_0": "If a person puts out four apples around their home on the same day, the molecules in which apple would be moving the most rapidly?", "arg_1": " the apple in a closet"}}, "resps": [[["-42.25", "False"]], [["-28.125", "False"]], [["-46.5", "False"]], [["-30.625", "False"]]], "filtered_resps": [["-42.25", "False"], ["-28.125", "False"], ["-46.5", "False"], ["-30.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "8fad0e2df47671de925695afbc1552faedbfcbc24c2be73a305d8d577635e470", "prompt_hash": "37aa6b5e275f07bf189160a1db2d11860bb83d810ffe8e4c01b6fee0f2859d51", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 37, "doc": {"id": "266", "question_stem": "What is used for sensing visual things?", "choices": {"text": ["nerves", "tibia", "nostril", "cornea"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "What is used for sensing visual things?", "arg_1": " nerves"}, "gen_args_1": {"arg_0": "What is used for sensing visual things?", "arg_1": " tibia"}, "gen_args_2": {"arg_0": "What is used for sensing visual things?", "arg_1": " nostril"}, "gen_args_3": {"arg_0": "What is used for sensing visual things?", "arg_1": " cornea"}}, "resps": [[["-19.625", "False"]], [["-23.75", "False"]], [["-23.75", "False"]], [["-16.75", "False"]]], "filtered_resps": [["-19.625", "False"], ["-23.75", "False"], ["-23.75", "False"], ["-16.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "8dd5f3bb6e079819a09fa00a26a91c04f144e516d3e02936a7219c6969830886", "prompt_hash": "c3fcb7150b07edc7d2ddb1df9e1cefbe5989ff770d6c2e0dd1994b92fb3e713e", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 38, "doc": {"id": "1309", "question_stem": "They studied the soil by using", "choices": {"text": ["plants", "a telescope", "roots", "a microscope"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "They studied the soil by using", "arg_1": " plants"}, "gen_args_1": {"arg_0": "They studied the soil by using", "arg_1": " a telescope"}, "gen_args_2": {"arg_0": "They studied the soil by using", "arg_1": " roots"}, "gen_args_3": {"arg_0": "They studied the soil by using", "arg_1": " a microscope"}}, "resps": [[["-9.75", "False"]], [["-8.5625", "False"]], [["-11.8125", "False"]], [["-3.921875", "False"]]], "filtered_resps": [["-9.75", "False"], ["-8.5625", "False"], ["-11.8125", "False"], ["-3.921875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2137735faee62d641a6e84c614cc84330657923068aef654f442b1ce73c8cf18", "prompt_hash": "826fa7d0eb97678e21f7bd4299cce2c9cd01fa1a023ffc2f4292a4a0f674bc0b", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 39, "doc": {"id": "7-1197", "question_stem": "Bill's arm got cold when he put it inside the", "choices": {"text": ["refrigerator", "room", "jacket", "oven"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Bill's arm got cold when he put it inside the", "arg_1": " refrigerator"}, "gen_args_1": {"arg_0": "Bill's arm got cold when he put it inside the", "arg_1": " room"}, "gen_args_2": {"arg_0": "Bill's arm got cold when he put it inside the", "arg_1": " jacket"}, "gen_args_3": {"arg_0": "Bill's arm got cold when he put it inside the", "arg_1": " oven"}}, "resps": [[["-3.765625", "False"]], [["-6.8125", "False"]], [["-3.078125", "False"]], [["-2.265625", "False"]]], "filtered_resps": [["-3.765625", "False"], ["-6.8125", "False"], ["-3.078125", "False"], ["-2.265625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f7ccbca38abeb66623f64fd9e308a216ac38000412e9f1040e84c6052ba6578e", "prompt_hash": "37768be773485204e28f9e9c8d5eb73eb5633bfd4cc44806f644ca0408c7a2f6", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 40, "doc": {"id": "7-891", "question_stem": "A recyclable material can be", "choices": {"text": ["transformed", "traded", "thrown away", "used more times"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "A recyclable material can be", "arg_1": " transformed"}, "gen_args_1": {"arg_0": "A recyclable material can be", "arg_1": " traded"}, "gen_args_2": {"arg_0": "A recyclable material can be", "arg_1": " thrown away"}, "gen_args_3": {"arg_0": "A recyclable material can be", "arg_1": " used more times"}}, "resps": [[["-2.0", "False"]], [["-7.6875", "False"]], [["-7.9375", "False"]], [["-16.625", "False"]]], "filtered_resps": [["-2.0", "False"], ["-7.6875", "False"], ["-7.9375", "False"], ["-16.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "30f2d03aef8c3a911770d7b9905790f7f97bc2b169fc8015242c42a511915423", "prompt_hash": "308d04682ddc43d5f859e24877b4ec75c8f23e32e8e8ff9587085a5890941322", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 41, "doc": {"id": "1180", "question_stem": "What is different about birth in humans and chickens?", "choices": {"text": ["Mother", "Fertilization", "Father", "the hard shell"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "What is different about birth in humans and chickens?", "arg_1": " Mother"}, "gen_args_1": {"arg_0": "What is different about birth in humans and chickens?", "arg_1": " Fertilization"}, "gen_args_2": {"arg_0": "What is different about birth in humans and chickens?", "arg_1": " Father"}, "gen_args_3": {"arg_0": "What is different about birth in humans and chickens?", "arg_1": " the hard shell"}}, "resps": [[["-15.875", "False"]], [["-11.25", "False"]], [["-17.5", "False"]], [["-23.25", "False"]]], "filtered_resps": [["-15.875", "False"], ["-11.25", "False"], ["-17.5", "False"], ["-23.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "74411967dee6d3ca7723fa4847e08d8c376a03fdc59569fa5409fae75dd92cce", "prompt_hash": "17095ccbff961693ba18fdddec7b2fe59341a5cd53cb639a1e94a452925ddd68", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 42, "doc": {"id": "1204", "question_stem": "Which of these situations is an example of pollutants?", "choices": {"text": ["plastic bags floating in the ocean", "mallard ducks floating on a lake", "cottonwood seeds floating in the air", "cirrus clouds floating in the sky"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Which of these situations is an example of pollutants?", "arg_1": " plastic bags floating in the ocean"}, "gen_args_1": {"arg_0": "Which of these situations is an example of pollutants?", "arg_1": " mallard ducks floating on a lake"}, "gen_args_2": {"arg_0": "Which of these situations is an example of pollutants?", "arg_1": " cottonwood seeds floating in the air"}, "gen_args_3": {"arg_0": "Which of these situations is an example of pollutants?", "arg_1": " cirrus clouds floating in the sky"}}, "resps": [[["-29.625", "False"]], [["-47.0", "False"]], [["-46.0", "False"]], [["-32.0", "False"]]], "filtered_resps": [["-29.625", "False"], ["-47.0", "False"], ["-46.0", "False"], ["-32.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "b84b895958855836c19e8fa1b164628b63b0f5aa351598891326fe55fe0b4b1b", "prompt_hash": "51f906fda30504de5fc5ffbefe83bb60d069b324bd5433191e021a30726c960a", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 43, "doc": {"id": "7-52", "question_stem": "Human reproduction requires", "choices": {"text": ["eggs with shells", "nest incubation", "a nest", "a womb"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Human reproduction requires", "arg_1": " eggs with shells"}, "gen_args_1": {"arg_0": "Human reproduction requires", "arg_1": " nest incubation"}, "gen_args_2": {"arg_0": "Human reproduction requires", "arg_1": " a nest"}, "gen_args_3": {"arg_0": "Human reproduction requires", "arg_1": " a womb"}}, "resps": [[["-36.25", "False"]], [["-20.75", "False"]], [["-13.75", "False"]], [["-14.6875", "False"]]], "filtered_resps": [["-36.25", "False"], ["-20.75", "False"], ["-13.75", "False"], ["-14.6875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "757e6f1b716d27dc4aab31cec60cc617a5976bc6032547be5f9581458e60d686", "prompt_hash": "c07aa39b13c1ddb4f7bb8d3d873b9e9de70465aa83e85559fbdb0207dd66215d", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 44, "doc": {"id": "1759", "question_stem": "Thermometers", "choices": {"text": ["can help you monitor a fever", "indicate levels of mercury in the blood", "read exactly at 98.6 degrees", "are used only for babies"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Thermometers", "arg_1": " can help you monitor a fever"}, "gen_args_1": {"arg_0": "Thermometers", "arg_1": " indicate levels of mercury in the blood"}, "gen_args_2": {"arg_0": "Thermometers", "arg_1": " read exactly at 98.6 degrees"}, "gen_args_3": {"arg_0": "Thermometers", "arg_1": " are used only for babies"}}, "resps": [[["-24.5", "False"]], [["-29.625", "False"]], [["-35.0", "False"]], [["-30.25", "False"]]], "filtered_resps": [["-24.5", "False"], ["-29.625", "False"], ["-35.0", "False"], ["-30.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "6b1c8ce0d97e387e0e5bf0cead0b8d2bed04ef5c0004c17439428059d81d05ad", "prompt_hash": "1e36111d82775381bc0ba148053a1f5ea561c650977cccd911a65ced88cc5ed1", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 45, "doc": {"id": "9-655", "question_stem": "if the earth was a living room, what can be done to melt the glaciers?", "choices": {"text": ["someone would turn up the room heater", "someone would turn up the air conditioner", "someone would turn up the music", "someone would turn on the light"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "if the earth was a living room, what can be done to melt the glaciers?", "arg_1": " someone would turn up the room heater"}, "gen_args_1": {"arg_0": "if the earth was a living room, what can be done to melt the glaciers?", "arg_1": " someone would turn up the air conditioner"}, "gen_args_2": {"arg_0": "if the earth was a living room, what can be done to melt the glaciers?", "arg_1": " someone would turn up the music"}, "gen_args_3": {"arg_0": "if the earth was a living room, what can be done to melt the glaciers?", "arg_1": " someone would turn on the light"}}, "resps": [[["-47.0", "False"]], [["-43.5", "False"]], [["-44.0", "False"]], [["-37.25", "False"]]], "filtered_resps": [["-47.0", "False"], ["-43.5", "False"], ["-44.0", "False"], ["-37.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "48ed5cfcae15ebe67b034885be372784aaae121d1987951ee9539e2689768aab", "prompt_hash": "1c1deffcac2b1a4d13a06b0478792d4b6ede2b3535d3af57cdc7f76a606a36c0", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 46, "doc": {"id": "132", "question_stem": "What would happen when balloons heat up?", "choices": {"text": ["they get bigger", "they get smaller", "nothing happens", "they fall down"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "What would happen when balloons heat up?", "arg_1": " they get bigger"}, "gen_args_1": {"arg_0": "What would happen when balloons heat up?", "arg_1": " they get smaller"}, "gen_args_2": {"arg_0": "What would happen when balloons heat up?", "arg_1": " nothing happens"}, "gen_args_3": {"arg_0": "What would happen when balloons heat up?", "arg_1": " they fall down"}}, "resps": [[["-17.75", "False"]], [["-18.25", "False"]], [["-22.875", "False"]], [["-22.125", "False"]]], "filtered_resps": [["-17.75", "False"], ["-18.25", "False"], ["-22.875", "False"], ["-22.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "0c4d80cbbf4bb6d19a28bb9a716f951e1a077430355289b11da4b061b397ba7a", "prompt_hash": "1fc4a09abf40540957277abf9d7d334ab1152a430971d3eb58f74a3fbb39e79b", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 47, "doc": {"id": "8-79", "question_stem": "A balloon is filled with helium for a party. After the party, the balloons are left in the living room, where a fireplace is heating the room. The balloons", "choices": {"text": ["expand", "melt", "shrink", "fall"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "A balloon is filled with helium for a party. After the party, the balloons are left in the living room, where a fireplace is heating the room. The balloons", "arg_1": " expand"}, "gen_args_1": {"arg_0": "A balloon is filled with helium for a party. After the party, the balloons are left in the living room, where a fireplace is heating the room. The balloons", "arg_1": " melt"}, "gen_args_2": {"arg_0": "A balloon is filled with helium for a party. After the party, the balloons are left in the living room, where a fireplace is heating the room. The balloons", "arg_1": " shrink"}, "gen_args_3": {"arg_0": "A balloon is filled with helium for a party. After the party, the balloons are left in the living room, where a fireplace is heating the room. The balloons", "arg_1": " fall"}}, "resps": [[["-6.78125", "False"]], [["-11.125", "False"]], [["-10.375", "False"]], [["-10.5625", "False"]]], "filtered_resps": [["-6.78125", "False"], ["-11.125", "False"], ["-10.375", "False"], ["-10.5625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "65c0b850ce5fa2e888f07d1f92022ea69db582b48a86d9fc297f604280a4e366", "prompt_hash": "ae8a61f5b0fb9391d377e57d541cc66e51abb7ce4a8d788055d18da5596e4194", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 48, "doc": {"id": "1835", "question_stem": "Seals are most likely to be found in what type of environment?", "choices": {"text": ["desert", "arctic", "Mediterranean", "tropical"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Seals are most likely to be found in what type of environment?", "arg_1": " desert"}, "gen_args_1": {"arg_0": "Seals are most likely to be found in what type of environment?", "arg_1": " arctic"}, "gen_args_2": {"arg_0": "Seals are most likely to be found in what type of environment?", "arg_1": " Mediterranean"}, "gen_args_3": {"arg_0": "Seals are most likely to be found in what type of environment?", "arg_1": " tropical"}}, "resps": [[["-21.0", "False"]], [["-19.625", "False"]], [["-22.75", "False"]], [["-23.5", "False"]]], "filtered_resps": [["-21.0", "False"], ["-19.625", "False"], ["-22.75", "False"], ["-23.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "3f3b529e8b2bb7c6017db7b777c193e00c23576061b81820cb7df6929a33eb98", "prompt_hash": "57ed1eba9dc5209e79c9f7d254e5284a99bd281a5a559ba8cd395a0ba788b118", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 49, "doc": {"id": "9-149", "question_stem": "When the eggs hatch, the offspring are", "choices": {"text": ["killed", "hurt", "born", "cold"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "When the eggs hatch, the offspring are", "arg_1": " killed"}, "gen_args_1": {"arg_0": "When the eggs hatch, the offspring are", "arg_1": " hurt"}, "gen_args_2": {"arg_0": "When the eggs hatch, the offspring are", "arg_1": " born"}, "gen_args_3": {"arg_0": "When the eggs hatch, the offspring are", "arg_1": " cold"}}, "resps": [[["-10.0", "False"]], [["-11.875", "False"]], [["-1.3046875", "True"]], [["-8.4375", "False"]]], "filtered_resps": [["-10.0", "False"], ["-11.875", "False"], ["-1.3046875", "True"], ["-8.4375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "0714d18f0edc9a8fe172e502462a3ac7d58cd15b1e42605e6f7b2b07e58bdce6", "prompt_hash": "a2f69eb17bfe0cf39a426e5c1e13044092ee2c608761ce5b1ab98b0008b1ad98", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 50, "doc": {"id": "695", "question_stem": "Some berries may be eaten by", "choices": {"text": ["a bear or person", "a bear or shark", "a bear or lion", "a bear or wolf"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Some berries may be eaten by", "arg_1": " a bear or person"}, "gen_args_1": {"arg_0": "Some berries may be eaten by", "arg_1": " a bear or shark"}, "gen_args_2": {"arg_0": "Some berries may be eaten by", "arg_1": " a bear or lion"}, "gen_args_3": {"arg_0": "Some berries may be eaten by", "arg_1": " a bear or wolf"}}, "resps": [[["-24.0", "False"]], [["-27.375", "False"]], [["-23.625", "False"]], [["-18.0", "False"]]], "filtered_resps": [["-24.0", "False"], ["-27.375", "False"], ["-23.625", "False"], ["-18.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f1201364f14743b6de56ca369991c15da2d6baea218ca6eef82ac17fbf369c5e", "prompt_hash": "dab4f95dceaf334c1eb681c36ab9a4463b4df6a028922a2fd9cc5bb8656b61ca", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 51, "doc": {"id": "8-179", "question_stem": "A person has a chance to experience an equinox", "choices": {"text": ["weekly", "monthly", "annually", "biannually"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "A person has a chance to experience an equinox", "arg_1": " weekly"}, "gen_args_1": {"arg_0": "A person has a chance to experience an equinox", "arg_1": " monthly"}, "gen_args_2": {"arg_0": "A person has a chance to experience an equinox", "arg_1": " annually"}, "gen_args_3": {"arg_0": "A person has a chance to experience an equinox", "arg_1": " biannually"}}, "resps": [[["-14.5", "False"]], [["-12.8125", "False"]], [["-12.5", "False"]], [["-18.625", "False"]]], "filtered_resps": [["-14.5", "False"], ["-12.8125", "False"], ["-12.5", "False"], ["-18.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2e229dd028e1032189441150bc05a79e144387d8c45cde2b7eebd9d1669f9f4c", "prompt_hash": "a53429868590c53a9aa720fdafd9ac9c22c8bfb5ec9ddc81b4c10560ef15326b", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 52, "doc": {"id": "7-50", "question_stem": "Overpopulation of an organism can", "choices": {"text": ["strain the resources of an ecosystem", "cause boundless growth of resources", "lead to extinction of the organism", "cause the ecosystem to flourish"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Overpopulation of an organism can", "arg_1": " strain the resources of an ecosystem"}, "gen_args_1": {"arg_0": "Overpopulation of an organism can", "arg_1": " cause boundless growth of resources"}, "gen_args_2": {"arg_0": "Overpopulation of an organism can", "arg_1": " lead to extinction of the organism"}, "gen_args_3": {"arg_0": "Overpopulation of an organism can", "arg_1": " cause the ecosystem to flourish"}}, "resps": [[["-10.875", "False"]], [["-26.5", "False"]], [["-12.0625", "False"]], [["-18.875", "False"]]], "filtered_resps": [["-10.875", "False"], ["-26.5", "False"], ["-12.0625", "False"], ["-18.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "fc1b39913517989df4850f66bc7257e88c883068081fc3106d6fee73af3631b6", "prompt_hash": "c1928267a8162cd2b0d24e98065a097674729e7c46f138c2f51e993e2cf82e8e", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 53, "doc": {"id": "508", "question_stem": "To improve health, what is a good strategy?", "choices": {"text": ["high risk lifestyle", "restaurant food", "business trip", "a spa trip"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "To improve health, what is a good strategy?", "arg_1": " high risk lifestyle"}, "gen_args_1": {"arg_0": "To improve health, what is a good strategy?", "arg_1": " restaurant food"}, "gen_args_2": {"arg_0": "To improve health, what is a good strategy?", "arg_1": " business trip"}, "gen_args_3": {"arg_0": "To improve health, what is a good strategy?", "arg_1": " a spa trip"}}, "resps": [[["-28.875", "False"]], [["-29.375", "False"]], [["-35.0", "False"]], [["-32.0", "False"]]], "filtered_resps": [["-28.875", "False"], ["-29.375", "False"], ["-35.0", "False"], ["-32.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2d6dc79706a6307963822d67b70e1bd8433c704b01a3d5606a04c9b36d67546a", "prompt_hash": "7ae20197ec1320e79a3f65bbde33559d31ef2203d2a2582de8e8e00d1b7f7539", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 54, "doc": {"id": "1674", "question_stem": "A girl and her mom have the same", "choices": {"text": ["date of birth", "shirt", "number of toenails", "hair length"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A girl and her mom have the same", "arg_1": " date of birth"}, "gen_args_1": {"arg_0": "A girl and her mom have the same", "arg_1": " shirt"}, "gen_args_2": {"arg_0": "A girl and her mom have the same", "arg_1": " number of toenails"}, "gen_args_3": {"arg_0": "A girl and her mom have the same", "arg_1": " hair length"}}, "resps": [[["-7.15625", "False"]], [["-7.90625", "False"]], [["-15.4375", "False"]], [["-15.5", "False"]]], "filtered_resps": [["-7.15625", "False"], ["-7.90625", "False"], ["-15.4375", "False"], ["-15.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "bacdeb354cf6adca1824c740216cafc12c0f1ab5fd94de3a0b6dd6cf4ce5e967", "prompt_hash": "14e0aabcdfcc99f638943a662ed40c121b600732fbe2800dae85c11af19deaae", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 55, "doc": {"id": "163", "question_stem": "The transportation with the most mass is likely a", "choices": {"text": ["commercial plane", "private plane", "bus", "private car"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "The transportation with the most mass is likely a", "arg_1": " commercial plane"}, "gen_args_1": {"arg_0": "The transportation with the most mass is likely a", "arg_1": " private plane"}, "gen_args_2": {"arg_0": "The transportation with the most mass is likely a", "arg_1": " bus"}, "gen_args_3": {"arg_0": "The transportation with the most mass is likely a", "arg_1": " private car"}}, "resps": [[["-10.0625", "False"]], [["-9.1875", "False"]], [["-4.34375", "False"]], [["-7.75", "False"]]], "filtered_resps": [["-10.0625", "False"], ["-9.1875", "False"], ["-4.34375", "False"], ["-7.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "8d43cfb3d5a9e149d466ade55d52c955f66036573b45c43e5e12db88fc6b24de", "prompt_hash": "7e03e06e0b1999d061a6db8b643137186d87a4ab38a7d7577c66c79300dda16c", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 56, "doc": {"id": "7-49", "question_stem": "A rabbit may enjoy", "choices": {"text": ["meat", "compost", "peas", "pebbles"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A rabbit may enjoy", "arg_1": " meat"}, "gen_args_1": {"arg_0": "A rabbit may enjoy", "arg_1": " compost"}, "gen_args_2": {"arg_0": "A rabbit may enjoy", "arg_1": " peas"}, "gen_args_3": {"arg_0": "A rabbit may enjoy", "arg_1": " pebbles"}}, "resps": [[["-11.3125", "False"]], [["-13.125", "False"]], [["-11.25", "False"]], [["-13.625", "False"]]], "filtered_resps": [["-11.3125", "False"], ["-13.125", "False"], ["-11.25", "False"], ["-13.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4a1558260b5788bc4ff9ce02da5a69c744309147ccc3071bcc1a23ba856acec5", "prompt_hash": "4b00a82b18e5007c77ffc6a9bb172225776f01a17ce28ef8ba381bfd967f0894", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 57, "doc": {"id": "8-393", "question_stem": "A construction group wants to put a shopping center in town, but the only place available is a small nature park with a trail. Deer and other wildlife frequent the park, since it is the only place in the city where trees and fresh water are available for them. The construction group decides to build the shopping center, which means that", "choices": {"text": ["the deer are moved to a zoo", "the trail is expanded", "the mall has a nature park in it", "the wildlife environment is destroyed"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "A construction group wants to put a shopping center in town, but the only place available is a small nature park with a trail. Deer and other wildlife frequent the park, since it is the only place in the city where trees and fresh water are available for them. The construction group decides to build the shopping center, which means that", "arg_1": " the deer are moved to a zoo"}, "gen_args_1": {"arg_0": "A construction group wants to put a shopping center in town, but the only place available is a small nature park with a trail. Deer and other wildlife frequent the park, since it is the only place in the city where trees and fresh water are available for them. The construction group decides to build the shopping center, which means that", "arg_1": " the trail is expanded"}, "gen_args_2": {"arg_0": "A construction group wants to put a shopping center in town, but the only place available is a small nature park with a trail. Deer and other wildlife frequent the park, since it is the only place in the city where trees and fresh water are available for them. The construction group decides to build the shopping center, which means that", "arg_1": " the mall has a nature park in it"}, "gen_args_3": {"arg_0": "A construction group wants to put a shopping center in town, but the only place available is a small nature park with a trail. Deer and other wildlife frequent the park, since it is the only place in the city where trees and fresh water are available for them. The construction group decides to build the shopping center, which means that", "arg_1": " the wildlife environment is destroyed"}}, "resps": [[["-18.5", "False"]], [["-16.125", "False"]], [["-29.75", "False"]], [["-18.25", "False"]]], "filtered_resps": [["-18.5", "False"], ["-16.125", "False"], ["-29.75", "False"], ["-18.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d32d54948a7489779a4e9b3c247d1d74d0886de0d60583b3c183cc4b97ba4098", "prompt_hash": "f48440679af78c139b769e47ae5d8fa573744a2deb6c4338b02ef269ec85adca", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 58, "doc": {"id": "788", "question_stem": "Owls are likely to hunt at", "choices": {"text": ["3pm", "2am", "6pm", "7am"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Owls are likely to hunt at", "arg_1": " 3pm"}, "gen_args_1": {"arg_0": "Owls are likely to hunt at", "arg_1": " 2am"}, "gen_args_2": {"arg_0": "Owls are likely to hunt at", "arg_1": " 6pm"}, "gen_args_3": {"arg_0": "Owls are likely to hunt at", "arg_1": " 7am"}}, "resps": [[["-18.375", "False"]], [["-17.875", "False"]], [["-17.875", "False"]], [["-21.625", "False"]]], "filtered_resps": [["-18.375", "False"], ["-17.875", "False"], ["-17.875", "False"], ["-21.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "21493f932b07767873b68ada318ef5bae6c0141991551b7219e81aeb5a5643d8", "prompt_hash": "ec9d30ff75fb012b885090369076ba1ebbe25062560d2a5e85b2705dfb373ecc", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 59, "doc": {"id": "9-29", "question_stem": "What could be a positive aspect of a tree being cut down?", "choices": {"text": ["the plants that were under the tree will have access to more light", "the squirrels that were in that tree will have an easier time getting to their home", "Plants under the tree will get cooled off by the shade", "The sun will shine brighter than before"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "What could be a positive aspect of a tree being cut down?", "arg_1": " the plants that were under the tree will have access to more light"}, "gen_args_1": {"arg_0": "What could be a positive aspect of a tree being cut down?", "arg_1": " the squirrels that were in that tree will have an easier time getting to their home"}, "gen_args_2": {"arg_0": "What could be a positive aspect of a tree being cut down?", "arg_1": " Plants under the tree will get cooled off by the shade"}, "gen_args_3": {"arg_0": "What could be a positive aspect of a tree being cut down?", "arg_1": " The sun will shine brighter than before"}}, "resps": [[["-51.5", "False"]], [["-66.5", "False"]], [["-55.5", "False"]], [["-36.0", "False"]]], "filtered_resps": [["-51.5", "False"], ["-66.5", "False"], ["-55.5", "False"], ["-36.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "3e7ac2a48de120aaba24c4b510beb815ae72cd29b0020bbc4365d0f02a3319f9", "prompt_hash": "b938264819b66956515e2ba7dd318c66d82335ea6fa7b14266327b37b06c2e4d", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 60, "doc": {"id": "9-368", "question_stem": "Birds carrying away fruit helps the tree", "choices": {"text": ["grow", "fertilize", "reproduce", "conquer"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Birds carrying away fruit helps the tree", "arg_1": " grow"}, "gen_args_1": {"arg_0": "Birds carrying away fruit helps the tree", "arg_1": " fertilize"}, "gen_args_2": {"arg_0": "Birds carrying away fruit helps the tree", "arg_1": " reproduce"}, "gen_args_3": {"arg_0": "Birds carrying away fruit helps the tree", "arg_1": " conquer"}}, "resps": [[["-3.359375", "False"]], [["-10.3125", "False"]], [["-7.28125", "False"]], [["-16.625", "False"]]], "filtered_resps": [["-3.359375", "False"], ["-10.3125", "False"], ["-7.28125", "False"], ["-16.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "06faf6146f6cb09ae13ed0d1d4421a1dd2fd5e887387c1a384ee2bac79dbdb4a", "prompt_hash": "97cb53761285208cffc2c30bbbbf9b7ba094b05ca8108cea104da4fa1df0b153", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 61, "doc": {"id": "7-671", "question_stem": "If a UFO is flying overhead and looks small, then large, then", "choices": {"text": ["the UFO is calling", "the UFO had been close", "the UFO is approaching", "the UFO is leaving"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "If a UFO is flying overhead and looks small, then large, then", "arg_1": " the UFO is calling"}, "gen_args_1": {"arg_0": "If a UFO is flying overhead and looks small, then large, then", "arg_1": " the UFO had been close"}, "gen_args_2": {"arg_0": "If a UFO is flying overhead and looks small, then large, then", "arg_1": " the UFO is approaching"}, "gen_args_3": {"arg_0": "If a UFO is flying overhead and looks small, then large, then", "arg_1": " the UFO is leaving"}}, "resps": [[["-22.75", "False"]], [["-31.0", "False"]], [["-17.625", "False"]], [["-19.25", "False"]]], "filtered_resps": [["-22.75", "False"], ["-31.0", "False"], ["-17.625", "False"], ["-19.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "8d2ada29e6a57690a2faf55bd083f9302502699982f8eb910f57436af26ddcc9", "prompt_hash": "f1b05dd3f65022fc408a675c3b4c882bd77b0cd2b391b6348a8238d742edfb0f", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 62, "doc": {"id": "1272", "question_stem": "Inherited behavior is exhibited when", "choices": {"text": ["bears take a long winter sleep", "dogs sit on command", "seals clap for their trainers", "rats navigate thru a maze"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Inherited behavior is exhibited when", "arg_1": " bears take a long winter sleep"}, "gen_args_1": {"arg_0": "Inherited behavior is exhibited when", "arg_1": " dogs sit on command"}, "gen_args_2": {"arg_0": "Inherited behavior is exhibited when", "arg_1": " seals clap for their trainers"}, "gen_args_3": {"arg_0": "Inherited behavior is exhibited when", "arg_1": " rats navigate thru a maze"}}, "resps": [[["-41.25", "False"]], [["-23.125", "False"]], [["-49.5", "False"]], [["-35.5", "False"]]], "filtered_resps": [["-41.25", "False"], ["-23.125", "False"], ["-49.5", "False"], ["-35.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "99ab0be367e07baf467e6e385497ab51980c9497435fc2cb90d028643c1b5005", "prompt_hash": "5327ad3e40e05abf7350c7490e7dd36bef93846fc372d96611df8ea3349dc47d", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 63, "doc": {"id": "648", "question_stem": "What likely explains deforestation?", "choices": {"text": ["Increased insect populations", "Clearing for farming", "reduction in rainfall", "More carbon dioxide"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "What likely explains deforestation?", "arg_1": " Increased insect populations"}, "gen_args_1": {"arg_0": "What likely explains deforestation?", "arg_1": " Clearing for farming"}, "gen_args_2": {"arg_0": "What likely explains deforestation?", "arg_1": " reduction in rainfall"}, "gen_args_3": {"arg_0": "What likely explains deforestation?", "arg_1": " More carbon dioxide"}}, "resps": [[["-25.0", "False"]], [["-25.75", "False"]], [["-24.625", "False"]], [["-25.375", "False"]]], "filtered_resps": [["-25.0", "False"], ["-25.75", "False"], ["-24.625", "False"], ["-25.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "7ee87eb060b61785d08844fde9ee75877b88bfa7897b56de8a411bc2d61dc6ee", "prompt_hash": "6e0e4bedf6d982bddbd814d1df770dc2718fff9e9d7cd597ea133a662adbf10e", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 64, "doc": {"id": "9-1180", "question_stem": "Mosquitoes enjoy all the people at a BBQ in the summer for what reason?", "choices": {"text": ["steak", "blood", "nice weather", "taking food"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Mosquitoes enjoy all the people at a BBQ in the summer for what reason?", "arg_1": " steak"}, "gen_args_1": {"arg_0": "Mosquitoes enjoy all the people at a BBQ in the summer for what reason?", "arg_1": " blood"}, "gen_args_2": {"arg_0": "Mosquitoes enjoy all the people at a BBQ in the summer for what reason?", "arg_1": " nice weather"}, "gen_args_3": {"arg_0": "Mosquitoes enjoy all the people at a BBQ in the summer for what reason?", "arg_1": " taking food"}}, "resps": [[["-21.0", "False"]], [["-15.3125", "False"]], [["-21.25", "False"]], [["-20.375", "False"]]], "filtered_resps": [["-21.0", "False"], ["-15.3125", "False"], ["-21.25", "False"], ["-20.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e36cce5ff198afc2a7a1de90e5c219c6c644960baff2fd19d6265d5903999b82", "prompt_hash": "8e620e888c0a654583d1647b03d0016a13ee9c1df667607f66e2d168a12e2451", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 65, "doc": {"id": "9-227", "question_stem": "The surface of the moon contains", "choices": {"text": ["dogs", "water", "high peaks", "humans"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "The surface of the moon contains", "arg_1": " dogs"}, "gen_args_1": {"arg_0": "The surface of the moon contains", "arg_1": " water"}, "gen_args_2": {"arg_0": "The surface of the moon contains", "arg_1": " high peaks"}, "gen_args_3": {"arg_0": "The surface of the moon contains", "arg_1": " humans"}}, "resps": [[["-19.75", "False"]], [["-3.71875", "False"]], [["-19.25", "False"]], [["-14.25", "False"]]], "filtered_resps": [["-19.75", "False"], ["-3.71875", "False"], ["-19.25", "False"], ["-14.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "b6cb60a8c385eee2a11a490e461f6a7ecf4161dc836f472515cf02b435a294eb", "prompt_hash": "f57e648c2fbce7a1b588d4d458b5c2a64cabfc5620f76f686b438bb5c4d52088", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 66, "doc": {"id": "1582", "question_stem": "A tool used to identify the percent chance of a trait being passed down has at least", "choices": {"text": ["four boxes", "eight boxes", "two boxes", "six boxes"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "A tool used to identify the percent chance of a trait being passed down has at least", "arg_1": " four boxes"}, "gen_args_1": {"arg_0": "A tool used to identify the percent chance of a trait being passed down has at least", "arg_1": " eight boxes"}, "gen_args_2": {"arg_0": "A tool used to identify the percent chance of a trait being passed down has at least", "arg_1": " two boxes"}, "gen_args_3": {"arg_0": "A tool used to identify the percent chance of a trait being passed down has at least", "arg_1": " six boxes"}}, "resps": [[["-13.125", "False"]], [["-17.5", "False"]], [["-9.375", "False"]], [["-15.25", "False"]]], "filtered_resps": [["-13.125", "False"], ["-17.5", "False"], ["-9.375", "False"], ["-15.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "c58bfc97b7f2ba2783e6efb99bb177e46e1ef6770efaf0d48d878666c4943b6f", "prompt_hash": "b2903eafd7e7df5b86db3f359e7aa30fc629c89a3e60a7390ec5e29b42a808f4", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 67, "doc": {"id": "8-125", "question_stem": "A prisoner is kept in a stone room, unable to see the sun. The prisoner knows that he needs vitamin D to survive, so he", "choices": {"text": ["asks for milk", "asks for television", "asks for water", "asks for sleep"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "A prisoner is kept in a stone room, unable to see the sun. The prisoner knows that he needs vitamin D to survive, so he", "arg_1": " asks for milk"}, "gen_args_1": {"arg_0": "A prisoner is kept in a stone room, unable to see the sun. The prisoner knows that he needs vitamin D to survive, so he", "arg_1": " asks for television"}, "gen_args_2": {"arg_0": "A prisoner is kept in a stone room, unable to see the sun. The prisoner knows that he needs vitamin D to survive, so he", "arg_1": " asks for water"}, "gen_args_3": {"arg_0": "A prisoner is kept in a stone room, unable to see the sun. The prisoner knows that he needs vitamin D to survive, so he", "arg_1": " asks for sleep"}}, "resps": [[["-15.875", "False"]], [["-20.75", "False"]], [["-10.6875", "False"]], [["-17.375", "False"]]], "filtered_resps": [["-15.875", "False"], ["-20.75", "False"], ["-10.6875", "False"], ["-17.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "83bd645b168b40f9443c00fba692e23b5743c40e1e735eab6e78f9587bbfe421", "prompt_hash": "2a834bfc3f8788712f9cb3a8a8640ea082a85c061a8d61316469deee2f121dd4", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 68, "doc": {"id": "1923", "question_stem": "When trying to pull a rose out of the ground why do you encounter resistance?", "choices": {"text": ["roots", "tensile strength", "plant temperature", "plant color"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "When trying to pull a rose out of the ground why do you encounter resistance?", "arg_1": " roots"}, "gen_args_1": {"arg_0": "When trying to pull a rose out of the ground why do you encounter resistance?", "arg_1": " tensile strength"}, "gen_args_2": {"arg_0": "When trying to pull a rose out of the ground why do you encounter resistance?", "arg_1": " plant temperature"}, "gen_args_3": {"arg_0": "When trying to pull a rose out of the ground why do you encounter resistance?", "arg_1": " plant color"}}, "resps": [[["-13.9375", "False"]], [["-23.0", "False"]], [["-30.5", "False"]], [["-26.0", "False"]]], "filtered_resps": [["-13.9375", "False"], ["-23.0", "False"], ["-30.5", "False"], ["-26.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "9595e21f39ad8d3c64c3b775e55f2f53d8e061528506a4782d0d484283c7d627", "prompt_hash": "cf80981b7b977e900069aec228554229b78a20d78eaf2becf1f512f967c1a0fe", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 69, "doc": {"id": "9-229", "question_stem": "A bat flew through the sky without hitting anything due to which of these?", "choices": {"text": ["rainy sky to fly in", "fast truck to drive", "a car with gasoline", "surfaces to reflect sound off"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "A bat flew through the sky without hitting anything due to which of these?", "arg_1": " rainy sky to fly in"}, "gen_args_1": {"arg_0": "A bat flew through the sky without hitting anything due to which of these?", "arg_1": " fast truck to drive"}, "gen_args_2": {"arg_0": "A bat flew through the sky without hitting anything due to which of these?", "arg_1": " a car with gasoline"}, "gen_args_3": {"arg_0": "A bat flew through the sky without hitting anything due to which of these?", "arg_1": " surfaces to reflect sound off"}}, "resps": [[["-44.0", "False"]], [["-51.0", "False"]], [["-41.5", "False"]], [["-46.75", "False"]]], "filtered_resps": [["-44.0", "False"], ["-51.0", "False"], ["-41.5", "False"], ["-46.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a8645d075bdb4c961df5b71583842950f6e1d89d0bc727441583d89add503668", "prompt_hash": "2a9526a4a7dbd10f7800cc635ef8005c6953d56c26577f266c0c2b840f182383", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 70, "doc": {"id": "1702", "question_stem": "How can we see that the coloration of fur is an inherited characteristic?", "choices": {"text": ["puppies have soft fur", "kittens look like their parents", "all mammals are born with fur", "baby rats are mostly bald"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "How can we see that the coloration of fur is an inherited characteristic?", "arg_1": " puppies have soft fur"}, "gen_args_1": {"arg_0": "How can we see that the coloration of fur is an inherited characteristic?", "arg_1": " kittens look like their parents"}, "gen_args_2": {"arg_0": "How can we see that the coloration of fur is an inherited characteristic?", "arg_1": " all mammals are born with fur"}, "gen_args_3": {"arg_0": "How can we see that the coloration of fur is an inherited characteristic?", "arg_1": " baby rats are mostly bald"}}, "resps": [[["-32.25", "False"]], [["-34.0", "False"]], [["-34.0", "False"]], [["-46.25", "False"]]], "filtered_resps": [["-32.25", "False"], ["-34.0", "False"], ["-34.0", "False"], ["-46.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "447b78ff04b1a03e01aa7a95fecefb0be5c547632a7d3d28ba72a5535b25b56d", "prompt_hash": "aad89329e0c236aebe70c646676dcd3ba5dcec705c88e3c6beeff3abe4e097b5", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 71, "doc": {"id": "8-260", "question_stem": "Decaying vegetation is part of the process that", "choices": {"text": ["enables nuclear power to function", "enables to emitting of light beams", "enables gas powered motors to operate", "enables windmills to power electric grids"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Decaying vegetation is part of the process that", "arg_1": " enables nuclear power to function"}, "gen_args_1": {"arg_0": "Decaying vegetation is part of the process that", "arg_1": " enables to emitting of light beams"}, "gen_args_2": {"arg_0": "Decaying vegetation is part of the process that", "arg_1": " enables gas powered motors to operate"}, "gen_args_3": {"arg_0": "Decaying vegetation is part of the process that", "arg_1": " enables windmills to power electric grids"}}, "resps": [[["-34.25", "False"]], [["-55.25", "False"]], [["-52.25", "False"]], [["-41.75", "False"]]], "filtered_resps": [["-34.25", "False"], ["-55.25", "False"], ["-52.25", "False"], ["-41.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "6671c930feb44e79e78ce194626fd1fd953d8228a03061931a9a89d9db053849", "prompt_hash": "7536dd7271f6c670619f758ad94e51373398215a561eac0d6509fc256d2f3a73", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 72, "doc": {"id": "9-491", "question_stem": "After a torrential downpour over a week, a man notices that the pond in his backyard is", "choices": {"text": ["melted", "dehydrated", "bloated", "salted"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "After a torrential downpour over a week, a man notices that the pond in his backyard is", "arg_1": " melted"}, "gen_args_1": {"arg_0": "After a torrential downpour over a week, a man notices that the pond in his backyard is", "arg_1": " dehydrated"}, "gen_args_2": {"arg_0": "After a torrential downpour over a week, a man notices that the pond in his backyard is", "arg_1": " bloated"}, "gen_args_3": {"arg_0": "After a torrential downpour over a week, a man notices that the pond in his backyard is", "arg_1": " salted"}}, "resps": [[["-16.125", "False"]], [["-19.0", "False"]], [["-9.875", "False"]], [["-16.0", "False"]]], "filtered_resps": [["-16.125", "False"], ["-19.0", "False"], ["-9.875", "False"], ["-16.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "8bceea39bae9a15c8bdef192f1096a820644f8068efa99e6c38e8eb6ee246727", "prompt_hash": "6b07391ffd205833b2bc81d9c29c6047d0d58403a05694439167fd6c44de8284", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 73, "doc": {"id": "75", "question_stem": "The amount of friction and the speed of an object have what kind of relationship?", "choices": {"text": ["inverse", "reverse", "direct", "equal"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "The amount of friction and the speed of an object have what kind of relationship?", "arg_1": " inverse"}, "gen_args_1": {"arg_0": "The amount of friction and the speed of an object have what kind of relationship?", "arg_1": " reverse"}, "gen_args_2": {"arg_0": "The amount of friction and the speed of an object have what kind of relationship?", "arg_1": " direct"}, "gen_args_3": {"arg_0": "The amount of friction and the speed of an object have what kind of relationship?", "arg_1": " equal"}}, "resps": [[["-18.25", "False"]], [["-20.0", "False"]], [["-19.75", "False"]], [["-20.5", "False"]]], "filtered_resps": [["-18.25", "False"], ["-20.0", "False"], ["-19.75", "False"], ["-20.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f7265a644b4f8a0ced0c79810cb9e2d1d753ed37585da93024d965813a0d79f5", "prompt_hash": "a38549c85193c6d459a660c12008534d1e5836538f361f0a6c52ad42d6dbdfa1", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 74, "doc": {"id": "1215", "question_stem": "A fallen leaf", "choices": {"text": ["will turn into a tree", "will become bright green", "will begin to recycle the nutrients that made up its structure", "is likely to continue to grow"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A fallen leaf", "arg_1": " will turn into a tree"}, "gen_args_1": {"arg_0": "A fallen leaf", "arg_1": " will become bright green"}, "gen_args_2": {"arg_0": "A fallen leaf", "arg_1": " will begin to recycle the nutrients that made up its structure"}, "gen_args_3": {"arg_0": "A fallen leaf", "arg_1": " is likely to continue to grow"}}, "resps": [[["-22.75", "False"]], [["-27.875", "False"]], [["-48.25", "False"]], [["-27.0", "False"]]], "filtered_resps": [["-22.75", "False"], ["-27.875", "False"], ["-48.25", "False"], ["-27.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "5d10831cba671c4b00629193c4960e0a4ebc4b735023d23d259859e0e99694e5", "prompt_hash": "853913e03d04a409aec1cc8faf17b02f0a8928107bebd4dfc3868d39a6386684", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 75, "doc": {"id": "8-93", "question_stem": "Over a period of time the weather can change", "choices": {"text": ["The color of my hair", "The way I walk", "The size of a statue", "The sound a computer makes"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Over a period of time the weather can change", "arg_1": " The color of my hair"}, "gen_args_1": {"arg_0": "Over a period of time the weather can change", "arg_1": " The way I walk"}, "gen_args_2": {"arg_0": "Over a period of time the weather can change", "arg_1": " The size of a statue"}, "gen_args_3": {"arg_0": "Over a period of time the weather can change", "arg_1": " The sound a computer makes"}}, "resps": [[["-35.0", "False"]], [["-37.25", "False"]], [["-38.25", "False"]], [["-42.5", "False"]]], "filtered_resps": [["-35.0", "False"], ["-37.25", "False"], ["-38.25", "False"], ["-42.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "992dce21e8614075308a8ba1b3770aa70a0d5967e7af00c0db20b32f22b6d934", "prompt_hash": "5e33b923292adcf0812d9d2bb5d40e2b3d8806e4c110665ee211036ca316e8b1", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 76, "doc": {"id": "7-988", "question_stem": "Plant growth may cause", "choices": {"text": ["an uptick in the number of leaves", "a surge in leaf disease", "a gradual decrease in leaves", "a rapid decline of the leaves"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Plant growth may cause", "arg_1": " an uptick in the number of leaves"}, "gen_args_1": {"arg_0": "Plant growth may cause", "arg_1": " a surge in leaf disease"}, "gen_args_2": {"arg_0": "Plant growth may cause", "arg_1": " a gradual decrease in leaves"}, "gen_args_3": {"arg_0": "Plant growth may cause", "arg_1": " a rapid decline of the leaves"}}, "resps": [[["-20.25", "False"]], [["-26.625", "False"]], [["-17.875", "False"]], [["-20.625", "False"]]], "filtered_resps": [["-20.25", "False"], ["-26.625", "False"], ["-17.875", "False"], ["-20.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2b853b85ceb301ce479f7502e67b50408acae01c2de79629580efa11133ac25b", "prompt_hash": "3fa6fedbfa824913538067e7ef379dbbe1a0b2683f8bbaa8fedaf872a8f66a13", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 77, "doc": {"id": "9-1139", "question_stem": "The man's heart skipped a beat and he felt pain after touching which of these?", "choices": {"text": ["ice cube", "water", "electrical transformer", "grass"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "The man's heart skipped a beat and he felt pain after touching which of these?", "arg_1": " ice cube"}, "gen_args_1": {"arg_0": "The man's heart skipped a beat and he felt pain after touching which of these?", "arg_1": " water"}, "gen_args_2": {"arg_0": "The man's heart skipped a beat and he felt pain after touching which of these?", "arg_1": " electrical transformer"}, "gen_args_3": {"arg_0": "The man's heart skipped a beat and he felt pain after touching which of these?", "arg_1": " grass"}}, "resps": [[["-22.75", "False"]], [["-15.1875", "False"]], [["-31.875", "False"]], [["-17.375", "False"]]], "filtered_resps": [["-22.75", "False"], ["-15.1875", "False"], ["-31.875", "False"], ["-17.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "7eaabf9fbb06b14897b4e9cc4e999a424600c4a8dabf97ba3802281136e7579e", "prompt_hash": "bf666e8b855d60b7de426816ca98e6d935f66226e8634a24f4485a2f54c55369", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 78, "doc": {"id": "1545", "question_stem": "Which substance is capable of dripping?", "choices": {"text": ["Oxygen", "Juice", "Wood", "Lightning"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Which substance is capable of dripping?", "arg_1": " Oxygen"}, "gen_args_1": {"arg_0": "Which substance is capable of dripping?", "arg_1": " Juice"}, "gen_args_2": {"arg_0": "Which substance is capable of dripping?", "arg_1": " Wood"}, "gen_args_3": {"arg_0": "Which substance is capable of dripping?", "arg_1": " Lightning"}}, "resps": [[["-13.0", "False"]], [["-11.875", "False"]], [["-12.3125", "False"]], [["-15.125", "False"]]], "filtered_resps": [["-13.0", "False"], ["-11.875", "False"], ["-12.3125", "False"], ["-15.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f85cd689d09366ca67ccfc58b60b0a135dc098eb838331f7e948de4eef69321e", "prompt_hash": "88a9e17f60820856e2e570fa97486662bf9e9a80ed99a6b65af0959ba867810d", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 79, "doc": {"id": "7-664", "question_stem": "If bacon is left too long on a hot stove top", "choices": {"text": ["it will be cooked perfectly", "it will be bacteria laden", "it will become blackened", "it will be left raw"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "If bacon is left too long on a hot stove top", "arg_1": " it will be cooked perfectly"}, "gen_args_1": {"arg_0": "If bacon is left too long on a hot stove top", "arg_1": " it will be bacteria laden"}, "gen_args_2": {"arg_0": "If bacon is left too long on a hot stove top", "arg_1": " it will become blackened"}, "gen_args_3": {"arg_0": "If bacon is left too long on a hot stove top", "arg_1": " it will be left raw"}}, "resps": [[["-18.625", "False"]], [["-23.625", "False"]], [["-12.1875", "False"]], [["-27.25", "False"]]], "filtered_resps": [["-18.625", "False"], ["-23.625", "False"], ["-12.1875", "False"], ["-27.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "09796f231ccb6ca7066facdb5994c4251a1d10820dd2c4941e4f330c7f5f4e12", "prompt_hash": "6966a55de09441452589dc2b46d9d0de207b3c774601bed3e48274ea98891bde", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 80, "doc": {"id": "8-53", "question_stem": "the dashboard reading in a jaguar would likely be set to which of these?", "choices": {"text": ["set to calories", "set to volume", "set to kilometers", "set to width"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "the dashboard reading in a jaguar would likely be set to which of these?", "arg_1": " set to calories"}, "gen_args_1": {"arg_0": "the dashboard reading in a jaguar would likely be set to which of these?", "arg_1": " set to volume"}, "gen_args_2": {"arg_0": "the dashboard reading in a jaguar would likely be set to which of these?", "arg_1": " set to kilometers"}, "gen_args_3": {"arg_0": "the dashboard reading in a jaguar would likely be set to which of these?", "arg_1": " set to width"}}, "resps": [[["-29.25", "False"]], [["-24.375", "False"]], [["-28.125", "False"]], [["-26.5", "False"]]], "filtered_resps": [["-29.25", "False"], ["-24.375", "False"], ["-28.125", "False"], ["-26.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "54471a31ddf4c7cc550defa4e3d709704083464fd71d3f721be90c515f18e948", "prompt_hash": "968eeb0ea04b00c5915d3a8763f7ffe2505c8f78d9e441612724841eb56da389", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 81, "doc": {"id": "7-1044", "question_stem": "are explosions safe?", "choices": {"text": ["they could harm living things", "they are very safe", "they cause nothing serious", "none of these"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "are explosions safe?", "arg_1": " they could harm living things"}, "gen_args_1": {"arg_0": "are explosions safe?", "arg_1": " they are very safe"}, "gen_args_2": {"arg_0": "are explosions safe?", "arg_1": " they cause nothing serious"}, "gen_args_3": {"arg_0": "are explosions safe?", "arg_1": " none of these"}}, "resps": [[["-24.875", "False"]], [["-22.125", "False"]], [["-30.25", "False"]], [["-15.375", "False"]]], "filtered_resps": [["-24.875", "False"], ["-22.125", "False"], ["-30.25", "False"], ["-15.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "869b5b6b181a3771fb10622c5b5baec75c33a3fd022a24c15a0711841fcc46d2", "prompt_hash": "066e0f047e6758c7ab8306a03ba6883c1e3b6b59ef673b16c09c3bc682a8653a", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 82, "doc": {"id": "7-1122", "question_stem": "The lowest temperature on the trip was at", "choices": {"text": ["the mountain pass", "the plain", "the large hill", "the canyon"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "The lowest temperature on the trip was at", "arg_1": " the mountain pass"}, "gen_args_1": {"arg_0": "The lowest temperature on the trip was at", "arg_1": " the plain"}, "gen_args_2": {"arg_0": "The lowest temperature on the trip was at", "arg_1": " the large hill"}, "gen_args_3": {"arg_0": "The lowest temperature on the trip was at", "arg_1": " the canyon"}}, "resps": [[["-10.875", "False"]], [["-17.25", "False"]], [["-22.125", "False"]], [["-11.0", "False"]]], "filtered_resps": [["-10.875", "False"], ["-17.25", "False"], ["-22.125", "False"], ["-11.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "94bc65c90e532d6618b4851eb2232ce8216d74e1f0ac3a1d9806d9579e667719", "prompt_hash": "7024e37119d5dd08076654efc406ef7ab6e156953849f77a770406e5b81d982e", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 83, "doc": {"id": "9-79", "question_stem": "What is the benefit to using a frosted window film over a non treated windows?", "choices": {"text": ["they are easier to make", "they let in less light", "they are cheaper to produce", "they are much stronger"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "What is the benefit to using a frosted window film over a non treated windows?", "arg_1": " they are easier to make"}, "gen_args_1": {"arg_0": "What is the benefit to using a frosted window film over a non treated windows?", "arg_1": " they let in less light"}, "gen_args_2": {"arg_0": "What is the benefit to using a frosted window film over a non treated windows?", "arg_1": " they are cheaper to produce"}, "gen_args_3": {"arg_0": "What is the benefit to using a frosted window film over a non treated windows?", "arg_1": " they are much stronger"}}, "resps": [[["-30.625", "False"]], [["-27.375", "False"]], [["-30.25", "False"]], [["-28.625", "False"]]], "filtered_resps": [["-30.625", "False"], ["-27.375", "False"], ["-30.25", "False"], ["-28.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e592c01540ec20eca0b2a2628a97e4f0ba3201e3eb34777bc594fa91050f3bf2", "prompt_hash": "8c56f9a43c9e4132c10cf7a6a0284606ef8bc8f7f617aef9e98d4d44382119c7", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 84, "doc": {"id": "7-157", "question_stem": "Ocean water contains", "choices": {"text": ["copious amounts of seltzer", "scant amounts of sodium chloride", "scant amounts of carbonation", "copious amounts of the combination of Na and Cl"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Ocean water contains", "arg_1": " copious amounts of seltzer"}, "gen_args_1": {"arg_0": "Ocean water contains", "arg_1": " scant amounts of sodium chloride"}, "gen_args_2": {"arg_0": "Ocean water contains", "arg_1": " scant amounts of carbonation"}, "gen_args_3": {"arg_0": "Ocean water contains", "arg_1": " copious amounts of the combination of Na and Cl"}}, "resps": [[["-31.75", "False"]], [["-25.5", "False"]], [["-35.0", "False"]], [["-43.0", "False"]]], "filtered_resps": [["-31.75", "False"], ["-25.5", "False"], ["-35.0", "False"], ["-43.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "cecb6a9b46f31ac3b61d5537f5e891da68413c2ec07123e9cb9dd27a0592f4bd", "prompt_hash": "54298233555d1b2da6579607a60e0f7e3189c09e2e0bd52c8906500ffc5f6682", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 85, "doc": {"id": "9-1164", "question_stem": "A cheetah that runs all day will find it has lost a lot of", "choices": {"text": ["blood", "water", "prey", "spots"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "A cheetah that runs all day will find it has lost a lot of", "arg_1": " blood"}, "gen_args_1": {"arg_0": "A cheetah that runs all day will find it has lost a lot of", "arg_1": " water"}, "gen_args_2": {"arg_0": "A cheetah that runs all day will find it has lost a lot of", "arg_1": " prey"}, "gen_args_3": {"arg_0": "A cheetah that runs all day will find it has lost a lot of", "arg_1": " spots"}}, "resps": [[["-6.65625", "False"]], [["-5.53125", "False"]], [["-8.875", "False"]], [["-9.3125", "False"]]], "filtered_resps": [["-6.65625", "False"], ["-5.53125", "False"], ["-8.875", "False"], ["-9.3125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "eaa516793865d2f3e01b59e0f8f7f833b83e9d35a71d6ccea7585370ac232f62", "prompt_hash": "aea36b2299998f82af4c17f3bc7c0525285ff6c8f07cabe8d679159d0c38111b", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 86, "doc": {"id": "8-63", "question_stem": "Beak shape can influence a bird's ability", "choices": {"text": ["to give birth to live young", "to mate with it's partner", "to fly to warmer climates", "to chew up certain worms"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Beak shape can influence a bird's ability", "arg_1": " to give birth to live young"}, "gen_args_1": {"arg_0": "Beak shape can influence a bird's ability", "arg_1": " to mate with it's partner"}, "gen_args_2": {"arg_0": "Beak shape can influence a bird's ability", "arg_1": " to fly to warmer climates"}, "gen_args_3": {"arg_0": "Beak shape can influence a bird's ability", "arg_1": " to chew up certain worms"}}, "resps": [[["-21.0", "False"]], [["-23.0", "False"]], [["-27.625", "False"]], [["-35.5", "False"]]], "filtered_resps": [["-21.0", "False"], ["-23.0", "False"], ["-27.625", "False"], ["-35.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "310d4e40e2e9e6feed75c2592d2cc3b69139ee845ee4513d85f898f6739353b5", "prompt_hash": "e1aafbc306e0a6a4def627d195a06b2329242cd5313acb96ca6bfb793b6c3ca9", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 87, "doc": {"id": "8-308", "question_stem": "Some blind people have demonstrated bat-like skills by:", "choices": {"text": ["sensing shapes by light and shadows", "having a unusually strong sense of smell", "sensing nearby objects by temperature change", "using sound to 'see'"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Some blind people have demonstrated bat-like skills by:", "arg_1": " sensing shapes by light and shadows"}, "gen_args_1": {"arg_0": "Some blind people have demonstrated bat-like skills by:", "arg_1": " having a unusually strong sense of smell"}, "gen_args_2": {"arg_0": "Some blind people have demonstrated bat-like skills by:", "arg_1": " sensing nearby objects by temperature change"}, "gen_args_3": {"arg_0": "Some blind people have demonstrated bat-like skills by:", "arg_1": " using sound to 'see'"}}, "resps": [[["-36.25", "False"]], [["-28.0", "False"]], [["-44.25", "False"]], [["-22.625", "False"]]], "filtered_resps": [["-36.25", "False"], ["-28.0", "False"], ["-44.25", "False"], ["-22.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "b7ffe94ba90f15a567406ace181f67879b0510ce8f0561f6f0344898f3fa3386", "prompt_hash": "3f0090fb0163bc587ca5d2665afc42de39478a1d4c3ea8c3bffe374d64cc199a", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 88, "doc": {"id": "326", "question_stem": "December 21st through March 20 is a three month period which is an example of what?", "choices": {"text": ["A session", "A Match", "A Season", "Autumn"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "December 21st through March 20 is a three month period which is an example of what?", "arg_1": " A session"}, "gen_args_1": {"arg_0": "December 21st through March 20 is a three month period which is an example of what?", "arg_1": " A Match"}, "gen_args_2": {"arg_0": "December 21st through March 20 is a three month period which is an example of what?", "arg_1": " A Season"}, "gen_args_3": {"arg_0": "December 21st through March 20 is a three month period which is an example of what?", "arg_1": " Autumn"}}, "resps": [[["-19.75", "False"]], [["-23.125", "False"]], [["-18.0", "False"]], [["-14.5625", "False"]]], "filtered_resps": [["-19.75", "False"], ["-23.125", "False"], ["-18.0", "False"], ["-14.5625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "dc51d0919648259a2fe1396bd3caa0fae52901ac580decba9350099a144b409b", "prompt_hash": "add1f83e850705cc09d4e36570e41f8b0761b2051fba4be3d2937b2e1b10f93b", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 89, "doc": {"id": "1184", "question_stem": "Seeds provide new plants with", "choices": {"text": ["life sustaining elements", "essentials for photosynthesis", "water and hydrogen", "storage for roots"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Seeds provide new plants with", "arg_1": " life sustaining elements"}, "gen_args_1": {"arg_0": "Seeds provide new plants with", "arg_1": " essentials for photosynthesis"}, "gen_args_2": {"arg_0": "Seeds provide new plants with", "arg_1": " water and hydrogen"}, "gen_args_3": {"arg_0": "Seeds provide new plants with", "arg_1": " storage for roots"}}, "resps": [[["-22.0", "False"]], [["-20.375", "False"]], [["-23.625", "False"]], [["-22.75", "False"]]], "filtered_resps": [["-22.0", "False"], ["-20.375", "False"], ["-23.625", "False"], ["-22.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d8a7e5a14f0c3185a47be6fd6f83cd24d4ede752591858752be942e3ba76a0af", "prompt_hash": "27ffa35336513780c09edb162c34589af1fe4fbc0fccf37a8cfebdb5583f6bd7", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 90, "doc": {"id": "359", "question_stem": "What is a more comfortable color to have for your automobile upholstery if living in a desert?", "choices": {"text": ["ecru", "red", "black", "navy"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "What is a more comfortable color to have for your automobile upholstery if living in a desert?", "arg_1": " ecru"}, "gen_args_1": {"arg_0": "What is a more comfortable color to have for your automobile upholstery if living in a desert?", "arg_1": " red"}, "gen_args_2": {"arg_0": "What is a more comfortable color to have for your automobile upholstery if living in a desert?", "arg_1": " black"}, "gen_args_3": {"arg_0": "What is a more comfortable color to have for your automobile upholstery if living in a desert?", "arg_1": " navy"}}, "resps": [[["-31.125", "False"]], [["-20.0", "False"]], [["-20.25", "False"]], [["-26.5", "False"]]], "filtered_resps": [["-31.125", "False"], ["-20.0", "False"], ["-20.25", "False"], ["-26.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "ccd52208016176774a7b48a09f551472450dd16d0cd2f8f6a27f08f3152fcfc4", "prompt_hash": "1949bf2a3cb7e4fa1ca60dbbaaf6776310102772d9322e20396c0ee3bfcf033c", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 91, "doc": {"id": "9-350", "question_stem": "The salamander could eat a large amounts of what?", "choices": {"text": ["fettuccine if left around", "waxy leaves from certain plants", "dead carcass meat from livestock", "six legged winged organisms"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "The salamander could eat a large amounts of what?", "arg_1": " fettuccine if left around"}, "gen_args_1": {"arg_0": "The salamander could eat a large amounts of what?", "arg_1": " waxy leaves from certain plants"}, "gen_args_2": {"arg_0": "The salamander could eat a large amounts of what?", "arg_1": " dead carcass meat from livestock"}, "gen_args_3": {"arg_0": "The salamander could eat a large amounts of what?", "arg_1": " six legged winged organisms"}}, "resps": [[["-52.0", "False"]], [["-38.75", "False"]], [["-47.25", "False"]], [["-42.25", "False"]]], "filtered_resps": [["-52.0", "False"], ["-38.75", "False"], ["-47.25", "False"], ["-42.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4b5b38d92afe3bc3d9c3edde03f375f64b43d9e59743010710e42a8742904f3c", "prompt_hash": "89b41a21e6d06a43ccebaaee65b1b7ba83572429cfe3b7f0e92828f8f0eb9fdf", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 92, "doc": {"id": "7-140", "question_stem": "A person can see", "choices": {"text": ["a radio recording", "an emotion", "a written message", "an abstract idea"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A person can see", "arg_1": " a radio recording"}, "gen_args_1": {"arg_0": "A person can see", "arg_1": " an emotion"}, "gen_args_2": {"arg_0": "A person can see", "arg_1": " a written message"}, "gen_args_3": {"arg_0": "A person can see", "arg_1": " an abstract idea"}}, "resps": [[["-21.875", "False"]], [["-12.25", "False"]], [["-13.5625", "False"]], [["-14.0", "False"]]], "filtered_resps": [["-21.875", "False"], ["-12.25", "False"], ["-13.5625", "False"], ["-14.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "eaa194b8074369689a5d24f39a310afafd86dd58a6459d7874e8e5385836f711", "prompt_hash": "c6a1c9e6911b15302d16c2f618dca511bfb1d53c80a76b9fb8d3c00387fe1c3c", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 93, "doc": {"id": "591", "question_stem": "When looking for good soil for plants, typically what is optimal?", "choices": {"text": ["malleable and nutritious", "dry and sandy", "grainy and bitter", "compact and hard"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "When looking for good soil for plants, typically what is optimal?", "arg_1": " malleable and nutritious"}, "gen_args_1": {"arg_0": "When looking for good soil for plants, typically what is optimal?", "arg_1": " dry and sandy"}, "gen_args_2": {"arg_0": "When looking for good soil for plants, typically what is optimal?", "arg_1": " grainy and bitter"}, "gen_args_3": {"arg_0": "When looking for good soil for plants, typically what is optimal?", "arg_1": " compact and hard"}}, "resps": [[["-28.875", "False"]], [["-21.0", "False"]], [["-37.0", "False"]], [["-23.5", "False"]]], "filtered_resps": [["-28.875", "False"], ["-21.0", "False"], ["-37.0", "False"], ["-23.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "7050bd67d7524a9e84b5397814d3868e3727f394be8f5436e7ff850384edd1b8", "prompt_hash": "f5c4b43db1d17b4442da4d3c840c91c924d26dacd9dc79b0518d73143e253c13", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 94, "doc": {"id": "7-391", "question_stem": "Animals are drawn to", "choices": {"text": ["gold", "houses", "feeders", "Carbon Dioxide"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Animals are drawn to", "arg_1": " gold"}, "gen_args_1": {"arg_0": "Animals are drawn to", "arg_1": " houses"}, "gen_args_2": {"arg_0": "Animals are drawn to", "arg_1": " feeders"}, "gen_args_3": {"arg_0": "Animals are drawn to", "arg_1": " Carbon Dioxide"}}, "resps": [[["-8.0625", "False"]], [["-10.0625", "False"]], [["-11.1875", "False"]], [["-21.0", "False"]]], "filtered_resps": [["-8.0625", "False"], ["-10.0625", "False"], ["-11.1875", "False"], ["-21.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "219c8d693a705eae439458a1a751cadbc5c39cf5a265794b0a78cae86e6f1a2b", "prompt_hash": "8fe0c44b29b24bd74845532ea2064f3e73fa40d180784f28688cc88c2c4ee811", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 95, "doc": {"id": "1672", "question_stem": "Squirrels spend their fall", "choices": {"text": ["looking for pretty leaves to collect", "stockpiling rocks for fighting in the winter", "stockpiling pecans for the frigid months", "collecting twigs to keep warm"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Squirrels spend their fall", "arg_1": " looking for pretty leaves to collect"}, "gen_args_1": {"arg_0": "Squirrels spend their fall", "arg_1": " stockpiling rocks for fighting in the winter"}, "gen_args_2": {"arg_0": "Squirrels spend their fall", "arg_1": " stockpiling pecans for the frigid months"}, "gen_args_3": {"arg_0": "Squirrels spend their fall", "arg_1": " collecting twigs to keep warm"}}, "resps": [[["-37.25", "False"]], [["-51.0", "False"]], [["-48.0", "False"]], [["-35.5", "False"]]], "filtered_resps": [["-37.25", "False"], ["-51.0", "False"], ["-48.0", "False"], ["-35.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "803577c666df2555383db4db62e8e85b02282b4c8005c1105d0b0244da208926", "prompt_hash": "f96d66998d943ecc8be90f06c0f52cf01d6c0deb417b989876726e086e95e5a1", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 96, "doc": {"id": "9-464", "question_stem": "Rainbows are always found after what?", "choices": {"text": ["A fire", "A tornado", "Rainfall", "Cereal"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Rainbows are always found after what?", "arg_1": " A fire"}, "gen_args_1": {"arg_0": "Rainbows are always found after what?", "arg_1": " A tornado"}, "gen_args_2": {"arg_0": "Rainbows are always found after what?", "arg_1": " Rainfall"}, "gen_args_3": {"arg_0": "Rainbows are always found after what?", "arg_1": " Cereal"}}, "resps": [[["-16.125", "False"]], [["-14.875", "False"]], [["-12.6875", "False"]], [["-23.25", "False"]]], "filtered_resps": [["-16.125", "False"], ["-14.875", "False"], ["-12.6875", "False"], ["-23.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d823eb776da1b747011717fd0608a469c6d12e0d5b0ce1fb979cb0c373b363d9", "prompt_hash": "4b3e5105e19aade17b9f886a5be04d9995fee1c5401e2941cde9aa375ccba64c", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 97, "doc": {"id": "9-983", "question_stem": "Crop rotation has a positive impact on what?", "choices": {"text": ["government mentality", "dirt quality", "town economies", "crop watering"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Crop rotation has a positive impact on what?", "arg_1": " government mentality"}, "gen_args_1": {"arg_0": "Crop rotation has a positive impact on what?", "arg_1": " dirt quality"}, "gen_args_2": {"arg_0": "Crop rotation has a positive impact on what?", "arg_1": " town economies"}, "gen_args_3": {"arg_0": "Crop rotation has a positive impact on what?", "arg_1": " crop watering"}}, "resps": [[["-39.0", "False"]], [["-25.375", "False"]], [["-33.5", "False"]], [["-34.25", "False"]]], "filtered_resps": [["-39.0", "False"], ["-25.375", "False"], ["-33.5", "False"], ["-34.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "ed1fb4ded5e91bcd78746e22bf8ccc91c26722d2b15182beb66dfe4af928b15a", "prompt_hash": "8c82746f5e62ae35b9970ed68fe91ba9c28ded89a8def433a6a2b08699ae142f", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 98, "doc": {"id": "9-179", "question_stem": "the best method for detecting texture is", "choices": {"text": ["rubbing it", "seeing it", "hearing it", "tasting it"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "the best method for detecting texture is", "arg_1": " rubbing it"}, "gen_args_1": {"arg_0": "the best method for detecting texture is", "arg_1": " seeing it"}, "gen_args_2": {"arg_0": "the best method for detecting texture is", "arg_1": " hearing it"}, "gen_args_3": {"arg_0": "the best method for detecting texture is", "arg_1": " tasting it"}}, "resps": [[["-16.0", "False"]], [["-13.75", "False"]], [["-18.125", "False"]], [["-23.125", "False"]]], "filtered_resps": [["-16.0", "False"], ["-13.75", "False"], ["-18.125", "False"], ["-23.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "29b0445560e33926a1096c893668be9e208cacf28f46f50e12fed8db949ed1f2", "prompt_hash": "753759e23f58108d0ac7f1ac03d5d246c819abc8e240d725fc42e58b1dd43a8f", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 99, "doc": {"id": "7-942", "question_stem": "Cold-blooded animals are often", "choices": {"text": ["fast", "large", "hairless", "slow"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Cold-blooded animals are often", "arg_1": " fast"}, "gen_args_1": {"arg_0": "Cold-blooded animals are often", "arg_1": " large"}, "gen_args_2": {"arg_0": "Cold-blooded animals are often", "arg_1": " hairless"}, "gen_args_3": {"arg_0": "Cold-blooded animals are often", "arg_1": " slow"}}, "resps": [[["-9.0", "False"]], [["-6.6875", "False"]], [["-16.375", "False"]], [["-4.5625", "False"]]], "filtered_resps": [["-9.0", "False"], ["-6.6875", "False"], ["-16.375", "False"], ["-4.5625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "3e4a244521e69901d5108b45da72d42730bad8d28896842f6e8a8fb95d7ac324", "prompt_hash": "f98eb6877bea9e6744753452512978d9bbd9485f92293ba3f257d19d86d80e4d", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 100, "doc": {"id": "7-100", "question_stem": "Grey clouds can bring", "choices": {"text": ["sunlight", "falling water molecules", "blooming flowers", "drought conditions"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Grey clouds can bring", "arg_1": " sunlight"}, "gen_args_1": {"arg_0": "Grey clouds can bring", "arg_1": " falling water molecules"}, "gen_args_2": {"arg_0": "Grey clouds can bring", "arg_1": " blooming flowers"}, "gen_args_3": {"arg_0": "Grey clouds can bring", "arg_1": " drought conditions"}}, "resps": [[["-13.5", "False"]], [["-31.5", "False"]], [["-15.625", "False"]], [["-11.9375", "False"]]], "filtered_resps": [["-13.5", "False"], ["-31.5", "False"], ["-15.625", "False"], ["-11.9375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "bfd962d0ad56661ebb9043f39008b82a4f6e2c133316e08bf1ad6eb563b7b844", "prompt_hash": "4b4d8e59422dcf88aabde70e9dbcd11e9344b0e66cb180d49f1a73dad83b396d", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 101, "doc": {"id": "9-30", "question_stem": "Which animal is considered a predator?", "choices": {"text": ["ant", "snake", "elephant", "giraffe"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Which animal is considered a predator?", "arg_1": " ant"}, "gen_args_1": {"arg_0": "Which animal is considered a predator?", "arg_1": " snake"}, "gen_args_2": {"arg_0": "Which animal is considered a predator?", "arg_1": " elephant"}, "gen_args_3": {"arg_0": "Which animal is considered a predator?", "arg_1": " giraffe"}}, "resps": [[["-18.625", "False"]], [["-14.9375", "False"]], [["-18.0", "False"]], [["-18.25", "False"]]], "filtered_resps": [["-18.625", "False"], ["-14.9375", "False"], ["-18.0", "False"], ["-18.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "169c26e27205de324d107679b60e78743f2aefe656d89f2f01ae721fdbe4b891", "prompt_hash": "594e4d032e4132a1a4170bddf792935edc62936ce262cd5142d73404119236cc", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 102, "doc": {"id": "1709", "question_stem": "Pollinators", "choices": {"text": ["enable plants to continue flourishing", "play an unimportant role in the reproduction process", "are useless to plants", "are considered unwanted pests"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Pollinators", "arg_1": " enable plants to continue flourishing"}, "gen_args_1": {"arg_0": "Pollinators", "arg_1": " play an unimportant role in the reproduction process"}, "gen_args_2": {"arg_0": "Pollinators", "arg_1": " are useless to plants"}, "gen_args_3": {"arg_0": "Pollinators", "arg_1": " are considered unwanted pests"}}, "resps": [[["-29.75", "False"]], [["-35.25", "False"]], [["-27.875", "False"]], [["-29.25", "False"]]], "filtered_resps": [["-29.75", "False"], ["-35.25", "False"], ["-27.875", "False"], ["-29.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4350ad07f8650a83a4449edc53caeb4cf82f852f219fa9f34de23c57d8a23500", "prompt_hash": "953a00f716a6bc8347cb7e5089093b44ce7c1e906d42048578c660b23b1cfba6", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 103, "doc": {"id": "8-491", "question_stem": "A farmer harvests seeds from some plants, such as tomatoes, in order to plant them later on. These seeds, once planted", "choices": {"text": ["have their own dirt", "have their own sunlight", "have a lot of sand", "contain their necessary nutrition"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "A farmer harvests seeds from some plants, such as tomatoes, in order to plant them later on. These seeds, once planted", "arg_1": " have their own dirt"}, "gen_args_1": {"arg_0": "A farmer harvests seeds from some plants, such as tomatoes, in order to plant them later on. These seeds, once planted", "arg_1": " have their own sunlight"}, "gen_args_2": {"arg_0": "A farmer harvests seeds from some plants, such as tomatoes, in order to plant them later on. These seeds, once planted", "arg_1": " have a lot of sand"}, "gen_args_3": {"arg_0": "A farmer harvests seeds from some plants, such as tomatoes, in order to plant them later on. These seeds, once planted", "arg_1": " contain their necessary nutrition"}}, "resps": [[["-28.0", "False"]], [["-25.0", "False"]], [["-30.25", "False"]], [["-32.75", "False"]]], "filtered_resps": [["-28.0", "False"], ["-25.0", "False"], ["-30.25", "False"], ["-32.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a75fcc561daf23a4f7bcc07e72eaccf88a3a97e3e5ab3e93ac24c5452bdf728a", "prompt_hash": "819a8aee811a127ace91689ac5c7c6e1406f76d622fff1a17aae6c1109cd69dd", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 104, "doc": {"id": "44", "question_stem": "What type of characteristics are people not born with?", "choices": {"text": ["genetics", "skills", "physical attributes", "height"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "What type of characteristics are people not born with?", "arg_1": " genetics"}, "gen_args_1": {"arg_0": "What type of characteristics are people not born with?", "arg_1": " skills"}, "gen_args_2": {"arg_0": "What type of characteristics are people not born with?", "arg_1": " physical attributes"}, "gen_args_3": {"arg_0": "What type of characteristics are people not born with?", "arg_1": " height"}}, "resps": [[["-14.5", "False"]], [["-13.875", "False"]], [["-16.75", "False"]], [["-18.875", "False"]]], "filtered_resps": [["-14.5", "False"], ["-13.875", "False"], ["-16.75", "False"], ["-18.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a0c9d3f6f08c0fa79d1927503939bd2f2e58d583876967ce4e8f0b9d8c52348e", "prompt_hash": "4bf612b890dd2991639e36a61ecfaf851692d7cb626f531cdcfa38efd323cb6f", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 105, "doc": {"id": "1023", "question_stem": "A spinning object is used to make", "choices": {"text": ["steam", "heat", "water", "electricity"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "A spinning object is used to make", "arg_1": " steam"}, "gen_args_1": {"arg_0": "A spinning object is used to make", "arg_1": " heat"}, "gen_args_2": {"arg_0": "A spinning object is used to make", "arg_1": " water"}, "gen_args_3": {"arg_0": "A spinning object is used to make", "arg_1": " electricity"}}, "resps": [[["-11.3125", "False"]], [["-10.3125", "False"]], [["-7.9375", "False"]], [["-9.875", "False"]]], "filtered_resps": [["-11.3125", "False"], ["-10.3125", "False"], ["-7.9375", "False"], ["-9.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "bf8ff8607cbe06510ad7f0ea7d023da088f0486bfa2d6b50840619ba7633c111", "prompt_hash": "596041b686998e9e434dfdcad03784b91114340c3469ca39de3df0c2d26f3a1d", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 106, "doc": {"id": "1911", "question_stem": "One of the negative consequences of offshore oil platforms is", "choices": {"text": ["evaporation of the surrounding water", "discharge of liquid petroleum in the surrounding sea", "improvement in the conditions of sea life", "increase in the birthrate of sea birds"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "One of the negative consequences of offshore oil platforms is", "arg_1": " evaporation of the surrounding water"}, "gen_args_1": {"arg_0": "One of the negative consequences of offshore oil platforms is", "arg_1": " discharge of liquid petroleum in the surrounding sea"}, "gen_args_2": {"arg_0": "One of the negative consequences of offshore oil platforms is", "arg_1": " improvement in the conditions of sea life"}, "gen_args_3": {"arg_0": "One of the negative consequences of offshore oil platforms is", "arg_1": " increase in the birthrate of sea birds"}}, "resps": [[["-21.625", "False"]], [["-35.5", "False"]], [["-28.75", "False"]], [["-37.5", "False"]]], "filtered_resps": [["-21.625", "False"], ["-35.5", "False"], ["-28.75", "False"], ["-37.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "98ad7abba62583b3f9e82f3f5740fb2f9b370797581def2504b93feefe6150c1", "prompt_hash": "64d94f88df9d2ccf72c26d71b99870667e92323d2f3ef45b64ab79869328df42", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 107, "doc": {"id": "429", "question_stem": "The unit of measure derived from French word millilitre is a unit used for measuring volume generally used for values between 1 and", "choices": {"text": ["1000", "250", "5000", "300"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "The unit of measure derived from French word millilitre is a unit used for measuring volume generally used for values between 1 and", "arg_1": " 1000"}, "gen_args_1": {"arg_0": "The unit of measure derived from French word millilitre is a unit used for measuring volume generally used for values between 1 and", "arg_1": " 250"}, "gen_args_2": {"arg_0": "The unit of measure derived from French word millilitre is a unit used for measuring volume generally used for values between 1 and", "arg_1": " 5000"}, "gen_args_3": {"arg_0": "The unit of measure derived from French word millilitre is a unit used for measuring volume generally used for values between 1 and", "arg_1": " 300"}}, "resps": [[["-0.1611328125", "True"]], [["-5.03125", "False"]], [["-9.0", "False"]], [["-6.25", "False"]]], "filtered_resps": [["-0.1611328125", "True"], ["-5.03125", "False"], ["-9.0", "False"], ["-6.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e0fc3597a359d8d4e8ab62e9835df68c19718ed7457ad39c0f3b650bfb033dae", "prompt_hash": "e93354a810b39bca05a902f7e1af5a86972773c205ca159ee76bb2e282136e7b", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 108, "doc": {"id": "8-49", "question_stem": "A man's child runs through the yard in the sprinklers, getting mud all over their feet. The child then runs around on the porch, tracking mud everywhere. While the mud is still wet, the man decides to clean off the porch by", "choices": {"text": ["getting a new child", "yelling at the mud", "asking the child to stop", "turning on the hose"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "A man's child runs through the yard in the sprinklers, getting mud all over their feet. The child then runs around on the porch, tracking mud everywhere. While the mud is still wet, the man decides to clean off the porch by", "arg_1": " getting a new child"}, "gen_args_1": {"arg_0": "A man's child runs through the yard in the sprinklers, getting mud all over their feet. The child then runs around on the porch, tracking mud everywhere. While the mud is still wet, the man decides to clean off the porch by", "arg_1": " yelling at the mud"}, "gen_args_2": {"arg_0": "A man's child runs through the yard in the sprinklers, getting mud all over their feet. The child then runs around on the porch, tracking mud everywhere. While the mud is still wet, the man decides to clean off the porch by", "arg_1": " asking the child to stop"}, "gen_args_3": {"arg_0": "A man's child runs through the yard in the sprinklers, getting mud all over their feet. The child then runs around on the porch, tracking mud everywhere. While the mud is still wet, the man decides to clean off the porch by", "arg_1": " turning on the hose"}}, "resps": [[["-23.875", "False"]], [["-19.375", "False"]], [["-15.3125", "False"]], [["-7.3125", "False"]]], "filtered_resps": [["-23.875", "False"], ["-19.375", "False"], ["-15.3125", "False"], ["-7.3125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d5dc236e3183f6cfe54ea07acafce700815eddaea3daa5627b5618de4957e567", "prompt_hash": "68b907c04b25634960e0ebef0ca418f7dd3527a85773da7c9c37e158ffe574a6", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 109, "doc": {"id": "520", "question_stem": "Earthquakes", "choices": {"text": ["only happen in California", "cause solar and lunar eclipses", "will break your vases", "make bridges much safer"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Earthquakes", "arg_1": " only happen in California"}, "gen_args_1": {"arg_0": "Earthquakes", "arg_1": " cause solar and lunar eclipses"}, "gen_args_2": {"arg_0": "Earthquakes", "arg_1": " will break your vases"}, "gen_args_3": {"arg_0": "Earthquakes", "arg_1": " make bridges much safer"}}, "resps": [[["-15.3125", "False"]], [["-35.0", "False"]], [["-33.0", "False"]], [["-34.5", "False"]]], "filtered_resps": [["-15.3125", "False"], ["-35.0", "False"], ["-33.0", "False"], ["-34.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "36e14924364c88e28a9b2388fef6d9d89602ff61aa64994f914c0f17d0dbdbfb", "prompt_hash": "d5db89539bd0882d75f56a4bce29454ef171359dc88514e2e4d897434fd95c58", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 110, "doc": {"id": "7-1128", "question_stem": "A seismograph can accurately describe", "choices": {"text": ["how rough the footing will be", "how bad the weather will be", "how stable the ground will be", "how shaky the horse will be"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A seismograph can accurately describe", "arg_1": " how rough the footing will be"}, "gen_args_1": {"arg_0": "A seismograph can accurately describe", "arg_1": " how bad the weather will be"}, "gen_args_2": {"arg_0": "A seismograph can accurately describe", "arg_1": " how stable the ground will be"}, "gen_args_3": {"arg_0": "A seismograph can accurately describe", "arg_1": " how shaky the horse will be"}}, "resps": [[["-32.75", "False"]], [["-28.625", "False"]], [["-23.625", "False"]], [["-39.75", "False"]]], "filtered_resps": [["-32.75", "False"], ["-28.625", "False"], ["-23.625", "False"], ["-39.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "6eac7c2b0898f3ad3531494d78ccbaedd318895ba44ac7821760c52c67f63a48", "prompt_hash": "4294462bcf922dcfc9b9c95daf6aec8e9e250bd7ecbc8b31bf1ca45f6bd970e8", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 111, "doc": {"id": "7-394", "question_stem": "Light from further away may appear to be less bright than other, closer sources, such as in which instance?", "choices": {"text": ["the sun is always bright", "the moon is brighter than stars", "the moon is brighter than a floodlight", "the sun is darker than the moon"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Light from further away may appear to be less bright than other, closer sources, such as in which instance?", "arg_1": " the sun is always bright"}, "gen_args_1": {"arg_0": "Light from further away may appear to be less bright than other, closer sources, such as in which instance?", "arg_1": " the moon is brighter than stars"}, "gen_args_2": {"arg_0": "Light from further away may appear to be less bright than other, closer sources, such as in which instance?", "arg_1": " the moon is brighter than a floodlight"}, "gen_args_3": {"arg_0": "Light from further away may appear to be less bright than other, closer sources, such as in which instance?", "arg_1": " the sun is darker than the moon"}}, "resps": [[["-22.125", "False"]], [["-27.625", "False"]], [["-37.0", "False"]], [["-28.0", "False"]]], "filtered_resps": [["-22.125", "False"], ["-27.625", "False"], ["-37.0", "False"], ["-28.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a5800e81e7ba29516c2c77b4c5b07b586d97ac6c1235cfbbd7f675f935d94ebe", "prompt_hash": "0fe24795af203cb010fb162d8e6221bea7a72babba6bcde4cceae4adde25e484", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 112, "doc": {"id": "9-1166", "question_stem": "A plant needs a specific climate to grow and", "choices": {"text": ["wither", "persist", "die", "decay"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "A plant needs a specific climate to grow and", "arg_1": " wither"}, "gen_args_1": {"arg_0": "A plant needs a specific climate to grow and", "arg_1": " persist"}, "gen_args_2": {"arg_0": "A plant needs a specific climate to grow and", "arg_1": " die"}, "gen_args_3": {"arg_0": "A plant needs a specific climate to grow and", "arg_1": " decay"}}, "resps": [[["-13.0", "False"]], [["-10.9375", "False"]], [["-12.0", "False"]], [["-16.125", "False"]]], "filtered_resps": [["-13.0", "False"], ["-10.9375", "False"], ["-12.0", "False"], ["-16.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "63b3daf22177ffa0dd44830b44d5e24f4e73cbe990ec8bd08e5451eef95f23ff", "prompt_hash": "e17c4ec436bc99f9413c97dfc2b6f5df00da109318c7dcdbcdecc4aaacad44b5", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 113, "doc": {"id": "7-884", "question_stem": "Banging on a drum causes", "choices": {"text": ["music to be loud", "music to be appealing", "reverberations to strike the eardrum", "concerts to sell out"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Banging on a drum causes", "arg_1": " music to be loud"}, "gen_args_1": {"arg_0": "Banging on a drum causes", "arg_1": " music to be appealing"}, "gen_args_2": {"arg_0": "Banging on a drum causes", "arg_1": " reverberations to strike the eardrum"}, "gen_args_3": {"arg_0": "Banging on a drum causes", "arg_1": " concerts to sell out"}}, "resps": [[["-14.375", "False"]], [["-19.5", "False"]], [["-42.25", "False"]], [["-21.0", "False"]]], "filtered_resps": [["-14.375", "False"], ["-19.5", "False"], ["-42.25", "False"], ["-21.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "670ddf306345620f0d9a0a19e76cc7d3bca5966a2b94f4a35e283ca18584a019", "prompt_hash": "9591328cab241b8d82e5a0d21bcb5a3ef6cdca654e8b931c2cb96d86caa230ed", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 114, "doc": {"id": "9-501", "question_stem": "What may have been formed by a volcano?", "choices": {"text": ["Mt. McKinley", "Lake Pontchartrain", "The great lakes", "Niagara Falls"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "What may have been formed by a volcano?", "arg_1": " Mt. McKinley"}, "gen_args_1": {"arg_0": "What may have been formed by a volcano?", "arg_1": " Lake Pontchartrain"}, "gen_args_2": {"arg_0": "What may have been formed by a volcano?", "arg_1": " The great lakes"}, "gen_args_3": {"arg_0": "What may have been formed by a volcano?", "arg_1": " Niagara Falls"}}, "resps": [[["-18.5", "False"]], [["-24.125", "False"]], [["-18.0", "False"]], [["-16.25", "False"]]], "filtered_resps": [["-18.5", "False"], ["-24.125", "False"], ["-18.0", "False"], ["-16.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f3c9d8506cde64dec5e6fb5b7da39a401faf47e5255efd30aa288bc78cb55c22", "prompt_hash": "6685b62005ed8ef569872513bea4ddd45a67b97d7b5cc229223f946b5421b03d", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 115, "doc": {"id": "9-757", "question_stem": "Humans, cats, dogs, and elephants are known as mammals because their kids are born alive. Non-mammalian babies are born", "choices": {"text": ["old", "dead", "in an egg", "big"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Humans, cats, dogs, and elephants are known as mammals because their kids are born alive. Non-mammalian babies are born", "arg_1": " old"}, "gen_args_1": {"arg_0": "Humans, cats, dogs, and elephants are known as mammals because their kids are born alive. Non-mammalian babies are born", "arg_1": " dead"}, "gen_args_2": {"arg_0": "Humans, cats, dogs, and elephants are known as mammals because their kids are born alive. Non-mammalian babies are born", "arg_1": " in an egg"}, "gen_args_3": {"arg_0": "Humans, cats, dogs, and elephants are known as mammals because their kids are born alive. Non-mammalian babies are born", "arg_1": " big"}}, "resps": [[["-8.6875", "False"]], [["-1.1328125", "True"]], [["-6.8125", "False"]], [["-15.0", "False"]]], "filtered_resps": [["-8.6875", "False"], ["-1.1328125", "True"], ["-6.8125", "False"], ["-15.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4a88f6d8901c673a5ced16039dd2813cb52b1194ee0bfedf609510fb2bc52ce8", "prompt_hash": "6b605a98c29f32959dfeb4933088f4680718c7e8eb5ca45404317cddd4924e56", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 116, "doc": {"id": "7-725", "question_stem": "Wind can cause", "choices": {"text": ["basements to flood due to weather", "small birds to kill large birds", "waterfalls to flow backwards", "stones to weather down to pebbles"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Wind can cause", "arg_1": " basements to flood due to weather"}, "gen_args_1": {"arg_0": "Wind can cause", "arg_1": " small birds to kill large birds"}, "gen_args_2": {"arg_0": "Wind can cause", "arg_1": " waterfalls to flow backwards"}, "gen_args_3": {"arg_0": "Wind can cause", "arg_1": " stones to weather down to pebbles"}}, "resps": [[["-34.25", "False"]], [["-33.25", "False"]], [["-20.0", "False"]], [["-33.5", "False"]]], "filtered_resps": [["-34.25", "False"], ["-33.25", "False"], ["-20.0", "False"], ["-33.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "887fcf04a089dd5fe99925d7b3d229088cfd6c286e12b0b284c987173fe5c8af", "prompt_hash": "38214f9c0a6eb1c2378a1f8f31ece909c9057ac1d6e0b010853628650785d4ac", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 117, "doc": {"id": "1300", "question_stem": "Inherited characteristics", "choices": {"text": ["include mice being able to navigate a maze", "include learning to sit on command", "include dolphins doing tricks for their trainers", "include spots on a ladybug"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Inherited characteristics", "arg_1": " include mice being able to navigate a maze"}, "gen_args_1": {"arg_0": "Inherited characteristics", "arg_1": " include learning to sit on command"}, "gen_args_2": {"arg_0": "Inherited characteristics", "arg_1": " include dolphins doing tricks for their trainers"}, "gen_args_3": {"arg_0": "Inherited characteristics", "arg_1": " include spots on a ladybug"}}, "resps": [[["-35.25", "False"]], [["-36.5", "False"]], [["-44.0", "False"]], [["-43.5", "False"]]], "filtered_resps": [["-35.25", "False"], ["-36.5", "False"], ["-44.0", "False"], ["-43.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4327e391f814858e9bd1410c92384f89913ee400b5f10e6e89a63c2e35e46e14", "prompt_hash": "e45c1404ef8d7af00dfca3dc0433577cf3b3c7d15b45fd921a2b1386a8909343", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 118, "doc": {"id": "9-230", "question_stem": "How could we determine approximately how far a bird is from the ground?", "choices": {"text": ["Measure the altitude of the bird using a reference point, such as a tall building.", "Identify the species of bird", "Ask the bird how high it was when it returns back to earth", "Measure the bird's mass"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "How could we determine approximately how far a bird is from the ground?", "arg_1": " Measure the altitude of the bird using a reference point, such as a tall building."}, "gen_args_1": {"arg_0": "How could we determine approximately how far a bird is from the ground?", "arg_1": " Identify the species of bird"}, "gen_args_2": {"arg_0": "How could we determine approximately how far a bird is from the ground?", "arg_1": " Ask the bird how high it was when it returns back to earth"}, "gen_args_3": {"arg_0": "How could we determine approximately how far a bird is from the ground?", "arg_1": " Measure the bird's mass"}}, "resps": [[["-40.0", "False"]], [["-28.75", "False"]], [["-63.0", "False"]], [["-24.25", "False"]]], "filtered_resps": [["-40.0", "False"], ["-28.75", "False"], ["-63.0", "False"], ["-24.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "0be4e5ab644337bd919e95bdac4e725c6fe4575100994f07b6eb4868f32a9e17", "prompt_hash": "0f6520c8d02bb6a7bd1afcd37ffdbd82f50d35acac8acbdf3ed1bf32512c36fa", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 119, "doc": {"id": "9-988", "question_stem": "Endangered pandas are sometimes", "choices": {"text": ["accidentally dropped into volcanoes", "confined to enclosures to be viewed by the public", "found eating corn in the middle of North America", "made into delicious rare steaks"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Endangered pandas are sometimes", "arg_1": " accidentally dropped into volcanoes"}, "gen_args_1": {"arg_0": "Endangered pandas are sometimes", "arg_1": " confined to enclosures to be viewed by the public"}, "gen_args_2": {"arg_0": "Endangered pandas are sometimes", "arg_1": " found eating corn in the middle of North America"}, "gen_args_3": {"arg_0": "Endangered pandas are sometimes", "arg_1": " made into delicious rare steaks"}}, "resps": [[["-26.125", "False"]], [["-32.75", "False"]], [["-40.5", "False"]], [["-39.0", "False"]]], "filtered_resps": [["-26.125", "False"], ["-32.75", "False"], ["-40.5", "False"], ["-39.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "37dee347d1198a497f7db5ab3bf070af4818a9562bb8587b386e16946178898f", "prompt_hash": "1d9b3f0fec03f39764f6228f7f45b15623df0f1fc6e3fbd04bb4b024c5064699", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 120, "doc": {"id": "9-393", "question_stem": "Algae can be found in", "choices": {"text": ["reservoir", "meat", "street", "tree"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Algae can be found in", "arg_1": " reservoir"}, "gen_args_1": {"arg_0": "Algae can be found in", "arg_1": " meat"}, "gen_args_2": {"arg_0": "Algae can be found in", "arg_1": " street"}, "gen_args_3": {"arg_0": "Algae can be found in", "arg_1": " tree"}}, "resps": [[["-13.0", "False"]], [["-13.3125", "False"]], [["-14.8125", "False"]], [["-14.8125", "False"]]], "filtered_resps": [["-13.0", "False"], ["-13.3125", "False"], ["-14.8125", "False"], ["-14.8125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "3befce96ea85b6add9845274ba9bf3871b71a5a9ece191079601e3f9fc98ed14", "prompt_hash": "3e4ae75282c145cf40e03be10d4477ca451eb94210eb31bae2c2ba0dda4c1c61", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 121, "doc": {"id": "7-823", "question_stem": "A toaster converts electrical energy into heat energy for toasting much like", "choices": {"text": ["a campfire toasts bread", "a microwave heats soup", "a fire burns paper", "a small oven works"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "A toaster converts electrical energy into heat energy for toasting much like", "arg_1": " a campfire toasts bread"}, "gen_args_1": {"arg_0": "A toaster converts electrical energy into heat energy for toasting much like", "arg_1": " a microwave heats soup"}, "gen_args_2": {"arg_0": "A toaster converts electrical energy into heat energy for toasting much like", "arg_1": " a fire burns paper"}, "gen_args_3": {"arg_0": "A toaster converts electrical energy into heat energy for toasting much like", "arg_1": " a small oven works"}}, "resps": [[["-26.5", "False"]], [["-18.125", "False"]], [["-23.0", "False"]], [["-11.4375", "False"]]], "filtered_resps": [["-26.5", "False"], ["-18.125", "False"], ["-23.0", "False"], ["-11.4375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "ba9e8eab1a59324902f53dc54af8a40837ae50ae33c8ddd9ea6fc3324757c9f9", "prompt_hash": "a75e56974faecc095f1f53005e28997885377ef07417ec4da7e9cc9744a1580d", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 122, "doc": {"id": "9-24", "question_stem": "An octopus, when in danger and unable to swim to safety, may find itself", "choices": {"text": ["mimicking other things", "melting into sand", "creating new homes", "mocking other fish"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "An octopus, when in danger and unable to swim to safety, may find itself", "arg_1": " mimicking other things"}, "gen_args_1": {"arg_0": "An octopus, when in danger and unable to swim to safety, may find itself", "arg_1": " melting into sand"}, "gen_args_2": {"arg_0": "An octopus, when in danger and unable to swim to safety, may find itself", "arg_1": " creating new homes"}, "gen_args_3": {"arg_0": "An octopus, when in danger and unable to swim to safety, may find itself", "arg_1": " mocking other fish"}}, "resps": [[["-20.875", "False"]], [["-20.75", "False"]], [["-23.75", "False"]], [["-26.25", "False"]]], "filtered_resps": [["-20.875", "False"], ["-20.75", "False"], ["-23.75", "False"], ["-26.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "fe5a66ac1238f854314e2f45e091c264209d0bf4eb55962d85fd36fd96e8da9f", "prompt_hash": "a3a55d0f8967aa162d2afcddea5422385154a3ed99340ef83c20523b38d292f6", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 123, "doc": {"id": "570", "question_stem": "What does someone do when creating music?", "choices": {"text": ["hit a toy baseball with a bat", "shake a baby rattle with your hand", "bang the wall with your fist", "pluck the strings of a fingerboard with your fingers"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "What does someone do when creating music?", "arg_1": " hit a toy baseball with a bat"}, "gen_args_1": {"arg_0": "What does someone do when creating music?", "arg_1": " shake a baby rattle with your hand"}, "gen_args_2": {"arg_0": "What does someone do when creating music?", "arg_1": " bang the wall with your fist"}, "gen_args_3": {"arg_0": "What does someone do when creating music?", "arg_1": " pluck the strings of a fingerboard with your fingers"}}, "resps": [[["-58.5", "False"]], [["-66.5", "False"]], [["-54.75", "False"]], [["-54.75", "False"]]], "filtered_resps": [["-58.5", "False"], ["-66.5", "False"], ["-54.75", "False"], ["-54.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "544b5a90fe1e9d92b9640026ec9f231f78ada554c649eeb9cc865d23b1a24f18", "prompt_hash": "52a739ce445e32eda1112d17fc0f65a93b02585e35e2d62c72bcda15f02d16d1", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 124, "doc": {"id": "9-124", "question_stem": "The pull the human planet space rock orbiter has on certain bodies of dihydrogen monooxide results in?", "choices": {"text": ["telescope views", "water level fluctuations", "animal", "plant harvesting"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "The pull the human planet space rock orbiter has on certain bodies of dihydrogen monooxide results in?", "arg_1": " telescope views"}, "gen_args_1": {"arg_0": "The pull the human planet space rock orbiter has on certain bodies of dihydrogen monooxide results in?", "arg_1": " water level fluctuations"}, "gen_args_2": {"arg_0": "The pull the human planet space rock orbiter has on certain bodies of dihydrogen monooxide results in?", "arg_1": " animal"}, "gen_args_3": {"arg_0": "The pull the human planet space rock orbiter has on certain bodies of dihydrogen monooxide results in?", "arg_1": " plant harvesting"}}, "resps": [[["-28.0", "False"]], [["-27.75", "False"]], [["-19.625", "False"]], [["-32.0", "False"]]], "filtered_resps": [["-28.0", "False"], ["-27.75", "False"], ["-19.625", "False"], ["-32.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e0d27c74e0c0755490fb0747528fbc6e0d6c38583cceabeff181d45a4158d0ce", "prompt_hash": "c6d6ba5070bd63f33e03986938d07b2a1668082266d8b80b584250a03226fa4e", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 125, "doc": {"id": "9-199", "question_stem": "Since density = mass / volume, denser liquids such as water sink more than", "choices": {"text": ["baby oil", "corn syrup or", "milk", "honey"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Since density = mass / volume, denser liquids such as water sink more than", "arg_1": " baby oil"}, "gen_args_1": {"arg_0": "Since density = mass / volume, denser liquids such as water sink more than", "arg_1": " corn syrup or"}, "gen_args_2": {"arg_0": "Since density = mass / volume, denser liquids such as water sink more than", "arg_1": " milk"}, "gen_args_3": {"arg_0": "Since density = mass / volume, denser liquids such as water sink more than", "arg_1": " honey"}}, "resps": [[["-29.25", "False"]], [["-22.25", "False"]], [["-19.25", "False"]], [["-15.6875", "False"]]], "filtered_resps": [["-29.25", "False"], ["-22.25", "False"], ["-19.25", "False"], ["-15.6875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "18b7d172158d2e7ac1c6c39c6c3bafd4c741fb4963c1849bda05ae3710d5f1b0", "prompt_hash": "f34e8d269f08e54b36562454d72cf264e92aee1492b2e9f571e94df0f9f28e9d", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 126, "doc": {"id": "767", "question_stem": "Photosynthesis can be performed by", "choices": {"text": ["a cabbage cell", "a bee cell", "a bear cell", "a cat cell"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Photosynthesis can be performed by", "arg_1": " a cabbage cell"}, "gen_args_1": {"arg_0": "Photosynthesis can be performed by", "arg_1": " a bee cell"}, "gen_args_2": {"arg_0": "Photosynthesis can be performed by", "arg_1": " a bear cell"}, "gen_args_3": {"arg_0": "Photosynthesis can be performed by", "arg_1": " a cat cell"}}, "resps": [[["-26.875", "False"]], [["-30.0", "False"]], [["-33.75", "False"]], [["-28.375", "False"]]], "filtered_resps": [["-26.875", "False"], ["-30.0", "False"], ["-33.75", "False"], ["-28.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "63a8c769cf567975393314551a94e2dc79fad8c81e6e923edd308470b22db2da", "prompt_hash": "8f29e2826065ad5d2f94fe8efe07d37d9d7b9ae9284d2935300415b8eae231ec", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 127, "doc": {"id": "28", "question_stem": "The force exerted on an object and distance traveled have what kind of relationship?", "choices": {"text": ["reverse", "inverse", "equal", "direct"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "The force exerted on an object and distance traveled have what kind of relationship?", "arg_1": " reverse"}, "gen_args_1": {"arg_0": "The force exerted on an object and distance traveled have what kind of relationship?", "arg_1": " inverse"}, "gen_args_2": {"arg_0": "The force exerted on an object and distance traveled have what kind of relationship?", "arg_1": " equal"}, "gen_args_3": {"arg_0": "The force exerted on an object and distance traveled have what kind of relationship?", "arg_1": " direct"}}, "resps": [[["-19.0", "False"]], [["-13.8125", "False"]], [["-16.875", "False"]], [["-16.75", "False"]]], "filtered_resps": [["-19.0", "False"], ["-13.8125", "False"], ["-16.875", "False"], ["-16.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "913d43092a576d0cfabe74b1c2fa8ff00ce3689b2cbbb8c3af62d9259f83aa25", "prompt_hash": "e1b309dd00ce6d4ad15d4c4b12787c8e7099d5eda51de5e212f600981153339c", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 128, "doc": {"id": "9-1134", "question_stem": "all cells use cellular respiration to", "choices": {"text": ["photosynthesize", "release waste", "perform meiosis", "release energy"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "all cells use cellular respiration to", "arg_1": " photosynthesize"}, "gen_args_1": {"arg_0": "all cells use cellular respiration to", "arg_1": " release waste"}, "gen_args_2": {"arg_0": "all cells use cellular respiration to", "arg_1": " perform meiosis"}, "gen_args_3": {"arg_0": "all cells use cellular respiration to", "arg_1": " release energy"}}, "resps": [[["-11.125", "False"]], [["-16.125", "False"]], [["-19.375", "False"]], [["-7.0", "False"]]], "filtered_resps": [["-11.125", "False"], ["-16.125", "False"], ["-19.375", "False"], ["-7.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "5f294b292363c9aa2d877d63b9cbf203c340e1aed1f4688004423197f953a60a", "prompt_hash": "463a7bcb3282b211fee0fb41aa859871371f534cbcacddab95f3d47d84be8d8d", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 129, "doc": {"id": "9-1030", "question_stem": "The viewing oriented sensor of a prairie creature are for what?", "choices": {"text": ["reproductive purposes", "viewing sounds", "sensing views", "sensing tastes"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "The viewing oriented sensor of a prairie creature are for what?", "arg_1": " reproductive purposes"}, "gen_args_1": {"arg_0": "The viewing oriented sensor of a prairie creature are for what?", "arg_1": " viewing sounds"}, "gen_args_2": {"arg_0": "The viewing oriented sensor of a prairie creature are for what?", "arg_1": " sensing views"}, "gen_args_3": {"arg_0": "The viewing oriented sensor of a prairie creature are for what?", "arg_1": " sensing tastes"}}, "resps": [[["-17.25", "False"]], [["-29.25", "False"]], [["-27.5", "False"]], [["-26.375", "False"]]], "filtered_resps": [["-17.25", "False"], ["-29.25", "False"], ["-27.5", "False"], ["-26.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "c7ef4ff01b7c36708e15e8fb53b1384ddb3eb76a5873f913e8eab2f10fa2bed1", "prompt_hash": "2d441f84792b7cd3b0f31646e760dd4afd8044964d0f44031f3cf88867121af2", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 130, "doc": {"id": "9-18", "question_stem": "A dog is going to have to corral sheep for the afternoon, so it needs to prepare its body for the enormous workload ahead of it. The dog is", "choices": {"text": ["breaks for birds on the road", "given a large breakfast", "eats a few corn cobs", "given two apples to watch"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "A dog is going to have to corral sheep for the afternoon, so it needs to prepare its body for the enormous workload ahead of it. The dog is", "arg_1": " breaks for birds on the road"}, "gen_args_1": {"arg_0": "A dog is going to have to corral sheep for the afternoon, so it needs to prepare its body for the enormous workload ahead of it. The dog is", "arg_1": " given a large breakfast"}, "gen_args_2": {"arg_0": "A dog is going to have to corral sheep for the afternoon, so it needs to prepare its body for the enormous workload ahead of it. The dog is", "arg_1": " eats a few corn cobs"}, "gen_args_3": {"arg_0": "A dog is going to have to corral sheep for the afternoon, so it needs to prepare its body for the enormous workload ahead of it. The dog is", "arg_1": " given two apples to watch"}}, "resps": [[["-46.75", "False"]], [["-19.375", "False"]], [["-31.25", "False"]], [["-29.375", "False"]]], "filtered_resps": [["-46.75", "False"], ["-19.375", "False"], ["-31.25", "False"], ["-29.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d316ea465276a050a46ab3d2f19065114c0c25dfbee4e1d25c604cc5ec442fc2", "prompt_hash": "dbfd49f5d64abbeec8cd1a26141810c11724a16778db4b3179eed2e843deb771", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 131, "doc": {"id": "8-378", "question_stem": "A woman sells bracelets that she makes. The bracelets gain popularity, and the woman makes incredibly large amounts of money from the sales. After a while, very few people are still buying the bracelets, so", "choices": {"text": ["the woman makes more money", "the woman makes the same amount of money", "the woman spends more money", "the woman makes less money"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "A woman sells bracelets that she makes. The bracelets gain popularity, and the woman makes incredibly large amounts of money from the sales. After a while, very few people are still buying the bracelets, so", "arg_1": " the woman makes more money"}, "gen_args_1": {"arg_0": "A woman sells bracelets that she makes. The bracelets gain popularity, and the woman makes incredibly large amounts of money from the sales. After a while, very few people are still buying the bracelets, so", "arg_1": " the woman makes the same amount of money"}, "gen_args_2": {"arg_0": "A woman sells bracelets that she makes. The bracelets gain popularity, and the woman makes incredibly large amounts of money from the sales. After a while, very few people are still buying the bracelets, so", "arg_1": " the woman spends more money"}, "gen_args_3": {"arg_0": "A woman sells bracelets that she makes. The bracelets gain popularity, and the woman makes incredibly large amounts of money from the sales. After a while, very few people are still buying the bracelets, so", "arg_1": " the woman makes less money"}}, "resps": [[["-13.875", "False"]], [["-14.25", "False"]], [["-13.3125", "False"]], [["-10.9375", "False"]]], "filtered_resps": [["-13.875", "False"], ["-14.25", "False"], ["-13.3125", "False"], ["-10.9375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "baade31ddb6e828c9b5740f42893cd9e6b28de189a84fbfca921f5afb50b76e7", "prompt_hash": "cc78c27d29d476ef8cfe24260fed40d926c4f1fa5a62ac0ad8f1b3844cc766ad", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 132, "doc": {"id": "7-677", "question_stem": "A desert environment is", "choices": {"text": ["dry, grass covered, and humid", "lush, green, and tropical", "arid, parched, and sun-baked", "dry, damp, and lush"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A desert environment is", "arg_1": " dry, grass covered, and humid"}, "gen_args_1": {"arg_0": "A desert environment is", "arg_1": " lush, green, and tropical"}, "gen_args_2": {"arg_0": "A desert environment is", "arg_1": " arid, parched, and sun-baked"}, "gen_args_3": {"arg_0": "A desert environment is", "arg_1": " dry, damp, and lush"}}, "resps": [[["-43.5", "False"]], [["-22.875", "False"]], [["-28.625", "False"]], [["-36.0", "False"]]], "filtered_resps": [["-43.5", "False"], ["-22.875", "False"], ["-28.625", "False"], ["-36.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "36bdb459f80c111287e14cd50b921db355b1417ab7f9fdaff1a51c22aee11d03", "prompt_hash": "caf17191fa956babfc17830b49598c203599f540c3dcf5aa481a78d7335f3cb0", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 133, "doc": {"id": "9-786", "question_stem": "Seasons are caused by what rotating on its axis?", "choices": {"text": ["Our Planet", "The Atmosphere", "The Equator", "The Sun"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Seasons are caused by what rotating on its axis?", "arg_1": " Our Planet"}, "gen_args_1": {"arg_0": "Seasons are caused by what rotating on its axis?", "arg_1": " The Atmosphere"}, "gen_args_2": {"arg_0": "Seasons are caused by what rotating on its axis?", "arg_1": " The Equator"}, "gen_args_3": {"arg_0": "Seasons are caused by what rotating on its axis?", "arg_1": " The Sun"}}, "resps": [[["-16.625", "False"]], [["-18.75", "False"]], [["-15.6875", "False"]], [["-10.125", "False"]]], "filtered_resps": [["-16.625", "False"], ["-18.75", "False"], ["-15.6875", "False"], ["-10.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "9248c0296560f2aa1491be5737d2ddb82d617a31b89372074e19498fafae0350", "prompt_hash": "feff6712f70be87c525240b8f1e68581d7a3f377c16208ac5a2fd2b3524c2f65", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 134, "doc": {"id": "9-463", "question_stem": "Which is best an letting electricity pass through?", "choices": {"text": ["tile flooring", "human flesh", "hockey stick", "a steak knife"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Which is best an letting electricity pass through?", "arg_1": " tile flooring"}, "gen_args_1": {"arg_0": "Which is best an letting electricity pass through?", "arg_1": " human flesh"}, "gen_args_2": {"arg_0": "Which is best an letting electricity pass through?", "arg_1": " hockey stick"}, "gen_args_3": {"arg_0": "Which is best an letting electricity pass through?", "arg_1": " a steak knife"}}, "resps": [[["-20.0", "False"]], [["-19.875", "False"]], [["-18.125", "False"]], [["-24.375", "False"]]], "filtered_resps": [["-20.0", "False"], ["-19.875", "False"], ["-18.125", "False"], ["-24.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f09e4c1e32a59c627a3503d48eef8a353c6693fb1bfff6afc66a9abc1438cc67", "prompt_hash": "d15ebc1a2f6aebbd4d6ee751b496d87378e39111b240c72b4fe34d84ac1f605a", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 135, "doc": {"id": "7-71", "question_stem": "Asteroids crashing on planets can leave behind", "choices": {"text": ["large, bowl-shaped cavities in the ground", "aliens and foreign foods", "small dents in the planet's core", "lakes filled with salty water"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Asteroids crashing on planets can leave behind", "arg_1": " large, bowl-shaped cavities in the ground"}, "gen_args_1": {"arg_0": "Asteroids crashing on planets can leave behind", "arg_1": " aliens and foreign foods"}, "gen_args_2": {"arg_0": "Asteroids crashing on planets can leave behind", "arg_1": " small dents in the planet's core"}, "gen_args_3": {"arg_0": "Asteroids crashing on planets can leave behind", "arg_1": " lakes filled with salty water"}}, "resps": [[["-33.5", "False"]], [["-39.75", "False"]], [["-34.75", "False"]], [["-27.75", "False"]]], "filtered_resps": [["-33.5", "False"], ["-39.75", "False"], ["-34.75", "False"], ["-27.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "22cab8cfde05af2550d61a24a52e6ea34c631668f5f2ba524a0d15ec3acc5c99", "prompt_hash": "11935f56ac0fdbc504da58e62e2a0f2fc0c79fa5618bc8d1e90cf2ef061f5783", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 136, "doc": {"id": "9-1053", "question_stem": "What is the formula of the substance which best helps plants grow", "choices": {"text": ["NH4", "C4H4", "CO2", "H2O"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "What is the formula of the substance which best helps plants grow", "arg_1": " NH4"}, "gen_args_1": {"arg_0": "What is the formula of the substance which best helps plants grow", "arg_1": " C4H4"}, "gen_args_2": {"arg_0": "What is the formula of the substance which best helps plants grow", "arg_1": " CO2"}, "gen_args_3": {"arg_0": "What is the formula of the substance which best helps plants grow", "arg_1": " H2O"}}, "resps": [[["-28.25", "False"]], [["-49.25", "False"]], [["-23.375", "False"]], [["-21.875", "False"]]], "filtered_resps": [["-28.25", "False"], ["-49.25", "False"], ["-23.375", "False"], ["-21.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "34f84fdbd7a0972537ca294edc5716068e2d4d0dfd886b8a908fa76eec285edf", "prompt_hash": "da3c6630dabb2837cb0c1ff68b653cdf406fa66a7fe046eac6e26aa8924ee7af", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 137, "doc": {"id": "9-437", "question_stem": "You can experience a change of pressure when", "choices": {"text": ["Yelling really loud", "Soaring the skies", "Going walking", "riding a bike"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "You can experience a change of pressure when", "arg_1": " Yelling really loud"}, "gen_args_1": {"arg_0": "You can experience a change of pressure when", "arg_1": " Soaring the skies"}, "gen_args_2": {"arg_0": "You can experience a change of pressure when", "arg_1": " Going walking"}, "gen_args_3": {"arg_0": "You can experience a change of pressure when", "arg_1": " riding a bike"}}, "resps": [[["-31.0", "False"]], [["-28.875", "False"]], [["-27.0", "False"]], [["-9.1875", "False"]]], "filtered_resps": [["-31.0", "False"], ["-28.875", "False"], ["-27.0", "False"], ["-9.1875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d567ed83c2bfa5ae429fe1106b4b6e02132739c245d5b10a8c0530680f306276", "prompt_hash": "d95678837251995b4624788b624a2d73cdb6808715a6587ee1366ff7b33b2783", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 138, "doc": {"id": "1787", "question_stem": "A small creek absorbing heat energy can result in", "choices": {"text": ["the creek water getting colder", "a parched creek bed", "tributaries branching off from the creek", "a runoff of extra water"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "A small creek absorbing heat energy can result in", "arg_1": " the creek water getting colder"}, "gen_args_1": {"arg_0": "A small creek absorbing heat energy can result in", "arg_1": " a parched creek bed"}, "gen_args_2": {"arg_0": "A small creek absorbing heat energy can result in", "arg_1": " tributaries branching off from the creek"}, "gen_args_3": {"arg_0": "A small creek absorbing heat energy can result in", "arg_1": " a runoff of extra water"}}, "resps": [[["-23.25", "False"]], [["-22.125", "False"]], [["-24.125", "False"]], [["-20.625", "False"]]], "filtered_resps": [["-23.25", "False"], ["-22.125", "False"], ["-24.125", "False"], ["-20.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "bfae0e6beefb8c4788b0e497eebb0f03162d1b80e2a77e232e87bb7d707310d4", "prompt_hash": "c8122768cdef951c6ddef67a153e122b1d35533398b9d87691030d8c3cee89ff", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 139, "doc": {"id": "7-107", "question_stem": "Summertime  happens during June in all but which location?", "choices": {"text": ["Australia", "in Canada", "United States", "Europe"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Summertime  happens during June in all but which location?", "arg_1": " Australia"}, "gen_args_1": {"arg_0": "Summertime  happens during June in all but which location?", "arg_1": " in Canada"}, "gen_args_2": {"arg_0": "Summertime  happens during June in all but which location?", "arg_1": " United States"}, "gen_args_3": {"arg_0": "Summertime  happens during June in all but which location?", "arg_1": " Europe"}}, "resps": [[["-14.625", "False"]], [["-18.125", "False"]], [["-14.1875", "False"]], [["-14.1875", "False"]]], "filtered_resps": [["-14.625", "False"], ["-18.125", "False"], ["-14.1875", "False"], ["-14.1875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2e93c55cd7a41a6eac04063adcc16cd4d006bef2354299ac46b08cf686a25198", "prompt_hash": "e5df20d188a22e3f43c6db12110a11c391a2c42b074efb1f1eefa1d8e2e63731", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 140, "doc": {"id": "769", "question_stem": "It takes more water to fill a bathtub than a", "choices": {"text": ["lake", "pool", "stomach", "holding tank"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "It takes more water to fill a bathtub than a", "arg_1": " lake"}, "gen_args_1": {"arg_0": "It takes more water to fill a bathtub than a", "arg_1": " pool"}, "gen_args_2": {"arg_0": "It takes more water to fill a bathtub than a", "arg_1": " stomach"}, "gen_args_3": {"arg_0": "It takes more water to fill a bathtub than a", "arg_1": " holding tank"}}, "resps": [[["-6.625", "False"]], [["-4.75", "False"]], [["-11.6875", "False"]], [["-12.1875", "False"]]], "filtered_resps": [["-6.625", "False"], ["-4.75", "False"], ["-11.6875", "False"], ["-12.1875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "07b8482c615e60d717be0a1d6bc9423d413ba78175ff4f1ebf8fc15ac27e6f1e", "prompt_hash": "8b25aa1a88832932eb4aa0e4e9acdd82927c608e190652d38923f2918340cf4f", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 141, "doc": {"id": "9-73", "question_stem": "The eighth month of the year is winter in", "choices": {"text": ["Brazil", "Indiana", "London", "Canada"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "The eighth month of the year is winter in", "arg_1": " Brazil"}, "gen_args_1": {"arg_0": "The eighth month of the year is winter in", "arg_1": " Indiana"}, "gen_args_2": {"arg_0": "The eighth month of the year is winter in", "arg_1": " London"}, "gen_args_3": {"arg_0": "The eighth month of the year is winter in", "arg_1": " Canada"}}, "resps": [[["-7.1875", "False"]], [["-11.125", "False"]], [["-7.3125", "False"]], [["-7.1875", "False"]]], "filtered_resps": [["-7.1875", "False"], ["-11.125", "False"], ["-7.3125", "False"], ["-7.1875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "b2629294b0067e31ac9259104049fe92e497ef71871a2cb28b182e9edf630026", "prompt_hash": "78de397e5a0ed6f30ac37a15c1a119436f55d9cbe9a74a79b3ae26ec270f21e5", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 142, "doc": {"id": "9-1194", "question_stem": "birds use their peckers to catch", "choices": {"text": ["dogs", "a tan", "a ball", "bees"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "birds use their peckers to catch", "arg_1": " dogs"}, "gen_args_1": {"arg_0": "birds use their peckers to catch", "arg_1": " a tan"}, "gen_args_2": {"arg_0": "birds use their peckers to catch", "arg_1": " a ball"}, "gen_args_3": {"arg_0": "birds use their peckers to catch", "arg_1": " bees"}}, "resps": [[["-12.625", "False"]], [["-15.625", "False"]], [["-11.25", "False"]], [["-9.0", "False"]]], "filtered_resps": [["-12.625", "False"], ["-15.625", "False"], ["-11.25", "False"], ["-9.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a535819349c70a6501041d35976ac855912ad71b07878d646424d67eae46ba37", "prompt_hash": "40a9bcae7daab108e4aaca9a5a69843f5bfd1b27df27a436077fcd5c6e8cc86a", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 143, "doc": {"id": "9-416", "question_stem": "transplanting seedling oaks has a positive impact on", "choices": {"text": ["fuel costs", "the economy", "housing value", "the environment"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "transplanting seedling oaks has a positive impact on", "arg_1": " fuel costs"}, "gen_args_1": {"arg_0": "transplanting seedling oaks has a positive impact on", "arg_1": " the economy"}, "gen_args_2": {"arg_0": "transplanting seedling oaks has a positive impact on", "arg_1": " housing value"}, "gen_args_3": {"arg_0": "transplanting seedling oaks has a positive impact on", "arg_1": " the environment"}}, "resps": [[["-18.0", "False"]], [["-8.1875", "False"]], [["-10.75", "False"]], [["-2.609375", "True"]]], "filtered_resps": [["-18.0", "False"], ["-8.1875", "False"], ["-10.75", "False"], ["-2.609375", "True"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d9e1944bf1cfc6e9f8ba2497490c59129ad442f824efbde3970fef7f95b3c77b", "prompt_hash": "05ed5027da0b4ee97e42a8ebba42e5a1fa9d00de3957a6272f2385cf94164704", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 144, "doc": {"id": "470", "question_stem": "What is an example of hunting?", "choices": {"text": ["humans throwing a spear through an animal", "humans chewing on boiled animal muscles", "humans gathering animals in a gate", "humans plucking fruit from a tree"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "What is an example of hunting?", "arg_1": " humans throwing a spear through an animal"}, "gen_args_1": {"arg_0": "What is an example of hunting?", "arg_1": " humans chewing on boiled animal muscles"}, "gen_args_2": {"arg_0": "What is an example of hunting?", "arg_1": " humans gathering animals in a gate"}, "gen_args_3": {"arg_0": "What is an example of hunting?", "arg_1": " humans plucking fruit from a tree"}}, "resps": [[["-38.25", "False"]], [["-48.5", "False"]], [["-47.75", "False"]], [["-35.5", "False"]]], "filtered_resps": [["-38.25", "False"], ["-48.5", "False"], ["-47.75", "False"], ["-35.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "9fb9fba593f07da9467b6318866b1548dc8a8f209bcad83df2beeda158487011", "prompt_hash": "b1842f59795e3dc8a9d46a2962caa393ddcf2ef535875361375ead4754c32b44", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 145, "doc": {"id": "1297", "question_stem": "Which of these has shape that changes depending on the container which it resides within?", "choices": {"text": ["paper", "wood", "stone", "orange juice"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Which of these has shape that changes depending on the container which it resides within?", "arg_1": " paper"}, "gen_args_1": {"arg_0": "Which of these has shape that changes depending on the container which it resides within?", "arg_1": " wood"}, "gen_args_2": {"arg_0": "Which of these has shape that changes depending on the container which it resides within?", "arg_1": " stone"}, "gen_args_3": {"arg_0": "Which of these has shape that changes depending on the container which it resides within?", "arg_1": " orange juice"}}, "resps": [[["-17.625", "False"]], [["-18.25", "False"]], [["-21.625", "False"]], [["-24.75", "False"]]], "filtered_resps": [["-17.625", "False"], ["-18.25", "False"], ["-21.625", "False"], ["-24.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "b38e4864bc5b5963a4339395b8a6abec012ad241f81088c18af7028836215d24", "prompt_hash": "91da07672c322f833ff6c149aeb62666c5bd1aa2738a0597f192f2cfbb7ccc81", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 146, "doc": {"id": "8-346", "question_stem": "What would be the flavor if you ate the item that fell and is thought to have hit Sir Issac Newton's head", "choices": {"text": ["Sweet", "Salty", "bitter", "sour"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "What would be the flavor if you ate the item that fell and is thought to have hit Sir Issac Newton's head", "arg_1": " Sweet"}, "gen_args_1": {"arg_0": "What would be the flavor if you ate the item that fell and is thought to have hit Sir Issac Newton's head", "arg_1": " Salty"}, "gen_args_2": {"arg_0": "What would be the flavor if you ate the item that fell and is thought to have hit Sir Issac Newton's head", "arg_1": " bitter"}, "gen_args_3": {"arg_0": "What would be the flavor if you ate the item that fell and is thought to have hit Sir Issac Newton's head", "arg_1": " sour"}}, "resps": [[["-22.375", "False"]], [["-22.25", "False"]], [["-19.75", "False"]], [["-18.0", "False"]]], "filtered_resps": [["-22.375", "False"], ["-22.25", "False"], ["-19.75", "False"], ["-18.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d6524f80fab251af04855d007e3d547532b29119f5d9540ed174fe27a84b9196", "prompt_hash": "7195ea0ce9413affbcabb7e403d65deaa95704e3d81aa2573e95f5962f2f646e", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 147, "doc": {"id": "7-807", "question_stem": "Earth's four layers are comprised mainly of", "choices": {"text": ["stone", "bacteria", "water", "air"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Earth's four layers are comprised mainly of", "arg_1": " stone"}, "gen_args_1": {"arg_0": "Earth's four layers are comprised mainly of", "arg_1": " bacteria"}, "gen_args_2": {"arg_0": "Earth's four layers are comprised mainly of", "arg_1": " water"}, "gen_args_3": {"arg_0": "Earth's four layers are comprised mainly of", "arg_1": " air"}}, "resps": [[["-7.21875", "False"]], [["-13.625", "False"]], [["-3.671875", "False"]], [["-5.15625", "False"]]], "filtered_resps": [["-7.21875", "False"], ["-13.625", "False"], ["-3.671875", "False"], ["-5.15625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "cf17cac461dbb332ec07faa0442c77b318464ccfe5a347b172e3278f0941580d", "prompt_hash": "314ca1d1abd522ddda44d416949ae42187392bb116267ecf87eaf86056acf1f2", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 148, "doc": {"id": "8-463", "question_stem": "if coffee sits in the fridge and loses its liquid form, what is that point known as?", "choices": {"text": ["the freezing point", "the prime point", "the boiling point", "the melting point"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "if coffee sits in the fridge and loses its liquid form, what is that point known as?", "arg_1": " the freezing point"}, "gen_args_1": {"arg_0": "if coffee sits in the fridge and loses its liquid form, what is that point known as?", "arg_1": " the prime point"}, "gen_args_2": {"arg_0": "if coffee sits in the fridge and loses its liquid form, what is that point known as?", "arg_1": " the boiling point"}, "gen_args_3": {"arg_0": "if coffee sits in the fridge and loses its liquid form, what is that point known as?", "arg_1": " the melting point"}}, "resps": [[["-13.25", "False"]], [["-26.875", "False"]], [["-14.0625", "False"]], [["-15.125", "False"]]], "filtered_resps": [["-13.25", "False"], ["-26.875", "False"], ["-14.0625", "False"], ["-15.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "3039cfac14db49778277d062b06c1f66f1a033517b7c13dffec70d9a288979c3", "prompt_hash": "b106c803ee6c094c15a3bc63fefbf7c46cf25a8a385db4ab489b0698d0596adf", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 149, "doc": {"id": "9-110", "question_stem": "Mammals give birth to live", "choices": {"text": ["children", "birds", "fish", "insects"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Mammals give birth to live", "arg_1": " children"}, "gen_args_1": {"arg_0": "Mammals give birth to live", "arg_1": " birds"}, "gen_args_2": {"arg_0": "Mammals give birth to live", "arg_1": " fish"}, "gen_args_3": {"arg_0": "Mammals give birth to live", "arg_1": " insects"}}, "resps": [[["-16.125", "False"]], [["-27.125", "False"]], [["-24.375", "False"]], [["-21.5", "False"]]], "filtered_resps": [["-16.125", "False"], ["-27.125", "False"], ["-24.375", "False"], ["-21.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "c2f2efb26ac3769626bb0d61c3382839697f254702dd8ba01b359d1621cae7a3", "prompt_hash": "8ef7bac8ba0527a2c3fd649e508a26a8bc656764f796ec5253ee8c5ae4af5973", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 150, "doc": {"id": "1611", "question_stem": "If a battery in an electromagnet is active, then what will happen to a nail in that electromagnet?", "choices": {"text": ["it loses its magnetization", "it loses its charge", "it may become magnetized", "it gains a charge"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "If a battery in an electromagnet is active, then what will happen to a nail in that electromagnet?", "arg_1": " it loses its magnetization"}, "gen_args_1": {"arg_0": "If a battery in an electromagnet is active, then what will happen to a nail in that electromagnet?", "arg_1": " it loses its charge"}, "gen_args_2": {"arg_0": "If a battery in an electromagnet is active, then what will happen to a nail in that electromagnet?", "arg_1": " it may become magnetized"}, "gen_args_3": {"arg_0": "If a battery in an electromagnet is active, then what will happen to a nail in that electromagnet?", "arg_1": " it gains a charge"}}, "resps": [[["-30.5", "False"]], [["-29.75", "False"]], [["-27.375", "False"]], [["-32.25", "False"]]], "filtered_resps": [["-30.5", "False"], ["-29.75", "False"], ["-27.375", "False"], ["-32.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "3ebff7325e7aa5ab7fcfaf5a37ea0d33f832a697509f40e8aad3fc8d31b74762", "prompt_hash": "b659accfed31c1e3a98c9b94ff4ac1bc0ddc31b3111a549f428784a6f2564f96", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 151, "doc": {"id": "9-942", "question_stem": "Which of these is a factor in the shape of a fern's seed?", "choices": {"text": ["luck", "humans", "gold", "inheritance"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Which of these is a factor in the shape of a fern's seed?", "arg_1": " luck"}, "gen_args_1": {"arg_0": "Which of these is a factor in the shape of a fern's seed?", "arg_1": " humans"}, "gen_args_2": {"arg_0": "Which of these is a factor in the shape of a fern's seed?", "arg_1": " gold"}, "gen_args_3": {"arg_0": "Which of these is a factor in the shape of a fern's seed?", "arg_1": " inheritance"}}, "resps": [[["-25.625", "False"]], [["-20.5", "False"]], [["-25.25", "False"]], [["-26.0", "False"]]], "filtered_resps": [["-25.625", "False"], ["-20.5", "False"], ["-25.25", "False"], ["-26.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "60eb0c06741fdf914e6a91a385ff1cad70bdad1dc8613a9b9869e0cbe5328fb7", "prompt_hash": "146ba08f0dee88de1f45fc8192ea9c40d87433958d01f7fa81f57d008910654c", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 152, "doc": {"id": "9-1102", "question_stem": "Which is recyclable?", "choices": {"text": ["An Elephant", "A school notebook", "A boat", "A lake"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Which is recyclable?", "arg_1": " An Elephant"}, "gen_args_1": {"arg_0": "Which is recyclable?", "arg_1": " A school notebook"}, "gen_args_2": {"arg_0": "Which is recyclable?", "arg_1": " A boat"}, "gen_args_3": {"arg_0": "Which is recyclable?", "arg_1": " A lake"}}, "resps": [[["-23.875", "False"]], [["-24.375", "False"]], [["-19.25", "False"]], [["-25.5", "False"]]], "filtered_resps": [["-23.875", "False"], ["-24.375", "False"], ["-19.25", "False"], ["-25.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "46f09378990af030a7d0985d0fb6ad22d4f1d287f08094e1b4c690eed7f44686", "prompt_hash": "83ede68b58e3715cd8f1b4908dbe2ce043b9728f0b6b2417b89b93dc8f108654", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 153, "doc": {"id": "9-774", "question_stem": "if the population in a habitat is on a steady decline, what condition is the habitat?", "choices": {"text": ["it is a place to emigrate from", "it is an ideal habitat", "it is an unsustainable habitat", "it is a thriving abode"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "if the population in a habitat is on a steady decline, what condition is the habitat?", "arg_1": " it is a place to emigrate from"}, "gen_args_1": {"arg_0": "if the population in a habitat is on a steady decline, what condition is the habitat?", "arg_1": " it is an ideal habitat"}, "gen_args_2": {"arg_0": "if the population in a habitat is on a steady decline, what condition is the habitat?", "arg_1": " it is an unsustainable habitat"}, "gen_args_3": {"arg_0": "if the population in a habitat is on a steady decline, what condition is the habitat?", "arg_1": " it is a thriving abode"}}, "resps": [[["-40.5", "False"]], [["-22.875", "False"]], [["-25.5", "False"]], [["-33.5", "False"]]], "filtered_resps": [["-40.5", "False"], ["-22.875", "False"], ["-25.5", "False"], ["-33.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "6b3d22776d6200d502ea73069198f5a230aa1f76cc291403e0ff3ebf26dd8bd3", "prompt_hash": "10cb08e2db1ae51050326f301f345ec298ad434fb030b90f0b3e0def425a15de", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 154, "doc": {"id": "8-333", "question_stem": "A teacher wants to show how to combine two substances together. The two things that he can use in order to mix them completely are", "choices": {"text": ["water and soda", "water and oil", "sand and rocks", "salt and bark"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "A teacher wants to show how to combine two substances together. The two things that he can use in order to mix them completely are", "arg_1": " water and soda"}, "gen_args_1": {"arg_0": "A teacher wants to show how to combine two substances together. The two things that he can use in order to mix them completely are", "arg_1": " water and oil"}, "gen_args_2": {"arg_0": "A teacher wants to show how to combine two substances together. The two things that he can use in order to mix them completely are", "arg_1": " sand and rocks"}, "gen_args_3": {"arg_0": "A teacher wants to show how to combine two substances together. The two things that he can use in order to mix them completely are", "arg_1": " salt and bark"}}, "resps": [[["-7.8125", "False"]], [["-8.625", "False"]], [["-15.3125", "False"]], [["-21.5", "False"]]], "filtered_resps": [["-7.8125", "False"], ["-8.625", "False"], ["-15.3125", "False"], ["-21.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "311bc35e55eccfe6b0c3654a40be8cbe8adab2496e133290070bacf71fdec78c", "prompt_hash": "9452386c41a80fbf6089f9003aaf8cb56d7076f588d1b2e2f76ff924a53d7eba", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 155, "doc": {"id": "9-573", "question_stem": "Medicine is used to cure but can cause people to have allergic reactions such as", "choices": {"text": ["spider bites", "vomiting", "placebo effect", "dance fever"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Medicine is used to cure but can cause people to have allergic reactions such as", "arg_1": " spider bites"}, "gen_args_1": {"arg_0": "Medicine is used to cure but can cause people to have allergic reactions such as", "arg_1": " vomiting"}, "gen_args_2": {"arg_0": "Medicine is used to cure but can cause people to have allergic reactions such as", "arg_1": " placebo effect"}, "gen_args_3": {"arg_0": "Medicine is used to cure but can cause people to have allergic reactions such as", "arg_1": " dance fever"}}, "resps": [[["-12.25", "False"]], [["-9.1875", "False"]], [["-13.0625", "False"]], [["-25.75", "False"]]], "filtered_resps": [["-12.25", "False"], ["-9.1875", "False"], ["-13.0625", "False"], ["-25.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "3edd3f53ed133ab8f76bddc807573f17d274c4fd8b4f85ff24daf4825fd4de96", "prompt_hash": "8c04cbe090902e09556b4a450e2a9ecaf175890a8e912f618dc6b70dcbde8fae", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 156, "doc": {"id": "1955", "question_stem": "Lightning may lead to", "choices": {"text": ["damage to local foliage", "rainbows across the sky", "growth of local flora", "firefighters getting the night off"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Lightning may lead to", "arg_1": " damage to local foliage"}, "gen_args_1": {"arg_0": "Lightning may lead to", "arg_1": " rainbows across the sky"}, "gen_args_2": {"arg_0": "Lightning may lead to", "arg_1": " growth of local flora"}, "gen_args_3": {"arg_0": "Lightning may lead to", "arg_1": " firefighters getting the night off"}}, "resps": [[["-25.125", "False"]], [["-21.125", "False"]], [["-23.625", "False"]], [["-31.0", "False"]]], "filtered_resps": [["-25.125", "False"], ["-21.125", "False"], ["-23.625", "False"], ["-31.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f334f0513f4c779c898b824236c635a29221d893bad59ac3b436a18777be2b41", "prompt_hash": "b9b4f9704fe074c37622a857059768e1cb8867bcdb280d5c69cf65ff09fe7c44", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 157, "doc": {"id": "8-45", "question_stem": "When would a nocturnal predator most likely hunt?", "choices": {"text": ["5 p.m.", "12 p.m.", "3 a.m.", "10 a.m."], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "When would a nocturnal predator most likely hunt?", "arg_1": " 5 p.m."}, "gen_args_1": {"arg_0": "When would a nocturnal predator most likely hunt?", "arg_1": " 12 p.m."}, "gen_args_2": {"arg_0": "When would a nocturnal predator most likely hunt?", "arg_1": " 3 a.m."}, "gen_args_3": {"arg_0": "When would a nocturnal predator most likely hunt?", "arg_1": " 10 a.m."}}, "resps": [[["-27.5", "False"]], [["-30.5", "False"]], [["-24.0", "False"]], [["-30.375", "False"]]], "filtered_resps": [["-27.5", "False"], ["-30.5", "False"], ["-24.0", "False"], ["-30.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4be674b1d6f4ef2abb1deae42c1d8e52712579c918e99a751c51134dcad8f32f", "prompt_hash": "a4b2709600861488e72e2e80ff48d90dc82d89665f7bf7bf7566fee63f2835a2", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 158, "doc": {"id": "9-674", "question_stem": "Where water be located in its gas form?", "choices": {"text": ["inside a disc golf driver", "inside of a brass pipe", "a mile up in the sky", "inside a leather baseball"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Where water be located in its gas form?", "arg_1": " inside a disc golf driver"}, "gen_args_1": {"arg_0": "Where water be located in its gas form?", "arg_1": " inside of a brass pipe"}, "gen_args_2": {"arg_0": "Where water be located in its gas form?", "arg_1": " a mile up in the sky"}, "gen_args_3": {"arg_0": "Where water be located in its gas form?", "arg_1": " inside a leather baseball"}}, "resps": [[["-43.5", "False"]], [["-29.125", "False"]], [["-32.5", "False"]], [["-41.0", "False"]]], "filtered_resps": [["-43.5", "False"], ["-29.125", "False"], ["-32.5", "False"], ["-41.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "1932b16433a1ed21595fb28f9c71e14ecb77246bb562b23b491ef49bcd845bce", "prompt_hash": "de9825fe0e6d4b731c856fd05e935ff97c03932eefdc87d2ced0644f3170522c", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 159, "doc": {"id": "898", "question_stem": "Green parts of a life form absorb", "choices": {"text": ["carbon dioxide", "light", "oxygen", "water"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Green parts of a life form absorb", "arg_1": " carbon dioxide"}, "gen_args_1": {"arg_0": "Green parts of a life form absorb", "arg_1": " light"}, "gen_args_2": {"arg_0": "Green parts of a life form absorb", "arg_1": " oxygen"}, "gen_args_3": {"arg_0": "Green parts of a life form absorb", "arg_1": " water"}}, "resps": [[["-2.171875", "False"]], [["-2.359375", "False"]], [["-7.5625", "False"]], [["-5.0", "False"]]], "filtered_resps": [["-2.171875", "False"], ["-2.359375", "False"], ["-7.5625", "False"], ["-5.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "c5ba83f32145bb3d03040ca95a6c81af6a6ba63130c4bbb5949d81398a7e173c", "prompt_hash": "7f5e9ab786cf5c672cb08307ef5725eef74a7d4cb8581cbc681cd83ae9ef04f9", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 160, "doc": {"id": "7-1159", "question_stem": "If a new species of predator joins a community", "choices": {"text": ["the new species will become herbivores", "prey will experience an increase in population", "prey will experience a drop in population", "the old species will die out"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "If a new species of predator joins a community", "arg_1": " the new species will become herbivores"}, "gen_args_1": {"arg_0": "If a new species of predator joins a community", "arg_1": " prey will experience an increase in population"}, "gen_args_2": {"arg_0": "If a new species of predator joins a community", "arg_1": " prey will experience a drop in population"}, "gen_args_3": {"arg_0": "If a new species of predator joins a community", "arg_1": " the old species will die out"}}, "resps": [[["-41.25", "False"]], [["-34.25", "False"]], [["-34.25", "False"]], [["-29.5", "False"]]], "filtered_resps": [["-41.25", "False"], ["-34.25", "False"], ["-34.25", "False"], ["-29.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "43d7e71c82418a27cacc12bd9868b17d27afffa9d2a0d764b4adf562c87c1edc", "prompt_hash": "fe2c44f03caf884c48ec60e117662f5cd6fc5d6b2227bc7a0764305895f43d15", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 161, "doc": {"id": "568", "question_stem": "Why would a perennial plant with an elongated stem a frequently used for lumber fall to the ground?", "choices": {"text": ["It's dead", "For water", "For food", "For sun"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Why would a perennial plant with an elongated stem a frequently used for lumber fall to the ground?", "arg_1": " It's dead"}, "gen_args_1": {"arg_0": "Why would a perennial plant with an elongated stem a frequently used for lumber fall to the ground?", "arg_1": " For water"}, "gen_args_2": {"arg_0": "Why would a perennial plant with an elongated stem a frequently used for lumber fall to the ground?", "arg_1": " For food"}, "gen_args_3": {"arg_0": "Why would a perennial plant with an elongated stem a frequently used for lumber fall to the ground?", "arg_1": " For sun"}}, "resps": [[["-20.25", "False"]], [["-22.5", "False"]], [["-23.5", "False"]], [["-25.0", "False"]]], "filtered_resps": [["-20.25", "False"], ["-22.5", "False"], ["-23.5", "False"], ["-25.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "52e7cb3424443d79ea6c11599d077091c7ae4272c59e3c847e5c726188b5ca8b", "prompt_hash": "731ddca185e5ef848d02654475d63f3056aeed73d61047ddc18fbb3afe57e271", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 162, "doc": {"id": "9-877", "question_stem": "I'm an animal with a white fur and a large fluffy tail that lives in arctic regions; what am I?", "choices": {"text": ["weasel", "snow fox", "wolf", "polar bear"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "I'm an animal with a white fur and a large fluffy tail that lives in arctic regions; what am I?", "arg_1": " weasel"}, "gen_args_1": {"arg_0": "I'm an animal with a white fur and a large fluffy tail that lives in arctic regions; what am I?", "arg_1": " snow fox"}, "gen_args_2": {"arg_0": "I'm an animal with a white fur and a large fluffy tail that lives in arctic regions; what am I?", "arg_1": " wolf"}, "gen_args_3": {"arg_0": "I'm an animal with a white fur and a large fluffy tail that lives in arctic regions; what am I?", "arg_1": " polar bear"}}, "resps": [[["-23.75", "False"]], [["-17.625", "False"]], [["-20.75", "False"]], [["-17.625", "False"]]], "filtered_resps": [["-23.75", "False"], ["-17.625", "False"], ["-20.75", "False"], ["-17.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "679b71cfa8059d87493790e36fd96b660db6c95725307126bb25a938cf8d53f4", "prompt_hash": "e9641a9072851617cc5b6e5ed4ea05448b9a2669b2959c32d3dee9f42550385b", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 163, "doc": {"id": "406", "question_stem": "Dairy is a source of", "choices": {"text": ["a vitamin that prevents blood loss", "a vitamin that treats amino acid deficiency", "a group of fat-soluble secosteroids", "a vitamin that helps treat liver problems"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Dairy is a source of", "arg_1": " a vitamin that prevents blood loss"}, "gen_args_1": {"arg_0": "Dairy is a source of", "arg_1": " a vitamin that treats amino acid deficiency"}, "gen_args_2": {"arg_0": "Dairy is a source of", "arg_1": " a group of fat-soluble secosteroids"}, "gen_args_3": {"arg_0": "Dairy is a source of", "arg_1": " a vitamin that helps treat liver problems"}}, "resps": [[["-32.25", "False"]], [["-39.25", "False"]], [["-39.75", "False"]], [["-31.75", "False"]]], "filtered_resps": [["-32.25", "False"], ["-39.25", "False"], ["-39.75", "False"], ["-31.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "5bba6c91035deb2caab2cac843663f27c2b95964338f377511fd07e03e566bee", "prompt_hash": "869a4de37554a29c2c7d53b3003bcf267ab713c0f31a24532ad46a9cf317ca91", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 164, "doc": {"id": "7-1132", "question_stem": "Atomic 26 is drawn to a device, it could be", "choices": {"text": ["magnetized", "Na", "compass", "K"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Atomic 26 is drawn to a device, it could be", "arg_1": " magnetized"}, "gen_args_1": {"arg_0": "Atomic 26 is drawn to a device, it could be", "arg_1": " Na"}, "gen_args_2": {"arg_0": "Atomic 26 is drawn to a device, it could be", "arg_1": " compass"}, "gen_args_3": {"arg_0": "Atomic 26 is drawn to a device, it could be", "arg_1": " K"}}, "resps": [[["-11.5625", "False"]], [["-20.0", "False"]], [["-14.125", "False"]], [["-13.25", "False"]]], "filtered_resps": [["-11.5625", "False"], ["-20.0", "False"], ["-14.125", "False"], ["-13.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "ffa501864b4bec3879030cfa2a662cb63e93840189f7678396061cc97ff4b908", "prompt_hash": "1da3b5421bdd6a7930fa5643c22db32566bbc8a3bdcdba4256f85f1b08acaa38", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 165, "doc": {"id": "7-479", "question_stem": "why do tadpoles change into frogs?", "choices": {"text": ["tadpoles change to different animals", "tadpoles are really just fish", "they are young frogs still growing", "none of these"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "why do tadpoles change into frogs?", "arg_1": " tadpoles change to different animals"}, "gen_args_1": {"arg_0": "why do tadpoles change into frogs?", "arg_1": " tadpoles are really just fish"}, "gen_args_2": {"arg_0": "why do tadpoles change into frogs?", "arg_1": " they are young frogs still growing"}, "gen_args_3": {"arg_0": "why do tadpoles change into frogs?", "arg_1": " none of these"}}, "resps": [[["-36.25", "False"]], [["-27.25", "False"]], [["-33.0", "False"]], [["-22.75", "False"]]], "filtered_resps": [["-36.25", "False"], ["-27.25", "False"], ["-33.0", "False"], ["-22.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e718e8e944257a4c652e9cc3edb2f348e12838226d5b1512b24c6fe012ce23be", "prompt_hash": "09ad404423f56a3c8483bb60c62653336f6492e0643194c691363c3c0847683f", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 166, "doc": {"id": "609", "question_stem": "What do cows eat?", "choices": {"text": ["Chickpeas", "Chocolate", "Steak", "Poultry"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "What do cows eat?", "arg_1": " Chickpeas"}, "gen_args_1": {"arg_0": "What do cows eat?", "arg_1": " Chocolate"}, "gen_args_2": {"arg_0": "What do cows eat?", "arg_1": " Steak"}, "gen_args_3": {"arg_0": "What do cows eat?", "arg_1": " Poultry"}}, "resps": [[["-21.25", "False"]], [["-18.5", "False"]], [["-18.625", "False"]], [["-17.5", "False"]]], "filtered_resps": [["-21.25", "False"], ["-18.5", "False"], ["-18.625", "False"], ["-17.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4a5a36ec5f25993437f29686d45bf70082f67f8db79f2b5778e1e3f48c0deae4", "prompt_hash": "194461de491f94fda819200769a96fb4dda0d9144602dd949463d7de45c81468", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 167, "doc": {"id": "1568", "question_stem": "Which object conducts electricity?", "choices": {"text": ["Window", "Rubik's Cube", "Ship Anchor", "Boulder"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Which object conducts electricity?", "arg_1": " Window"}, "gen_args_1": {"arg_0": "Which object conducts electricity?", "arg_1": " Rubik's Cube"}, "gen_args_2": {"arg_0": "Which object conducts electricity?", "arg_1": " Ship Anchor"}, "gen_args_3": {"arg_0": "Which object conducts electricity?", "arg_1": " Boulder"}}, "resps": [[["-19.875", "False"]], [["-27.5", "False"]], [["-33.25", "False"]], [["-21.5", "False"]]], "filtered_resps": [["-19.875", "False"], ["-27.5", "False"], ["-33.25", "False"], ["-21.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "b66377849de829cbbc420bd299dc421d11e21bbb8b2fff2b2c9d9416cc693393", "prompt_hash": "43c9e70b76f221508dcd1992cb154d8998d10d77e898fb7b2fd657d81805d5c5", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 168, "doc": {"id": "9-418", "question_stem": "Glucose travels", "choices": {"text": ["from roots to leaves of a daffodil", "from a rose's leaves to the atmosphere", "from a daisy's leaves into it's underground support system", "from the sun to a sunflower's buds"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Glucose travels", "arg_1": " from roots to leaves of a daffodil"}, "gen_args_1": {"arg_0": "Glucose travels", "arg_1": " from a rose's leaves to the atmosphere"}, "gen_args_2": {"arg_0": "Glucose travels", "arg_1": " from a daisy's leaves into it's underground support system"}, "gen_args_3": {"arg_0": "Glucose travels", "arg_1": " from the sun to a sunflower's buds"}}, "resps": [[["-43.25", "False"]], [["-47.5", "False"]], [["-69.0", "False"]], [["-55.5", "False"]]], "filtered_resps": [["-43.25", "False"], ["-47.5", "False"], ["-69.0", "False"], ["-55.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "8fa62d16fa5aa195f80ac1e38146491a8ba754628ce9eb2f07ac139bb01593b1", "prompt_hash": "0b1747b1330510c6e6d80a02dd492ae3ee758b4c70220919a5c61a1f64bc6076", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 169, "doc": {"id": "7-1050", "question_stem": "Evaporation", "choices": {"text": ["only happens in the summer", "is like nature's disappearing water trick", "is caused by snow", "involves the disappearance of sunlight"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Evaporation", "arg_1": " only happens in the summer"}, "gen_args_1": {"arg_0": "Evaporation", "arg_1": " is like nature's disappearing water trick"}, "gen_args_2": {"arg_0": "Evaporation", "arg_1": " is caused by snow"}, "gen_args_3": {"arg_0": "Evaporation", "arg_1": " involves the disappearance of sunlight"}}, "resps": [[["-19.75", "False"]], [["-38.0", "False"]], [["-23.0", "False"]], [["-33.25", "False"]]], "filtered_resps": [["-19.75", "False"], ["-38.0", "False"], ["-23.0", "False"], ["-33.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "0c6a144b29736b45c2cfda89b55b14ff97736edb5076baf00de2f4a2c80bbd45", "prompt_hash": "5bbc10ae437da3b951d1ab2308e7b6e8f7877adfccfb3b4967765ca8aa9e9c94", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 170, "doc": {"id": "9-510", "question_stem": "In order for crops to grow food safely, pesticides are used on them. When it floods, this causes t he what to be poisonous?", "choices": {"text": ["air", "Corn", "Runoff", "farmers"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "In order for crops to grow food safely, pesticides are used on them. When it floods, this causes t he what to be poisonous?", "arg_1": " air"}, "gen_args_1": {"arg_0": "In order for crops to grow food safely, pesticides are used on them. When it floods, this causes t he what to be poisonous?", "arg_1": " Corn"}, "gen_args_2": {"arg_0": "In order for crops to grow food safely, pesticides are used on them. When it floods, this causes t he what to be poisonous?", "arg_1": " Runoff"}, "gen_args_3": {"arg_0": "In order for crops to grow food safely, pesticides are used on them. When it floods, this causes t he what to be poisonous?", "arg_1": " farmers"}}, "resps": [[["-12.4375", "False"]], [["-13.4375", "False"]], [["-11.375", "False"]], [["-11.9375", "False"]]], "filtered_resps": [["-12.4375", "False"], ["-13.4375", "False"], ["-11.375", "False"], ["-11.9375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "13dafdc84a1c78086fd46ffbd06b2cffb773aa333f9938347c0be3254e54dad4", "prompt_hash": "600c77645c3a8d2c36607c980374d0c7b2ab2440cb3406a16d84f7b186571225", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 171, "doc": {"id": "9-519", "question_stem": "The boy was able to warm the fireplace without a lighter thanks to what?", "choices": {"text": ["friction", "metal", "wishing", "magic"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "The boy was able to warm the fireplace without a lighter thanks to what?", "arg_1": " friction"}, "gen_args_1": {"arg_0": "The boy was able to warm the fireplace without a lighter thanks to what?", "arg_1": " metal"}, "gen_args_2": {"arg_0": "The boy was able to warm the fireplace without a lighter thanks to what?", "arg_1": " wishing"}, "gen_args_3": {"arg_0": "The boy was able to warm the fireplace without a lighter thanks to what?", "arg_1": " magic"}}, "resps": [[["-17.625", "False"]], [["-17.5", "False"]], [["-24.75", "False"]], [["-13.75", "False"]]], "filtered_resps": [["-17.625", "False"], ["-17.5", "False"], ["-24.75", "False"], ["-13.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "be7da4686f42575c1d1f56baa6c7a32133f1445e14fb43ae2bc7876743865ba8", "prompt_hash": "29e50fd4da82c07dc0d3a8ebe009d629702c9e0c4a8023a2edc6992b9dd55abc", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 172, "doc": {"id": "9-637", "question_stem": "Where would a duck like to live?", "choices": {"text": ["the Sahara", "Antarctica", "the Appalachian mountains", "Death Valley"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Where would a duck like to live?", "arg_1": " the Sahara"}, "gen_args_1": {"arg_0": "Where would a duck like to live?", "arg_1": " Antarctica"}, "gen_args_2": {"arg_0": "Where would a duck like to live?", "arg_1": " the Appalachian mountains"}, "gen_args_3": {"arg_0": "Where would a duck like to live?", "arg_1": " Death Valley"}}, "resps": [[["-23.75", "False"]], [["-17.5", "False"]], [["-28.125", "False"]], [["-21.25", "False"]]], "filtered_resps": [["-23.75", "False"], ["-17.5", "False"], ["-28.125", "False"], ["-21.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "5640627a0d376ad9beaf091fa3a5affface78c1abaa7158dc3e2cf615967933b", "prompt_hash": "4715460dcc5fb16811bfa312cf0c825be78f9ecd9c6ba8747d80f0233d59d19d", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 173, "doc": {"id": "473", "question_stem": "What do tuna eat?", "choices": {"text": ["Atlantic menhaden", "Swedish fish", "gummy fish", "laminariales"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "What do tuna eat?", "arg_1": " Atlantic menhaden"}, "gen_args_1": {"arg_0": "What do tuna eat?", "arg_1": " Swedish fish"}, "gen_args_2": {"arg_0": "What do tuna eat?", "arg_1": " gummy fish"}, "gen_args_3": {"arg_0": "What do tuna eat?", "arg_1": " laminariales"}}, "resps": [[["-28.375", "False"]], [["-20.625", "False"]], [["-24.375", "False"]], [["-41.0", "False"]]], "filtered_resps": [["-28.375", "False"], ["-20.625", "False"], ["-24.375", "False"], ["-41.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "cbc71017c3de380363aa54f90a390be76215bf67e93552d235434c361ddb192e", "prompt_hash": "355646ec265353593522d47ccc90fe862e66ba29d812738247622423d3bc68d7", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 174, "doc": {"id": "8-445", "question_stem": "A woman notices that she is depressed every autumn, and wonders why. A friend suggests to her that perhaps certain changes that take place as seasons move from warm to cold may be having an effect on her. When pressed for an example of these changes, the friend cites", "choices": {"text": ["flowers blooming", "grass turning brown", "trees growing", "blossoms blooming"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "A woman notices that she is depressed every autumn, and wonders why. A friend suggests to her that perhaps certain changes that take place as seasons move from warm to cold may be having an effect on her. When pressed for an example of these changes, the friend cites", "arg_1": " flowers blooming"}, "gen_args_1": {"arg_0": "A woman notices that she is depressed every autumn, and wonders why. A friend suggests to her that perhaps certain changes that take place as seasons move from warm to cold may be having an effect on her. When pressed for an example of these changes, the friend cites", "arg_1": " grass turning brown"}, "gen_args_2": {"arg_0": "A woman notices that she is depressed every autumn, and wonders why. A friend suggests to her that perhaps certain changes that take place as seasons move from warm to cold may be having an effect on her. When pressed for an example of these changes, the friend cites", "arg_1": " trees growing"}, "gen_args_3": {"arg_0": "A woman notices that she is depressed every autumn, and wonders why. A friend suggests to her that perhaps certain changes that take place as seasons move from warm to cold may be having an effect on her. When pressed for an example of these changes, the friend cites", "arg_1": " blossoms blooming"}}, "resps": [[["-12.0", "False"]], [["-14.4375", "False"]], [["-20.375", "False"]], [["-21.875", "False"]]], "filtered_resps": [["-12.0", "False"], ["-14.4375", "False"], ["-20.375", "False"], ["-21.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e1be01921a010101d026ef04a34cc49b331e07868dc5391141c39c2a3efd1df6", "prompt_hash": "9a0905491130217b2c4a8fd98f895a501ba06a4683aa78aef0c16f61e0676dd4", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 175, "doc": {"id": "9-575", "question_stem": "is it normal for an adult animal to lay eggs?", "choices": {"text": ["it has never happened", "yes it is standard", "it is abnormal and weird", "all of these"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "is it normal for an adult animal to lay eggs?", "arg_1": " it has never happened"}, "gen_args_1": {"arg_0": "is it normal for an adult animal to lay eggs?", "arg_1": " yes it is standard"}, "gen_args_2": {"arg_0": "is it normal for an adult animal to lay eggs?", "arg_1": " it is abnormal and weird"}, "gen_args_3": {"arg_0": "is it normal for an adult animal to lay eggs?", "arg_1": " all of these"}}, "resps": [[["-20.875", "False"]], [["-26.5", "False"]], [["-36.0", "False"]], [["-20.375", "False"]]], "filtered_resps": [["-20.875", "False"], ["-26.5", "False"], ["-36.0", "False"], ["-20.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "b5bf9bee2cb4990391f0c3ae6d7c43b704194a175cb76153d0cc7f84fcca9dd1", "prompt_hash": "a57b868d67426417fe14f7ccca34b9280e53e6429c4830fadaabe89d9bd82404", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 176, "doc": {"id": "7-284", "question_stem": "If your dog is overweight", "choices": {"text": ["add more fat to their diet", "cut back their caloric intake", "let them sleep more", "increase their caloric intake"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "If your dog is overweight", "arg_1": " add more fat to their diet"}, "gen_args_1": {"arg_0": "If your dog is overweight", "arg_1": " cut back their caloric intake"}, "gen_args_2": {"arg_0": "If your dog is overweight", "arg_1": " let them sleep more"}, "gen_args_3": {"arg_0": "If your dog is overweight", "arg_1": " increase their caloric intake"}}, "resps": [[["-28.875", "False"]], [["-29.125", "False"]], [["-31.125", "False"]], [["-22.125", "False"]]], "filtered_resps": [["-28.875", "False"], ["-29.125", "False"], ["-31.125", "False"], ["-22.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2e30b35e536d5970b85ac8c37b264b7d03d9791dab300db7a1e2c9bdc6053a3d", "prompt_hash": "af66ae23f3cc593e42010c2b6031fe8b15577cbd882784dd4cc5e5308b1123ae", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 177, "doc": {"id": "8-135", "question_stem": "Eyes allow humans", "choices": {"text": ["to detect when a traffic light changes", "detect sour flavors in candy", "hear music at concerts", "detect acrid odors in the air"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Eyes allow humans", "arg_1": " to detect when a traffic light changes"}, "gen_args_1": {"arg_0": "Eyes allow humans", "arg_1": " detect sour flavors in candy"}, "gen_args_2": {"arg_0": "Eyes allow humans", "arg_1": " hear music at concerts"}, "gen_args_3": {"arg_0": "Eyes allow humans", "arg_1": " detect acrid odors in the air"}}, "resps": [[["-32.0", "False"]], [["-48.0", "False"]], [["-35.75", "False"]], [["-36.5", "False"]]], "filtered_resps": [["-32.0", "False"], ["-48.0", "False"], ["-35.75", "False"], ["-36.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "aca0e6203cc4f4217d7d6e8b0d2945264ab8eec56122ecb02bd495b6fc9acacc", "prompt_hash": "52f8b98595864e99a80b9c24bbd03b7ac97eaae02ce9a8a05c4bbc4db4b73802", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 178, "doc": {"id": "397", "question_stem": "Organisms covered by layers of sediment", "choices": {"text": ["become fossils over night", "may end up reanimated over time", "develop characteristics for survival", "may end up fueling a car"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Organisms covered by layers of sediment", "arg_1": " become fossils over night"}, "gen_args_1": {"arg_0": "Organisms covered by layers of sediment", "arg_1": " may end up reanimated over time"}, "gen_args_2": {"arg_0": "Organisms covered by layers of sediment", "arg_1": " develop characteristics for survival"}, "gen_args_3": {"arg_0": "Organisms covered by layers of sediment", "arg_1": " may end up fueling a car"}}, "resps": [[["-37.0", "False"]], [["-43.25", "False"]], [["-25.75", "False"]], [["-40.75", "False"]]], "filtered_resps": [["-37.0", "False"], ["-43.25", "False"], ["-25.75", "False"], ["-40.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "c261f9f09a040e9536588e51ec03b082a4fc7532f27dacdf30d11d363c5972e4", "prompt_hash": "0d98879d7cc717513b44037d03a6c69c57e6536a823128edabab0e27f099b3fe", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 179, "doc": {"id": "9-32", "question_stem": "The winter solstice is on December 21st in the", "choices": {"text": ["counties", "north of equator", "states", "southern hemisphere"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "The winter solstice is on December 21st in the", "arg_1": " counties"}, "gen_args_1": {"arg_0": "The winter solstice is on December 21st in the", "arg_1": " north of equator"}, "gen_args_2": {"arg_0": "The winter solstice is on December 21st in the", "arg_1": " states"}, "gen_args_3": {"arg_0": "The winter solstice is on December 21st in the", "arg_1": " southern hemisphere"}}, "resps": [[["-25.25", "False"]], [["-38.25", "False"]], [["-22.375", "False"]], [["-11.3125", "False"]]], "filtered_resps": [["-25.25", "False"], ["-38.25", "False"], ["-22.375", "False"], ["-11.3125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "fa80567ea12a3fa0590f606ed77498661ddf90fa788fa13ec829043d92247719", "prompt_hash": "f2d84d6343c88ceaa151bd6f32d149c99db1596fc9a916d74c612c0d80fa9321", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 180, "doc": {"id": "48", "question_stem": "A lake environment is a good setup for what to happen to organic remains?", "choices": {"text": ["bleaching", "burning", "fossilization", "drying"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A lake environment is a good setup for what to happen to organic remains?", "arg_1": " bleaching"}, "gen_args_1": {"arg_0": "A lake environment is a good setup for what to happen to organic remains?", "arg_1": " burning"}, "gen_args_2": {"arg_0": "A lake environment is a good setup for what to happen to organic remains?", "arg_1": " fossilization"}, "gen_args_3": {"arg_0": "A lake environment is a good setup for what to happen to organic remains?", "arg_1": " drying"}}, "resps": [[["-18.75", "False"]], [["-20.875", "False"]], [["-17.375", "False"]], [["-16.375", "False"]]], "filtered_resps": [["-18.75", "False"], ["-20.875", "False"], ["-17.375", "False"], ["-16.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "fe890f7e0e98d1078ebc72ad39bc68a35c8ac9c6c01b000bd504c8b70949c80e", "prompt_hash": "0112756bdc3fe6509acaa5d2b9c8cee05e0b8c4eead672a206ab480deedd061f", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 181, "doc": {"id": "8-69", "question_stem": "were there fossil fuels in the ground when humans evolved?", "choices": {"text": ["this was only created by humans", "humans predate fossil fuel formation", "significant supplies accumulated prior", "none of these"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "were there fossil fuels in the ground when humans evolved?", "arg_1": " this was only created by humans"}, "gen_args_1": {"arg_0": "were there fossil fuels in the ground when humans evolved?", "arg_1": " humans predate fossil fuel formation"}, "gen_args_2": {"arg_0": "were there fossil fuels in the ground when humans evolved?", "arg_1": " significant supplies accumulated prior"}, "gen_args_3": {"arg_0": "were there fossil fuels in the ground when humans evolved?", "arg_1": " none of these"}}, "resps": [[["-38.75", "False"]], [["-30.875", "False"]], [["-36.0", "False"]], [["-25.5", "False"]]], "filtered_resps": [["-38.75", "False"], ["-30.875", "False"], ["-36.0", "False"], ["-25.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2d095c6a320f2df71509eec39d8599798b4268582f2548df85571cc835083d06", "prompt_hash": "ef20cbe36c906c68b6ab009ae631f4515dfbbf0a2dde3d2f44932ffab7b109aa", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 182, "doc": {"id": "9-159", "question_stem": "The Grand Canyon is massive, with large, high peaks and very deep lows, which was formed when", "choices": {"text": ["some water is around it", "water rained on it", "natural waters weathered it", "a pool was opened"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "The Grand Canyon is massive, with large, high peaks and very deep lows, which was formed when", "arg_1": " some water is around it"}, "gen_args_1": {"arg_0": "The Grand Canyon is massive, with large, high peaks and very deep lows, which was formed when", "arg_1": " water rained on it"}, "gen_args_2": {"arg_0": "The Grand Canyon is massive, with large, high peaks and very deep lows, which was formed when", "arg_1": " natural waters weathered it"}, "gen_args_3": {"arg_0": "The Grand Canyon is massive, with large, high peaks and very deep lows, which was formed when", "arg_1": " a pool was opened"}}, "resps": [[["-37.25", "False"]], [["-27.125", "False"]], [["-34.0", "False"]], [["-32.5", "False"]]], "filtered_resps": [["-37.25", "False"], ["-27.125", "False"], ["-34.0", "False"], ["-32.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "44cb88b51ad62f82924a257a7e5a8fdfc28b8ef3eb6cc4f05e2bdb66ae6bf66f", "prompt_hash": "331665d9c5775f401b87999ffa26b601e611027ac82c205d439e4d8aa7a9a422", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 183, "doc": {"id": "9-317", "question_stem": "What type of useful product can be made from the moving winds?", "choices": {"text": ["wood", "bananas", "electricity", "metal"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "What type of useful product can be made from the moving winds?", "arg_1": " wood"}, "gen_args_1": {"arg_0": "What type of useful product can be made from the moving winds?", "arg_1": " bananas"}, "gen_args_2": {"arg_0": "What type of useful product can be made from the moving winds?", "arg_1": " electricity"}, "gen_args_3": {"arg_0": "What type of useful product can be made from the moving winds?", "arg_1": " metal"}}, "resps": [[["-23.875", "False"]], [["-25.125", "False"]], [["-18.5", "False"]], [["-19.375", "False"]]], "filtered_resps": [["-23.875", "False"], ["-25.125", "False"], ["-18.5", "False"], ["-19.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4b1311a6a93aa4e211ac055f436d5bea24f956346466241097f236b46127afaa", "prompt_hash": "5e2190e2a0cb7b1eb9209d0ac2fad9cc2794d3306eead7cae177e6ae86f606aa", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 184, "doc": {"id": "423", "question_stem": "The sides of the canyon are", "choices": {"text": ["metal", "water", "rivers", "stone"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "The sides of the canyon are", "arg_1": " metal"}, "gen_args_1": {"arg_0": "The sides of the canyon are", "arg_1": " water"}, "gen_args_2": {"arg_0": "The sides of the canyon are", "arg_1": " rivers"}, "gen_args_3": {"arg_0": "The sides of the canyon are", "arg_1": " stone"}}, "resps": [[["-12.125", "False"]], [["-10.6875", "False"]], [["-13.5", "False"]], [["-12.4375", "False"]]], "filtered_resps": [["-12.125", "False"], ["-10.6875", "False"], ["-13.5", "False"], ["-12.4375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "ea90b859c8f946a2e1be87ef9a42bd87472b7484db78e8435ce138aadcc6980a", "prompt_hash": "1b506ecb492aca8dc591722a40bf8ed16dbc0b21005f14ecba7536a05b934448", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 185, "doc": {"id": "8-304", "question_stem": "which of these people would have the worst air quality at their residence?", "choices": {"text": ["a man who lives next to a landfill", "a man who lives in a city with the best air quality", "none of these", "a man who lives in a great suburb"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "which of these people would have the worst air quality at their residence?", "arg_1": " a man who lives next to a landfill"}, "gen_args_1": {"arg_0": "which of these people would have the worst air quality at their residence?", "arg_1": " a man who lives in a city with the best air quality"}, "gen_args_2": {"arg_0": "which of these people would have the worst air quality at their residence?", "arg_1": " none of these"}, "gen_args_3": {"arg_0": "which of these people would have the worst air quality at their residence?", "arg_1": " a man who lives in a great suburb"}}, "resps": [[["-36.25", "False"]], [["-39.5", "False"]], [["-16.625", "False"]], [["-43.5", "False"]]], "filtered_resps": [["-36.25", "False"], ["-39.5", "False"], ["-16.625", "False"], ["-43.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "27279a1d1201408a51487af539d5ec7f7132ac375fc0d16f5a8e1ed2aca06e23", "prompt_hash": "d04cfac2b7c34d3fda054a1a75a1fbf1afd8052da01d492c99b995d697185cd1", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 186, "doc": {"id": "785", "question_stem": "What is a stopwatch used for?", "choices": {"text": ["to rewind 5 minutes", "to tell what will happen 5 minutes from now", "to voice the time", "to measure minutes and hours"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "What is a stopwatch used for?", "arg_1": " to rewind 5 minutes"}, "gen_args_1": {"arg_0": "What is a stopwatch used for?", "arg_1": " to tell what will happen 5 minutes from now"}, "gen_args_2": {"arg_0": "What is a stopwatch used for?", "arg_1": " to voice the time"}, "gen_args_3": {"arg_0": "What is a stopwatch used for?", "arg_1": " to measure minutes and hours"}}, "resps": [[["-46.75", "False"]], [["-57.0", "False"]], [["-36.0", "False"]], [["-32.5", "False"]]], "filtered_resps": [["-46.75", "False"], ["-57.0", "False"], ["-36.0", "False"], ["-32.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "9e4d47e2ec02a11a6437f5f8e95616d9b2627be6a8f250c2322d8e1145c33206", "prompt_hash": "fbb374dfb18065cc32a6be563665a5d27511f6a351206a41d91fdd74fa74c6d3", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 187, "doc": {"id": "9-1087", "question_stem": "As a drought worsens the level at an aquifer will", "choices": {"text": ["stay the same", "fluctuate wildly", "decrease", "increase"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "As a drought worsens the level at an aquifer will", "arg_1": " stay the same"}, "gen_args_1": {"arg_0": "As a drought worsens the level at an aquifer will", "arg_1": " fluctuate wildly"}, "gen_args_2": {"arg_0": "As a drought worsens the level at an aquifer will", "arg_1": " decrease"}, "gen_args_3": {"arg_0": "As a drought worsens the level at an aquifer will", "arg_1": " increase"}}, "resps": [[["-11.9375", "False"]], [["-12.0625", "False"]], [["-3.765625", "False"]], [["-2.390625", "False"]]], "filtered_resps": [["-11.9375", "False"], ["-12.0625", "False"], ["-3.765625", "False"], ["-2.390625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "3cb8c9fd2554ecd3eb7ce3238f6693c88669d9354dc19add2cd848e4547310bc", "prompt_hash": "7e0ec6047f92cbcff47d499e31f04f664a8376a7b5244e27c4fcc7fd7b9b2573", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 188, "doc": {"id": "485", "question_stem": "Wind frequently helps transport from one place to another", "choices": {"text": ["marble statues", "molten magma", "subterranean termites", "exposed topsoil"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Wind frequently helps transport from one place to another", "arg_1": " marble statues"}, "gen_args_1": {"arg_0": "Wind frequently helps transport from one place to another", "arg_1": " molten magma"}, "gen_args_2": {"arg_0": "Wind frequently helps transport from one place to another", "arg_1": " subterranean termites"}, "gen_args_3": {"arg_0": "Wind frequently helps transport from one place to another", "arg_1": " exposed topsoil"}}, "resps": [[["-25.5", "False"]], [["-23.75", "False"]], [["-27.125", "False"]], [["-23.875", "False"]]], "filtered_resps": [["-25.5", "False"], ["-23.75", "False"], ["-27.125", "False"], ["-23.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "ba47b56d18b153b602433a901c18c31322846071783624a967dbb29986c73914", "prompt_hash": "0a1dc950991564845384286552caa913370631d09955865263f51dc4fd754946", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 189, "doc": {"id": "9-908", "question_stem": "When a kid slams on the brakes on their bike what is caused?", "choices": {"text": ["bike helmet", "avoiding accidents", "friction", "gearing"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "When a kid slams on the brakes on their bike what is caused?", "arg_1": " bike helmet"}, "gen_args_1": {"arg_0": "When a kid slams on the brakes on their bike what is caused?", "arg_1": " avoiding accidents"}, "gen_args_2": {"arg_0": "When a kid slams on the brakes on their bike what is caused?", "arg_1": " friction"}, "gen_args_3": {"arg_0": "When a kid slams on the brakes on their bike what is caused?", "arg_1": " gearing"}}, "resps": [[["-25.875", "False"]], [["-26.75", "False"]], [["-16.875", "False"]], [["-20.75", "False"]]], "filtered_resps": [["-25.875", "False"], ["-26.75", "False"], ["-16.875", "False"], ["-20.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d2eab25200c02f0ac0badd046d4268a91cb9d546b2c2f7d6b84184a868f3a3dd", "prompt_hash": "0cabf7482e712d7f07d4b10971df80f6e3fc4ede4dde79deb2b294031ec80289", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 190, "doc": {"id": "1231", "question_stem": "Sources of spices have", "choices": {"text": ["crystals", "feathers", "cell walls", "craters"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Sources of spices have", "arg_1": " crystals"}, "gen_args_1": {"arg_0": "Sources of spices have", "arg_1": " feathers"}, "gen_args_2": {"arg_0": "Sources of spices have", "arg_1": " cell walls"}, "gen_args_3": {"arg_0": "Sources of spices have", "arg_1": " craters"}}, "resps": [[["-20.625", "False"]], [["-20.625", "False"]], [["-25.0", "False"]], [["-15.875", "False"]]], "filtered_resps": [["-20.625", "False"], ["-20.625", "False"], ["-25.0", "False"], ["-15.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "cef9716c417aa28d5069fc13313082d32fa0d59a119ec2cd820e91ce13f0204b", "prompt_hash": "a09a15e797d8a13d5b8f3b4c0e8c0abf3fe0df61e722485599a870a95b63ba87", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 191, "doc": {"id": "810", "question_stem": "Conservation", "choices": {"text": ["leads to longer drought of resources", "leads to longer availability of resources", "leads to more consumption", "leads to short supply of resources"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Conservation", "arg_1": " leads to longer drought of resources"}, "gen_args_1": {"arg_0": "Conservation", "arg_1": " leads to longer availability of resources"}, "gen_args_2": {"arg_0": "Conservation", "arg_1": " leads to more consumption"}, "gen_args_3": {"arg_0": "Conservation", "arg_1": " leads to short supply of resources"}}, "resps": [[["-47.5", "False"]], [["-34.75", "False"]], [["-25.75", "False"]], [["-39.25", "False"]]], "filtered_resps": [["-47.5", "False"], ["-34.75", "False"], ["-25.75", "False"], ["-39.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d29b176efb686697fb0418c6dcf1d53dbbc08e483928b722675a9a93709bf2de", "prompt_hash": "e43f60bbc4e69bac962d92f4e903eb284d459dd45574a3cdc7d767b5943fb26d", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 192, "doc": {"id": "158", "question_stem": "A tree is not the habitat of a", "choices": {"text": ["squirrel", "woodpecker", "monkey", "lion"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "A tree is not the habitat of a", "arg_1": " squirrel"}, "gen_args_1": {"arg_0": "A tree is not the habitat of a", "arg_1": " woodpecker"}, "gen_args_2": {"arg_0": "A tree is not the habitat of a", "arg_1": " monkey"}, "gen_args_3": {"arg_0": "A tree is not the habitat of a", "arg_1": " lion"}}, "resps": [[["-3.421875", "False"]], [["-8.0625", "False"]], [["-6.6875", "False"]], [["-6.9375", "False"]]], "filtered_resps": [["-3.421875", "False"], ["-8.0625", "False"], ["-6.6875", "False"], ["-6.9375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a8274e762c8f7d8bf130b8c2980233634123b46ac3e5cb5c85a190e7cf282132", "prompt_hash": "d53cc6d05597c24b8624ad82b4f2d13dd9939d928950038bc8b0c1ea2abab169", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 193, "doc": {"id": "7-445", "question_stem": "Erosion could lead to", "choices": {"text": ["a change in the direction of a stream", "a change in ocean temperatures", "an increase in rainy weather", "an increase in plants and animals"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Erosion could lead to", "arg_1": " a change in the direction of a stream"}, "gen_args_1": {"arg_0": "Erosion could lead to", "arg_1": " a change in ocean temperatures"}, "gen_args_2": {"arg_0": "Erosion could lead to", "arg_1": " an increase in rainy weather"}, "gen_args_3": {"arg_0": "Erosion could lead to", "arg_1": " an increase in plants and animals"}}, "resps": [[["-18.875", "False"]], [["-19.625", "False"]], [["-23.75", "False"]], [["-19.5", "False"]]], "filtered_resps": [["-18.875", "False"], ["-19.625", "False"], ["-23.75", "False"], ["-19.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "eb571a944d0a1e96db06fce9c55e3bfec583cd5347a3c40e2fd90a20932567f5", "prompt_hash": "da2f153550b7a669f719fe4140eab6050471f2c9a7660ce78f029e3ae73a68dd", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 194, "doc": {"id": "1502", "question_stem": "A hemisphere experiences summer when", "choices": {"text": ["it's tilted towards Jupiter", "it's angled towards the moon", "it's angled towards the largest star in the solar system", "it spins counter clockwise on Earth's axis"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A hemisphere experiences summer when", "arg_1": " it's tilted towards Jupiter"}, "gen_args_1": {"arg_0": "A hemisphere experiences summer when", "arg_1": " it's angled towards the moon"}, "gen_args_2": {"arg_0": "A hemisphere experiences summer when", "arg_1": " it's angled towards the largest star in the solar system"}, "gen_args_3": {"arg_0": "A hemisphere experiences summer when", "arg_1": " it spins counter clockwise on Earth's axis"}}, "resps": [[["-17.25", "False"]], [["-21.25", "False"]], [["-35.75", "False"]], [["-35.0", "False"]]], "filtered_resps": [["-17.25", "False"], ["-21.25", "False"], ["-35.75", "False"], ["-35.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "affa04157eba13134ce6fdb4cf2a084125891a15e0a3e7e9db5cba64572b4c98", "prompt_hash": "ff2bcad46408a6be33d1453489db66cd60104b47d3d5fa3e482457f397bdd65e", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 195, "doc": {"id": "1200", "question_stem": "What date is the amount of daylight minimized", "choices": {"text": ["Jul 4th", "Feb 29th", "May 3rd", "Sep 1st"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "What date is the amount of daylight minimized", "arg_1": " Jul 4th"}, "gen_args_1": {"arg_0": "What date is the amount of daylight minimized", "arg_1": " Feb 29th"}, "gen_args_2": {"arg_0": "What date is the amount of daylight minimized", "arg_1": " May 3rd"}, "gen_args_3": {"arg_0": "What date is the amount of daylight minimized", "arg_1": " Sep 1st"}}, "resps": [[["-22.5", "False"]], [["-23.125", "False"]], [["-25.0", "False"]], [["-25.125", "False"]]], "filtered_resps": [["-22.5", "False"], ["-23.125", "False"], ["-25.0", "False"], ["-25.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a57721b44290c2a1b67e71efb93ee7654d036ffae451ed123aa74f29240ec938", "prompt_hash": "f628eaf2ff5b38faf804c047d8b85d72c1764257d7fc7479f4fcd3b97f027fba", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 196, "doc": {"id": "437", "question_stem": "Which term is involved with protection by skin?", "choices": {"text": ["Eucerin pH5 range", "Sagittal plane", "pyogenic vibrio", "popliteus"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Which term is involved with protection by skin?", "arg_1": " Eucerin pH5 range"}, "gen_args_1": {"arg_0": "Which term is involved with protection by skin?", "arg_1": " Sagittal plane"}, "gen_args_2": {"arg_0": "Which term is involved with protection by skin?", "arg_1": " pyogenic vibrio"}, "gen_args_3": {"arg_0": "Which term is involved with protection by skin?", "arg_1": " popliteus"}}, "resps": [[["-43.0", "False"]], [["-20.5", "False"]], [["-44.75", "False"]], [["-27.625", "False"]]], "filtered_resps": [["-43.0", "False"], ["-20.5", "False"], ["-44.75", "False"], ["-27.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e40ca7e87e8636a698ffcb76f13a240c502ec69bf83279bfddc1452990decc5e", "prompt_hash": "4deac27faa63a1df00475229b923d9a6bb363909efdb986aba62eee644546730", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 197, "doc": {"id": "8-205", "question_stem": "The reason Earth is so sturdy is because", "choices": {"text": ["It is made from rock", "It eats three meals a day", "It has a loving family", "It is made from metal"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "The reason Earth is so sturdy is because", "arg_1": " It is made from rock"}, "gen_args_1": {"arg_0": "The reason Earth is so sturdy is because", "arg_1": " It eats three meals a day"}, "gen_args_2": {"arg_0": "The reason Earth is so sturdy is because", "arg_1": " It has a loving family"}, "gen_args_3": {"arg_0": "The reason Earth is so sturdy is because", "arg_1": " It is made from metal"}}, "resps": [[["-24.375", "False"]], [["-37.75", "False"]], [["-29.625", "False"]], [["-25.875", "False"]]], "filtered_resps": [["-24.375", "False"], ["-37.75", "False"], ["-29.625", "False"], ["-25.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "b506ee0fc80b750d21ef65ceef7e7c6493e0272dbd8c335976374817244be146", "prompt_hash": "7567fc914858c0b30e0cd660afe2c2faa66cddebcfd23b95d1d0bac499eb7b03", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 198, "doc": {"id": "9-270", "question_stem": "A plant left in the dark", "choices": {"text": ["produces fruit", "grows faster", "fails to grow", "gets greener"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A plant left in the dark", "arg_1": " produces fruit"}, "gen_args_1": {"arg_0": "A plant left in the dark", "arg_1": " grows faster"}, "gen_args_2": {"arg_0": "A plant left in the dark", "arg_1": " fails to grow"}, "gen_args_3": {"arg_0": "A plant left in the dark", "arg_1": " gets greener"}}, "resps": [[["-14.125", "False"]], [["-12.875", "False"]], [["-15.5625", "False"]], [["-14.9375", "False"]]], "filtered_resps": [["-14.125", "False"], ["-12.875", "False"], ["-15.5625", "False"], ["-14.9375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "8c16f6893badd42ddd5579338d60c17c8eeddc0885729f2e9e552d770b78a766", "prompt_hash": "610c3116a4d3b0bef1a1922a859de26354af2fec4f66fe8e6bbb112328cf851e", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 199, "doc": {"id": "8-130", "question_stem": "A boy wants to use his Walkman so that he can listen to some music. When he tries to turn it on, it us unable to, and the boy realizes that he will need", "choices": {"text": ["heat", "metal", "lithium-ion", "plastic"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A boy wants to use his Walkman so that he can listen to some music. When he tries to turn it on, it us unable to, and the boy realizes that he will need", "arg_1": " heat"}, "gen_args_1": {"arg_0": "A boy wants to use his Walkman so that he can listen to some music. When he tries to turn it on, it us unable to, and the boy realizes that he will need", "arg_1": " metal"}, "gen_args_2": {"arg_0": "A boy wants to use his Walkman so that he can listen to some music. When he tries to turn it on, it us unable to, and the boy realizes that he will need", "arg_1": " lithium-ion"}, "gen_args_3": {"arg_0": "A boy wants to use his Walkman so that he can listen to some music. When he tries to turn it on, it us unable to, and the boy realizes that he will need", "arg_1": " plastic"}}, "resps": [[["-16.25", "False"]], [["-12.6875", "False"]], [["-20.375", "False"]], [["-12.9375", "False"]]], "filtered_resps": [["-16.25", "False"], ["-12.6875", "False"], ["-20.375", "False"], ["-12.9375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "228e798fdf6ba04e536f557cb5fdf4bd10c5e361f4980d42ad0fe950b627440c", "prompt_hash": "3223316523df058f7a8b8d4ea83d660d6ff3ab264b3b16be42bf940ba6e2b0c3", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 200, "doc": {"id": "229", "question_stem": "Nuclear activity is the cause of what celestial occurrence?", "choices": {"text": ["axial planetary rotation", "comets", "planetary formation", "the sun's rays"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Nuclear activity is the cause of what celestial occurrence?", "arg_1": " axial planetary rotation"}, "gen_args_1": {"arg_0": "Nuclear activity is the cause of what celestial occurrence?", "arg_1": " comets"}, "gen_args_2": {"arg_0": "Nuclear activity is the cause of what celestial occurrence?", "arg_1": " planetary formation"}, "gen_args_3": {"arg_0": "Nuclear activity is the cause of what celestial occurrence?", "arg_1": " the sun's rays"}}, "resps": [[["-36.75", "False"]], [["-17.125", "False"]], [["-17.25", "False"]], [["-23.875", "False"]]], "filtered_resps": [["-36.75", "False"], ["-17.125", "False"], ["-17.25", "False"], ["-23.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "3dac18d625b7d3262c1d45e1f6d6023a201acc611dd007910fc4c13c5f881fbc", "prompt_hash": "fb2c537a026c7ae4927a429133470a130d39b7917c0b21bb55ce037a57f8384f", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 201, "doc": {"id": "9-390", "question_stem": "Which source provides the safest water?", "choices": {"text": ["River", "Sea", "Ocean", "Rain"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Which source provides the safest water?", "arg_1": " River"}, "gen_args_1": {"arg_0": "Which source provides the safest water?", "arg_1": " Sea"}, "gen_args_2": {"arg_0": "Which source provides the safest water?", "arg_1": " Ocean"}, "gen_args_3": {"arg_0": "Which source provides the safest water?", "arg_1": " Rain"}}, "resps": [[["-14.0", "False"]], [["-22.25", "False"]], [["-19.875", "False"]], [["-15.4375", "False"]]], "filtered_resps": [["-14.0", "False"], ["-22.25", "False"], ["-19.875", "False"], ["-15.4375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "fe8f3b52677678f43c52dd273525a052ff0e599669f010de0954eb99d4e39b51", "prompt_hash": "a185a6c8cf53a3036f787ad95a1575ae1ddb7cfb6c55dbc3da99e98e13bde4c7", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 202, "doc": {"id": "8-107", "question_stem": "A rabbit has a litter of bunnies! Most of the babies are white, just like the mother rabbit, but one baby has brown spots, like the father rabbit. The father rabbit", "choices": {"text": ["spread out some fur", "has black on his ears", "passed down inherited characteristics", "is the same size as the mother"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A rabbit has a litter of bunnies! Most of the babies are white, just like the mother rabbit, but one baby has brown spots, like the father rabbit. The father rabbit", "arg_1": " spread out some fur"}, "gen_args_1": {"arg_0": "A rabbit has a litter of bunnies! Most of the babies are white, just like the mother rabbit, but one baby has brown spots, like the father rabbit. The father rabbit", "arg_1": " has black on his ears"}, "gen_args_2": {"arg_0": "A rabbit has a litter of bunnies! Most of the babies are white, just like the mother rabbit, but one baby has brown spots, like the father rabbit. The father rabbit", "arg_1": " passed down inherited characteristics"}, "gen_args_3": {"arg_0": "A rabbit has a litter of bunnies! Most of the babies are white, just like the mother rabbit, but one baby has brown spots, like the father rabbit. The father rabbit", "arg_1": " is the same size as the mother"}}, "resps": [[["-23.25", "False"]], [["-24.25", "False"]], [["-29.75", "False"]], [["-17.125", "False"]]], "filtered_resps": [["-23.25", "False"], ["-24.25", "False"], ["-29.75", "False"], ["-17.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a95fc31124c0590352d4aba67d03c5d5faae2669bb74d18f3889348fcb9b3ed2", "prompt_hash": "7e1f92e22d9393308a4b8cd81bf8b0292a52737c4146f8f329ac5d71123842fc", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 203, "doc": {"id": "7-527", "question_stem": "Photosynthesis means plants are unable to", "choices": {"text": ["convert sunlight to sand", "provide food sources for others", "be producers in an ecosystem", "make their own food"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Photosynthesis means plants are unable to", "arg_1": " convert sunlight to sand"}, "gen_args_1": {"arg_0": "Photosynthesis means plants are unable to", "arg_1": " provide food sources for others"}, "gen_args_2": {"arg_0": "Photosynthesis means plants are unable to", "arg_1": " be producers in an ecosystem"}, "gen_args_3": {"arg_0": "Photosynthesis means plants are unable to", "arg_1": " make their own food"}}, "resps": [[["-21.875", "False"]], [["-21.25", "False"]], [["-22.0", "False"]], [["-2.703125", "False"]]], "filtered_resps": [["-21.875", "False"], ["-21.25", "False"], ["-22.0", "False"], ["-2.703125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "76f5b33f3d05ce2a742a1c9a54dd9e132687c0bffe73c4845f8631b32553a74f", "prompt_hash": "99b715a89269639192e437c9e4bee6840c5d2da5e827786ea15332b239c43d25", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 204, "doc": {"id": "7-333", "question_stem": "Through DNA, a rabbit will have long ears if", "choices": {"text": ["rabbits are born with ears", "there was a lot of food", "genetic contributors had long ears", "parents were also rabbits"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Through DNA, a rabbit will have long ears if", "arg_1": " rabbits are born with ears"}, "gen_args_1": {"arg_0": "Through DNA, a rabbit will have long ears if", "arg_1": " there was a lot of food"}, "gen_args_2": {"arg_0": "Through DNA, a rabbit will have long ears if", "arg_1": " genetic contributors had long ears"}, "gen_args_3": {"arg_0": "Through DNA, a rabbit will have long ears if", "arg_1": " parents were also rabbits"}}, "resps": [[["-16.75", "False"]], [["-21.375", "False"]], [["-32.25", "False"]], [["-20.0", "False"]]], "filtered_resps": [["-16.75", "False"], ["-21.375", "False"], ["-32.25", "False"], ["-20.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "0c9ca64a237ba6724ab33c9984a9f076acb246cfe5ad9c5d0df1118f3ffab82f", "prompt_hash": "1e092d338e7113132244dbc59043f15619d0073192e63b52d8bf9627df685719", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 205, "doc": {"id": "9-44", "question_stem": "There are various creatures that live in forests, such as", "choices": {"text": ["giant fish", "enormous crabs", "whitetails", "desert jackals"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "There are various creatures that live in forests, such as", "arg_1": " giant fish"}, "gen_args_1": {"arg_0": "There are various creatures that live in forests, such as", "arg_1": " enormous crabs"}, "gen_args_2": {"arg_0": "There are various creatures that live in forests, such as", "arg_1": " whitetails"}, "gen_args_3": {"arg_0": "There are various creatures that live in forests, such as", "arg_1": " desert jackals"}}, "resps": [[["-18.875", "False"]], [["-25.375", "False"]], [["-19.75", "False"]], [["-25.375", "False"]]], "filtered_resps": [["-18.875", "False"], ["-25.375", "False"], ["-19.75", "False"], ["-25.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "1532cc5bb996d4478549babe44bece595f6491b038732ebfe7f30dcbd03e485f", "prompt_hash": "5fbdac04e1c1418e8f5b21da01a3ad6442c62d2cb449d3dcfb50b0f5321bc90d", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 206, "doc": {"id": "7-160", "question_stem": "An octopus protects itself with", "choices": {"text": ["water splashing", "running fast", "long hands", "pigment squirting"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "An octopus protects itself with", "arg_1": " water splashing"}, "gen_args_1": {"arg_0": "An octopus protects itself with", "arg_1": " running fast"}, "gen_args_2": {"arg_0": "An octopus protects itself with", "arg_1": " long hands"}, "gen_args_3": {"arg_0": "An octopus protects itself with", "arg_1": " pigment squirting"}}, "resps": [[["-20.875", "False"]], [["-23.0", "False"]], [["-27.25", "False"]], [["-25.0", "False"]]], "filtered_resps": [["-20.875", "False"], ["-23.0", "False"], ["-27.25", "False"], ["-25.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4d9fc5d2ce901c32dbb5619d2d41a979fba9866fbb9e215b53e858d6e92de7a7", "prompt_hash": "4c55b19f2ab565b1b7e451c47c7a80b7c91efa102fd056446c114edac0cc6102", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 207, "doc": {"id": "1942", "question_stem": "Resources decreasing in an environment", "choices": {"text": ["induces organisms to use more of their resources", "causes an increase in use of resources", "causes an uptick in birthrate", "induces organisms to be more economical with resources"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Resources decreasing in an environment", "arg_1": " induces organisms to use more of their resources"}, "gen_args_1": {"arg_0": "Resources decreasing in an environment", "arg_1": " causes an increase in use of resources"}, "gen_args_2": {"arg_0": "Resources decreasing in an environment", "arg_1": " causes an uptick in birthrate"}, "gen_args_3": {"arg_0": "Resources decreasing in an environment", "arg_1": " induces organisms to be more economical with resources"}}, "resps": [[["-35.75", "False"]], [["-29.0", "False"]], [["-37.75", "False"]], [["-40.0", "False"]]], "filtered_resps": [["-35.75", "False"], ["-29.0", "False"], ["-37.75", "False"], ["-40.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e911f5240c219c19809309a7cd8c115727c9c4680129f4e79fdbad17a7590a1d", "prompt_hash": "6c60bdbdd68fea4c363c2e09a37a98926b254f04d5d1ecc3db7d7478890994b1", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 208, "doc": {"id": "9-597", "question_stem": "The sidewalk next to a house having a crack in it and having vegetation growing from it is considered?", "choices": {"text": ["insects", "weathering", "lava", "erosion"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "The sidewalk next to a house having a crack in it and having vegetation growing from it is considered?", "arg_1": " insects"}, "gen_args_1": {"arg_0": "The sidewalk next to a house having a crack in it and having vegetation growing from it is considered?", "arg_1": " weathering"}, "gen_args_2": {"arg_0": "The sidewalk next to a house having a crack in it and having vegetation growing from it is considered?", "arg_1": " lava"}, "gen_args_3": {"arg_0": "The sidewalk next to a house having a crack in it and having vegetation growing from it is considered?", "arg_1": " erosion"}}, "resps": [[["-21.0", "False"]], [["-20.75", "False"]], [["-24.75", "False"]], [["-18.625", "False"]]], "filtered_resps": [["-21.0", "False"], ["-20.75", "False"], ["-24.75", "False"], ["-18.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2477a85da73d567a7f238029bcff50706c8138f1ff9f442b340264ca4407932f", "prompt_hash": "58a8ae3a38ddd2658b9636936597be5dcdd4609d525ea8c1fb88da51f881d3b7", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 209, "doc": {"id": "9-35", "question_stem": "If you wanted to make a necklace, how long would you have to wait for the materials to appear inside the Earth?", "choices": {"text": ["millions of years", "1 day", "10 days", "100 days"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "If you wanted to make a necklace, how long would you have to wait for the materials to appear inside the Earth?", "arg_1": " millions of years"}, "gen_args_1": {"arg_0": "If you wanted to make a necklace, how long would you have to wait for the materials to appear inside the Earth?", "arg_1": " 1 day"}, "gen_args_2": {"arg_0": "If you wanted to make a necklace, how long would you have to wait for the materials to appear inside the Earth?", "arg_1": " 10 days"}, "gen_args_3": {"arg_0": "If you wanted to make a necklace, how long would you have to wait for the materials to appear inside the Earth?", "arg_1": " 100 days"}}, "resps": [[["-17.5", "False"]], [["-21.0", "False"]], [["-20.0", "False"]], [["-20.125", "False"]]], "filtered_resps": [["-17.5", "False"], ["-21.0", "False"], ["-20.0", "False"], ["-20.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "01b612e57318a50e4279522e55729b6910ec16d27bb2d2898803a4cb6d48b53e", "prompt_hash": "bda307189635c9b215e954eb915603e7c55b453ef54ca45423848ae2fca98dcc", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 210, "doc": {"id": "1161", "question_stem": "Moon phases", "choices": {"text": ["change the moon into cheese", "alter the way the moon's facade looks", "change moon lakes into vapor", "cause lunar eclipse every day"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Moon phases", "arg_1": " change the moon into cheese"}, "gen_args_1": {"arg_0": "Moon phases", "arg_1": " alter the way the moon's facade looks"}, "gen_args_2": {"arg_0": "Moon phases", "arg_1": " change moon lakes into vapor"}, "gen_args_3": {"arg_0": "Moon phases", "arg_1": " cause lunar eclipse every day"}}, "resps": [[["-34.25", "False"]], [["-35.0", "False"]], [["-44.75", "False"]], [["-35.25", "False"]]], "filtered_resps": [["-34.25", "False"], ["-35.0", "False"], ["-44.75", "False"], ["-35.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "eb662f6d4f94a95ed2201648170e78718746a5542860efb9619db2de0e442736", "prompt_hash": "cc7ed0186416a6c793520c9ff6fad53e84f701851a540a01254d75f4a40539d0", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 211, "doc": {"id": "7-171", "question_stem": "If hot water were poured on an arm, what would happen to internal organs?", "choices": {"text": ["they would be scalded", "organs would remain uneffected", "they would begin to decay", "they would experience pain"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "If hot water were poured on an arm, what would happen to internal organs?", "arg_1": " they would be scalded"}, "gen_args_1": {"arg_0": "If hot water were poured on an arm, what would happen to internal organs?", "arg_1": " organs would remain uneffected"}, "gen_args_2": {"arg_0": "If hot water were poured on an arm, what would happen to internal organs?", "arg_1": " they would begin to decay"}, "gen_args_3": {"arg_0": "If hot water were poured on an arm, what would happen to internal organs?", "arg_1": " they would experience pain"}}, "resps": [[["-24.0", "False"]], [["-46.5", "False"]], [["-33.0", "False"]], [["-31.25", "False"]]], "filtered_resps": [["-24.0", "False"], ["-46.5", "False"], ["-33.0", "False"], ["-31.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "6940a73eaa710e53629ef8d395c2f195ff1ed4518e7d8860859dc48cbc8f8bb2", "prompt_hash": "29eb0264527a604358456b79886bffeece77bef1ea0706ccce40cfea32c9be38", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 212, "doc": {"id": "1139", "question_stem": "Barnyard bovines", "choices": {"text": ["eat organic chicken", "eat eggs", "eat beef", "eat alfalfa hay"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Barnyard bovines", "arg_1": " eat organic chicken"}, "gen_args_1": {"arg_0": "Barnyard bovines", "arg_1": " eat eggs"}, "gen_args_2": {"arg_0": "Barnyard bovines", "arg_1": " eat beef"}, "gen_args_3": {"arg_0": "Barnyard bovines", "arg_1": " eat alfalfa hay"}}, "resps": [[["-25.5", "False"]], [["-20.0", "False"]], [["-20.75", "False"]], [["-27.0", "False"]]], "filtered_resps": [["-25.5", "False"], ["-20.0", "False"], ["-20.75", "False"], ["-27.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "171776cb5b4e01011d12cf46d4c743af541c08dc7012d26968f159a2018b566f", "prompt_hash": "ac358bd9d737aba2d6656a06295842593781d39dc0a66b3eea928fd57c50eb1d", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 213, "doc": {"id": "1924", "question_stem": "Desert environments features", "choices": {"text": ["tropical plants", "tons of sun", "massive rain totals", "icy precipitation"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Desert environments features", "arg_1": " tropical plants"}, "gen_args_1": {"arg_0": "Desert environments features", "arg_1": " tons of sun"}, "gen_args_2": {"arg_0": "Desert environments features", "arg_1": " massive rain totals"}, "gen_args_3": {"arg_0": "Desert environments features", "arg_1": " icy precipitation"}}, "resps": [[["-13.875", "False"]], [["-15.1875", "False"]], [["-26.375", "False"]], [["-15.0625", "False"]]], "filtered_resps": [["-13.875", "False"], ["-15.1875", "False"], ["-26.375", "False"], ["-15.0625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "8e0566cc7e493b93d3817cdb779192e16436b1b4dd2aa790dbeb577d646d7ad1", "prompt_hash": "2fefbb64958dbbe79b3f0f3d48afba847aa6d6dd14e5de1388a25725b276297d", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 214, "doc": {"id": "9-440", "question_stem": "a large cluster of humans, dogs, apple trees, atmosphere and more can be called", "choices": {"text": ["army of ants", "a community", "a toy store", "a shopping mall"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "a large cluster of humans, dogs, apple trees, atmosphere and more can be called", "arg_1": " army of ants"}, "gen_args_1": {"arg_0": "a large cluster of humans, dogs, apple trees, atmosphere and more can be called", "arg_1": " a community"}, "gen_args_2": {"arg_0": "a large cluster of humans, dogs, apple trees, atmosphere and more can be called", "arg_1": " a toy store"}, "gen_args_3": {"arg_0": "a large cluster of humans, dogs, apple trees, atmosphere and more can be called", "arg_1": " a shopping mall"}}, "resps": [[["-24.0", "False"]], [["-8.3125", "False"]], [["-18.625", "False"]], [["-13.5625", "False"]]], "filtered_resps": [["-24.0", "False"], ["-8.3125", "False"], ["-18.625", "False"], ["-13.5625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "08d15f16fff3d30e901a14ed31350c1ab5403beb820d3bc3d342f32cb86baa16", "prompt_hash": "2d2aeac8bbc0819f378b619c5cac695ec94ed8ded355fe437cfd8988c80a7ed9", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 215, "doc": {"id": "9-528", "question_stem": "A person is considering various organs, and is looking at which ones will be most muscular. A contender for most muscular is", "choices": {"text": ["the lungs", "the kidney", "the heart", "the liver"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A person is considering various organs, and is looking at which ones will be most muscular. A contender for most muscular is", "arg_1": " the lungs"}, "gen_args_1": {"arg_0": "A person is considering various organs, and is looking at which ones will be most muscular. A contender for most muscular is", "arg_1": " the kidney"}, "gen_args_2": {"arg_0": "A person is considering various organs, and is looking at which ones will be most muscular. A contender for most muscular is", "arg_1": " the heart"}, "gen_args_3": {"arg_0": "A person is considering various organs, and is looking at which ones will be most muscular. A contender for most muscular is", "arg_1": " the liver"}}, "resps": [[["-9.375", "False"]], [["-7.53125", "False"]], [["-1.1796875", "True"]], [["-5.375", "False"]]], "filtered_resps": [["-9.375", "False"], ["-7.53125", "False"], ["-1.1796875", "True"], ["-5.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e5199825893b6b1aee671e125b9c97cf405c7de456897c00df783c7f0144f418", "prompt_hash": "2fce26bd6d92b03485e804508bf0121acc53649fe119d8656950e3d781187ffd", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 216, "doc": {"id": "170", "question_stem": "Which pair don't reproduce the same way?", "choices": {"text": ["rabbit and hare", "mule and hinny", "cat and catfish", "caterpillar and butterfly"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Which pair don't reproduce the same way?", "arg_1": " rabbit and hare"}, "gen_args_1": {"arg_0": "Which pair don't reproduce the same way?", "arg_1": " mule and hinny"}, "gen_args_2": {"arg_0": "Which pair don't reproduce the same way?", "arg_1": " cat and catfish"}, "gen_args_3": {"arg_0": "Which pair don't reproduce the same way?", "arg_1": " caterpillar and butterfly"}}, "resps": [[["-19.75", "False"]], [["-27.0", "False"]], [["-28.375", "False"]], [["-17.875", "False"]]], "filtered_resps": [["-19.75", "False"], ["-27.0", "False"], ["-28.375", "False"], ["-17.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "b1b08d50d5f17ba57bfbd90a4e1d2379192b842e3e3d72bfad86d8e2076df492", "prompt_hash": "12f31252ad0dbcbc1b1f2ef9a3c06fdc6b93a676ab2a69907b601f6b2ec80d12", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 217, "doc": {"id": "395", "question_stem": "Which of the following is warm blooded?", "choices": {"text": ["toad", "snake", "turtle", "skunk"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Which of the following is warm blooded?", "arg_1": " toad"}, "gen_args_1": {"arg_0": "Which of the following is warm blooded?", "arg_1": " snake"}, "gen_args_2": {"arg_0": "Which of the following is warm blooded?", "arg_1": " turtle"}, "gen_args_3": {"arg_0": "Which of the following is warm blooded?", "arg_1": " skunk"}}, "resps": [[["-17.5", "False"]], [["-12.9375", "False"]], [["-15.1875", "False"]], [["-16.875", "False"]]], "filtered_resps": [["-17.5", "False"], ["-12.9375", "False"], ["-15.1875", "False"], ["-16.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "fed0dedc5d571b34027b38a8eb4a5ab28b527126c79211c6165362f470ded7ac", "prompt_hash": "5da4e87b95da17ff989a92df6ab273d94174354a64322d3cb79052816dc530dc", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 218, "doc": {"id": "9-633", "question_stem": "A male bird spots a female of his species and begins a fancy dance, flashing his bright feathers around in the air, showing off. This male is attempting to procure", "choices": {"text": ["a manager", "an agent", "a meal", "a reproductive companion"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "A male bird spots a female of his species and begins a fancy dance, flashing his bright feathers around in the air, showing off. This male is attempting to procure", "arg_1": " a manager"}, "gen_args_1": {"arg_0": "A male bird spots a female of his species and begins a fancy dance, flashing his bright feathers around in the air, showing off. This male is attempting to procure", "arg_1": " an agent"}, "gen_args_2": {"arg_0": "A male bird spots a female of his species and begins a fancy dance, flashing his bright feathers around in the air, showing off. This male is attempting to procure", "arg_1": " a meal"}, "gen_args_3": {"arg_0": "A male bird spots a female of his species and begins a fancy dance, flashing his bright feathers around in the air, showing off. This male is attempting to procure", "arg_1": " a reproductive companion"}}, "resps": [[["-25.0", "False"]], [["-20.375", "False"]], [["-9.125", "False"]], [["-21.375", "False"]]], "filtered_resps": [["-25.0", "False"], ["-20.375", "False"], ["-9.125", "False"], ["-21.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4eff730d9c46a49caf547ad62817891b2b41bd727d643f6d62507ab7b6a1c5c8", "prompt_hash": "25d15b2a54e1b33d7722788773464f9b118581ab4e105a2498ce1ec649c81627", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 219, "doc": {"id": "9-504", "question_stem": "Vast quantities of metal can be obtained from", "choices": {"text": ["a quarry", "concerts", "forests", "salt"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Vast quantities of metal can be obtained from", "arg_1": " a quarry"}, "gen_args_1": {"arg_0": "Vast quantities of metal can be obtained from", "arg_1": " concerts"}, "gen_args_2": {"arg_0": "Vast quantities of metal can be obtained from", "arg_1": " forests"}, "gen_args_3": {"arg_0": "Vast quantities of metal can be obtained from", "arg_1": " salt"}}, "resps": [[["-11.5", "False"]], [["-14.625", "False"]], [["-9.9375", "False"]], [["-8.875", "False"]]], "filtered_resps": [["-11.5", "False"], ["-14.625", "False"], ["-9.9375", "False"], ["-8.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "79a25c41cb62cf9a18b2dac3c8fe91a0215ace78fb92e4983c2cb6859e968f4e", "prompt_hash": "f6f90aecfd33cd3edd15c0f95bb1c21bf0d154ed7f588233f11c109d075f9a63", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 220, "doc": {"id": "8-192", "question_stem": "Roasting a turkey requires adding what type of energy", "choices": {"text": ["Heat", "Kinetic", "Magnetic", "Chemical"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Roasting a turkey requires adding what type of energy", "arg_1": " Heat"}, "gen_args_1": {"arg_0": "Roasting a turkey requires adding what type of energy", "arg_1": " Kinetic"}, "gen_args_2": {"arg_0": "Roasting a turkey requires adding what type of energy", "arg_1": " Magnetic"}, "gen_args_3": {"arg_0": "Roasting a turkey requires adding what type of energy", "arg_1": " Chemical"}}, "resps": [[["-20.625", "False"]], [["-23.625", "False"]], [["-25.375", "False"]], [["-22.75", "False"]]], "filtered_resps": [["-20.625", "False"], ["-23.625", "False"], ["-25.375", "False"], ["-22.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f74b5395cc719d987d9f9dc26acf5fd89aee72cddcf939a95e574064c89f6531", "prompt_hash": "9ab8782991e829f806226ffffd25f2d4073e0f28dee78c096fa711fcef28788f", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 221, "doc": {"id": "7-1108", "question_stem": "Dry environments often", "choices": {"text": ["liberally use water for everything", "allow plants to flourish", "require people to move", "institute rules about water usage"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Dry environments often", "arg_1": " liberally use water for everything"}, "gen_args_1": {"arg_0": "Dry environments often", "arg_1": " allow plants to flourish"}, "gen_args_2": {"arg_0": "Dry environments often", "arg_1": " require people to move"}, "gen_args_3": {"arg_0": "Dry environments often", "arg_1": " institute rules about water usage"}}, "resps": [[["-32.0", "False"]], [["-13.875", "False"]], [["-19.125", "False"]], [["-37.25", "False"]]], "filtered_resps": [["-32.0", "False"], ["-13.875", "False"], ["-19.125", "False"], ["-37.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "41c4abe0f19df21edebe04b4931f07583230db541d7fdc6ab51e541329985e30", "prompt_hash": "4412d1cb1360d02e79e24e27926e2dc11daca30c8bd99a45734ac1be39dad8cf", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 222, "doc": {"id": "7-852", "question_stem": "Phloem moves things around a plant similar to how", "choices": {"text": ["blood moves in a body", "leaves move in the wind", "water moves in a system", "cars move on a street"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Phloem moves things around a plant similar to how", "arg_1": " blood moves in a body"}, "gen_args_1": {"arg_0": "Phloem moves things around a plant similar to how", "arg_1": " leaves move in the wind"}, "gen_args_2": {"arg_0": "Phloem moves things around a plant similar to how", "arg_1": " water moves in a system"}, "gen_args_3": {"arg_0": "Phloem moves things around a plant similar to how", "arg_1": " cars move on a street"}}, "resps": [[["-11.5625", "False"]], [["-24.75", "False"]], [["-11.875", "False"]], [["-19.875", "False"]]], "filtered_resps": [["-11.5625", "False"], ["-24.75", "False"], ["-11.875", "False"], ["-19.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "0fe34ad442d2feb523d3eedcd31ed6e69714ad7aebbfbbe6bae7ba20534fe492", "prompt_hash": "dd50005d28cd50001ef479a0834c9a93e6bbeb6599e3bc805f306cdf6e835db0", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 223, "doc": {"id": "761", "question_stem": "Where would a polar bear be most comfortable?", "choices": {"text": ["Arizona", "Georgia", "Florida", "Nebraska"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Where would a polar bear be most comfortable?", "arg_1": " Arizona"}, "gen_args_1": {"arg_0": "Where would a polar bear be most comfortable?", "arg_1": " Georgia"}, "gen_args_2": {"arg_0": "Where would a polar bear be most comfortable?", "arg_1": " Florida"}, "gen_args_3": {"arg_0": "Where would a polar bear be most comfortable?", "arg_1": " Nebraska"}}, "resps": [[["-17.0", "False"]], [["-16.25", "False"]], [["-15.5", "False"]], [["-20.375", "False"]]], "filtered_resps": [["-17.0", "False"], ["-16.25", "False"], ["-15.5", "False"], ["-20.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a4e31d8e0a6510e1a05552c88f2659e69a46e51b83cd37d93754284c2c776b56", "prompt_hash": "46073fb76748d3734a8a56644543d83a63fbdde9aa15bca120c060b3024b7a36", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 224, "doc": {"id": "8-318", "question_stem": "When a plane is in the sky and is several miles away, the light seen is barely visible, but when it is drawing near", "choices": {"text": ["light is far away", "light is more easily seen", "light is more distant", "light is further away"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "When a plane is in the sky and is several miles away, the light seen is barely visible, but when it is drawing near", "arg_1": " light is far away"}, "gen_args_1": {"arg_0": "When a plane is in the sky and is several miles away, the light seen is barely visible, but when it is drawing near", "arg_1": " light is more easily seen"}, "gen_args_2": {"arg_0": "When a plane is in the sky and is several miles away, the light seen is barely visible, but when it is drawing near", "arg_1": " light is more distant"}, "gen_args_3": {"arg_0": "When a plane is in the sky and is several miles away, the light seen is barely visible, but when it is drawing near", "arg_1": " light is further away"}}, "resps": [[["-24.0", "False"]], [["-17.5", "False"]], [["-23.875", "False"]], [["-25.5", "False"]]], "filtered_resps": [["-24.0", "False"], ["-17.5", "False"], ["-23.875", "False"], ["-25.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "87e73ff00df9358436362d3606476eb13c9acd9cdd6fa73f037be565ea1dd1d0", "prompt_hash": "577ef4e0d27a7365ab459131fe4456a8f9ddd6e1bd3845a5a3b890d25b534003", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 225, "doc": {"id": "636", "question_stem": "Members of rock bands often perform with", "choices": {"text": ["flutes", "sandals", "earplugs", "gloves"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Members of rock bands often perform with", "arg_1": " flutes"}, "gen_args_1": {"arg_0": "Members of rock bands often perform with", "arg_1": " sandals"}, "gen_args_2": {"arg_0": "Members of rock bands often perform with", "arg_1": " earplugs"}, "gen_args_3": {"arg_0": "Members of rock bands often perform with", "arg_1": " gloves"}}, "resps": [[["-10.8125", "False"]], [["-14.1875", "False"]], [["-10.9375", "False"]], [["-9.625", "False"]]], "filtered_resps": [["-10.8125", "False"], ["-14.1875", "False"], ["-10.9375", "False"], ["-9.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "0c246cb0970678e12858be393a491366d2ea6bbb4ad70f1e9250191398103d13", "prompt_hash": "285e193a4678ff9060533c71059e4816f54b019eaa103ed22310e14b5cf61fb9", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 226, "doc": {"id": "7-444", "question_stem": "Muscles move bones to produce movement like when", "choices": {"text": ["arms are resting", "hair is growing", "smiles are invisible", "toes are wiggled"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Muscles move bones to produce movement like when", "arg_1": " arms are resting"}, "gen_args_1": {"arg_0": "Muscles move bones to produce movement like when", "arg_1": " hair is growing"}, "gen_args_2": {"arg_0": "Muscles move bones to produce movement like when", "arg_1": " smiles are invisible"}, "gen_args_3": {"arg_0": "Muscles move bones to produce movement like when", "arg_1": " toes are wiggled"}}, "resps": [[["-28.25", "False"]], [["-30.25", "False"]], [["-34.0", "False"]], [["-26.25", "False"]]], "filtered_resps": [["-28.25", "False"], ["-30.25", "False"], ["-34.0", "False"], ["-26.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "c486ff346600469176d9333cbe533896ab85e7e155b6c6d69133ff8a961fb631", "prompt_hash": "67b05057773f18b4d07b78f2a4949390e25404f503c237182e4d6e4e12bf313e", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 227, "doc": {"id": "8-57", "question_stem": "How is electricity produced from the ocean?", "choices": {"text": ["decaying organic material from sealife", "energy is accessed underwater from tides", "drills to access oil supplies", "chemical reactions produced from the salt in the water"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "How is electricity produced from the ocean?", "arg_1": " decaying organic material from sealife"}, "gen_args_1": {"arg_0": "How is electricity produced from the ocean?", "arg_1": " energy is accessed underwater from tides"}, "gen_args_2": {"arg_0": "How is electricity produced from the ocean?", "arg_1": " drills to access oil supplies"}, "gen_args_3": {"arg_0": "How is electricity produced from the ocean?", "arg_1": " chemical reactions produced from the salt in the water"}}, "resps": [[["-43.75", "False"]], [["-46.5", "False"]], [["-47.5", "False"]], [["-44.0", "False"]]], "filtered_resps": [["-43.75", "False"], ["-46.5", "False"], ["-47.5", "False"], ["-44.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "7213ec03ffe72b5b8a38948eb75314ef38520c2254eae66f4aa41f8d3a14c09b", "prompt_hash": "5cc474782281a38767d2c03f4121941a65919f62052e33f317a80bc9c03067f4", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 228, "doc": {"id": "9-187", "question_stem": "small reptile's diet consists mostly of", "choices": {"text": ["invertebrates", "insects", "mammals", "fish"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "small reptile's diet consists mostly of", "arg_1": " invertebrates"}, "gen_args_1": {"arg_0": "small reptile's diet consists mostly of", "arg_1": " insects"}, "gen_args_2": {"arg_0": "small reptile's diet consists mostly of", "arg_1": " mammals"}, "gen_args_3": {"arg_0": "small reptile's diet consists mostly of", "arg_1": " fish"}}, "resps": [[["-5.25", "False"]], [["-0.9296875", "True"]], [["-11.875", "False"]], [["-8.75", "False"]]], "filtered_resps": [["-5.25", "False"], ["-0.9296875", "True"], ["-11.875", "False"], ["-8.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a98f4eb4d562415536f7319e54630e9798af9d092592a0bb3ef04148469cb6b4", "prompt_hash": "5647277afcb5406862cfd21e478fb30e1538f20a3dec83b1a67169f0a3af5944", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 229, "doc": {"id": "1345", "question_stem": "The life work of a flower is to", "choices": {"text": ["provide nice scents", "be successfully fertilized", "grow very tall", "look pretty"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "The life work of a flower is to", "arg_1": " provide nice scents"}, "gen_args_1": {"arg_0": "The life work of a flower is to", "arg_1": " be successfully fertilized"}, "gen_args_2": {"arg_0": "The life work of a flower is to", "arg_1": " grow very tall"}, "gen_args_3": {"arg_0": "The life work of a flower is to", "arg_1": " look pretty"}}, "resps": [[["-23.625", "False"]], [["-24.0", "False"]], [["-16.375", "False"]], [["-12.125", "False"]]], "filtered_resps": [["-23.625", "False"], ["-24.0", "False"], ["-16.375", "False"], ["-12.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "390ac293115e023fc2039f8bce9de4f3782bbd2fab8d813250dffee72bb5daad", "prompt_hash": "7aefe6a8d060f10f70ed2dc619c4419c5de24d1f2fc8f4f8ecffed24650ac316", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 230, "doc": {"id": "8-59", "question_stem": "Which of these would create the most sound if struck with a metal spoon?", "choices": {"text": ["the plastic water bottle", "the backside of a person", "the hair on a doll", "the chassis of a car"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Which of these would create the most sound if struck with a metal spoon?", "arg_1": " the plastic water bottle"}, "gen_args_1": {"arg_0": "Which of these would create the most sound if struck with a metal spoon?", "arg_1": " the backside of a person"}, "gen_args_2": {"arg_0": "Which of these would create the most sound if struck with a metal spoon?", "arg_1": " the hair on a doll"}, "gen_args_3": {"arg_0": "Which of these would create the most sound if struck with a metal spoon?", "arg_1": " the chassis of a car"}}, "resps": [[["-27.875", "False"]], [["-32.0", "False"]], [["-33.75", "False"]], [["-25.0", "False"]]], "filtered_resps": [["-27.875", "False"], ["-32.0", "False"], ["-33.75", "False"], ["-25.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "6d5eb5665a24e26ee86df6a498043a7c664dc3fbf5c8713b5c1a30494aeeb11d", "prompt_hash": "542380735c93f12b5f520118257f903549e1fda2ded739ea694ca18e738753f3", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 231, "doc": {"id": "178", "question_stem": "During landslides there is often a lot of", "choices": {"text": ["air", "mud", "snow", "wind"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "During landslides there is often a lot of", "arg_1": " air"}, "gen_args_1": {"arg_0": "During landslides there is often a lot of", "arg_1": " mud"}, "gen_args_2": {"arg_0": "During landslides there is often a lot of", "arg_1": " snow"}, "gen_args_3": {"arg_0": "During landslides there is often a lot of", "arg_1": " wind"}}, "resps": [[["-6.3125", "False"]], [["-4.6875", "False"]], [["-5.5", "False"]], [["-7.9375", "False"]]], "filtered_resps": [["-6.3125", "False"], ["-4.6875", "False"], ["-5.5", "False"], ["-7.9375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "6e969544d13b3c71c4a6c7b43be6783771a7320f07ac459ff0c8b4683c9d7bae", "prompt_hash": "89da988a28b130f5114a9d4afd4ba73cb242894877f5eb5295f29efb4b13ee05", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 232, "doc": {"id": "9-1186", "question_stem": "An example of a chemical reaction would be", "choices": {"text": ["A rusty fence", "Sleeping", "Drinking water", "Rain"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "An example of a chemical reaction would be", "arg_1": " A rusty fence"}, "gen_args_1": {"arg_0": "An example of a chemical reaction would be", "arg_1": " Sleeping"}, "gen_args_2": {"arg_0": "An example of a chemical reaction would be", "arg_1": " Drinking water"}, "gen_args_3": {"arg_0": "An example of a chemical reaction would be", "arg_1": " Rain"}}, "resps": [[["-45.0", "False"]], [["-17.875", "False"]], [["-22.0", "False"]], [["-14.625", "False"]]], "filtered_resps": [["-45.0", "False"], ["-17.875", "False"], ["-22.0", "False"], ["-14.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "cb5ac14e5dd64f0f7aef01c0d3d2539bd79aa042f9df5629990f3777b20df3a3", "prompt_hash": "dcf3e57dd29ba1aa68ee1893588a7e3b5e976f5201b91dab345e704c540d67b4", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 233, "doc": {"id": "82", "question_stem": "The size of an object and the ability to see it more easily have what kind of relationship?", "choices": {"text": ["equal", "inverse", "direct", "reverse"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "The size of an object and the ability to see it more easily have what kind of relationship?", "arg_1": " equal"}, "gen_args_1": {"arg_0": "The size of an object and the ability to see it more easily have what kind of relationship?", "arg_1": " inverse"}, "gen_args_2": {"arg_0": "The size of an object and the ability to see it more easily have what kind of relationship?", "arg_1": " direct"}, "gen_args_3": {"arg_0": "The size of an object and the ability to see it more easily have what kind of relationship?", "arg_1": " reverse"}}, "resps": [[["-16.875", "False"]], [["-15.6875", "False"]], [["-15.9375", "False"]], [["-17.5", "False"]]], "filtered_resps": [["-16.875", "False"], ["-15.6875", "False"], ["-15.9375", "False"], ["-17.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "273744dbb2da74343a2903d311990f844d97e0d4528e1d5c4e24fae5e866c58a", "prompt_hash": "41c7e47996f6503bbf86e2cff1636cf0fd477b2b245bd434ff13960d2ca66af6", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 234, "doc": {"id": "8-165", "question_stem": "Tuna primarily eat", "choices": {"text": ["parasites, soybeans and flaxseeds", "sea turtles, sharks and coral reefs", "spineless marine organisms, cartilaginous and gelatinous organisms", "sea vegetables like kelp, Irish moss and Arame"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Tuna primarily eat", "arg_1": " parasites, soybeans and flaxseeds"}, "gen_args_1": {"arg_0": "Tuna primarily eat", "arg_1": " sea turtles, sharks and coral reefs"}, "gen_args_2": {"arg_0": "Tuna primarily eat", "arg_1": " spineless marine organisms, cartilaginous and gelatinous organisms"}, "gen_args_3": {"arg_0": "Tuna primarily eat", "arg_1": " sea vegetables like kelp, Irish moss and Arame"}}, "resps": [[["-32.0", "False"]], [["-26.375", "False"]], [["-65.5", "False"]], [["-41.75", "False"]]], "filtered_resps": [["-32.0", "False"], ["-26.375", "False"], ["-65.5", "False"], ["-41.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "75320b18b3cbad823b4c24166378afec69369fd52e23fe9c92e63cd656d34c0a", "prompt_hash": "381646c7fc2ac959c984c5575f5f05074a70bb2e30099b6fe4e9e38a705cb890", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 235, "doc": {"id": "404", "question_stem": "Evaporation of water can lead to", "choices": {"text": ["waterfalls", "blizzards", "earthquakes", "hot springs"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Evaporation of water can lead to", "arg_1": " waterfalls"}, "gen_args_1": {"arg_0": "Evaporation of water can lead to", "arg_1": " blizzards"}, "gen_args_2": {"arg_0": "Evaporation of water can lead to", "arg_1": " earthquakes"}, "gen_args_3": {"arg_0": "Evaporation of water can lead to", "arg_1": " hot springs"}}, "resps": [[["-13.1875", "False"]], [["-15.3125", "False"]], [["-10.25", "False"]], [["-16.5", "False"]]], "filtered_resps": [["-13.1875", "False"], ["-15.3125", "False"], ["-10.25", "False"], ["-16.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e9eef03d7ac6e05708d1a397629c6fc91174f34f6616887698de78896f2c11ac", "prompt_hash": "5c0fd891eb29e3c661e7ee478c7212f3c80166d19ed145891fd2ceb67dfc6710", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 236, "doc": {"id": "279", "question_stem": "What is an electrical energy conductor?", "choices": {"text": ["horseshoe", "tire", "cotton shirt", "maple branch"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "What is an electrical energy conductor?", "arg_1": " horseshoe"}, "gen_args_1": {"arg_0": "What is an electrical energy conductor?", "arg_1": " tire"}, "gen_args_2": {"arg_0": "What is an electrical energy conductor?", "arg_1": " cotton shirt"}, "gen_args_3": {"arg_0": "What is an electrical energy conductor?", "arg_1": " maple branch"}}, "resps": [[["-23.125", "False"]], [["-25.125", "False"]], [["-35.25", "False"]], [["-36.0", "False"]]], "filtered_resps": [["-23.125", "False"], ["-25.125", "False"], ["-35.25", "False"], ["-36.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "1d8e818c3f77a570fb4918eaae61a49ec9e601e9dc921a54cf044614d8b0bc21", "prompt_hash": "02901910e37a78c1ffc58b35255c95a2094bee87634b3c2fd5a38a5c01332cf9", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 237, "doc": {"id": "9-532", "question_stem": "What kind of substance will cool when it touches a cold object?", "choices": {"text": ["warm", "frozen", "chilly", "cold"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "What kind of substance will cool when it touches a cold object?", "arg_1": " warm"}, "gen_args_1": {"arg_0": "What kind of substance will cool when it touches a cold object?", "arg_1": " frozen"}, "gen_args_2": {"arg_0": "What kind of substance will cool when it touches a cold object?", "arg_1": " chilly"}, "gen_args_3": {"arg_0": "What kind of substance will cool when it touches a cold object?", "arg_1": " cold"}}, "resps": [[["-15.1875", "False"]], [["-20.75", "False"]], [["-21.625", "False"]], [["-14.4375", "False"]]], "filtered_resps": [["-15.1875", "False"], ["-20.75", "False"], ["-21.625", "False"], ["-14.4375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "cea5c1a1bad9379ed2abaf1c1a1c6b294e4963140cc1955d552962702262e1ba", "prompt_hash": "8f2838690c3eec8832aaebde0d43da10cf7f90c810be2f6496f6de3b586b5be1", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 238, "doc": {"id": "268", "question_stem": "Scraping an object", "choices": {"text": ["may cause the object to grow in size", "may cause the object to fall", "may cause pieces to flake off the object", "may cause the object to snap in half"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Scraping an object", "arg_1": " may cause the object to grow in size"}, "gen_args_1": {"arg_0": "Scraping an object", "arg_1": " may cause the object to fall"}, "gen_args_2": {"arg_0": "Scraping an object", "arg_1": " may cause pieces to flake off the object"}, "gen_args_3": {"arg_0": "Scraping an object", "arg_1": " may cause the object to snap in half"}}, "resps": [[["-29.125", "False"]], [["-25.125", "False"]], [["-38.75", "False"]], [["-36.75", "False"]]], "filtered_resps": [["-29.125", "False"], ["-25.125", "False"], ["-38.75", "False"], ["-36.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "77882cef5ebbd4f275c83f1d5e35f64853a7a4e4d48674ab5fe108a98898f3bf", "prompt_hash": "168b51bcd92a0dcbf84fa1071fbce9f8dd771e5528a89a7c6e551ed733f2b45f", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 239, "doc": {"id": "7-1018", "question_stem": "A pulley is used to lift a flag on a flagpole by", "choices": {"text": ["moving a rope sideways", "putting something in the air", "moving things with wheels", "yanking string up a wheel"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "A pulley is used to lift a flag on a flagpole by", "arg_1": " moving a rope sideways"}, "gen_args_1": {"arg_0": "A pulley is used to lift a flag on a flagpole by", "arg_1": " putting something in the air"}, "gen_args_2": {"arg_0": "A pulley is used to lift a flag on a flagpole by", "arg_1": " moving things with wheels"}, "gen_args_3": {"arg_0": "A pulley is used to lift a flag on a flagpole by", "arg_1": " yanking string up a wheel"}}, "resps": [[["-18.0", "False"]], [["-27.625", "False"]], [["-30.5", "False"]], [["-50.5", "False"]]], "filtered_resps": [["-18.0", "False"], ["-27.625", "False"], ["-30.5", "False"], ["-50.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "9c480216f66b6db18b8857d966225b16a16dfbd8ed56222326ab730567f8a55d", "prompt_hash": "8817999eef5fc3447bb3b58bdc0965abb0f125d9c5bf958a9f2bfe5de4a9455e", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 240, "doc": {"id": "1756", "question_stem": "Which animal lays eggs", "choices": {"text": ["emus", "dogs", "squirrels", "giraffes"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Which animal lays eggs", "arg_1": " emus"}, "gen_args_1": {"arg_0": "Which animal lays eggs", "arg_1": " dogs"}, "gen_args_2": {"arg_0": "Which animal lays eggs", "arg_1": " squirrels"}, "gen_args_3": {"arg_0": "Which animal lays eggs", "arg_1": " giraffes"}}, "resps": [[["-22.0", "False"]], [["-20.0", "False"]], [["-16.875", "False"]], [["-20.375", "False"]]], "filtered_resps": [["-22.0", "False"], ["-20.0", "False"], ["-16.875", "False"], ["-20.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "074b8a1b009317d2c4704385500f456592bd8073c769fa75a53f22c7df881a40", "prompt_hash": "980b37f9b5c644b94cc9592086382f8bf8015733676e38aa34edcef0f8f83e9b", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 241, "doc": {"id": "1137", "question_stem": "Dunes can be made out of the same thing as", "choices": {"text": ["clothes", "food", "forests", "castles"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Dunes can be made out of the same thing as", "arg_1": " clothes"}, "gen_args_1": {"arg_0": "Dunes can be made out of the same thing as", "arg_1": " food"}, "gen_args_2": {"arg_0": "Dunes can be made out of the same thing as", "arg_1": " forests"}, "gen_args_3": {"arg_0": "Dunes can be made out of the same thing as", "arg_1": " castles"}}, "resps": [[["-10.1875", "False"]], [["-11.625", "False"]], [["-10.125", "False"]], [["-8.5", "False"]]], "filtered_resps": [["-10.1875", "False"], ["-11.625", "False"], ["-10.125", "False"], ["-8.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "09fcdc7dedfacc420b7b7080ba32982491b504bba5e9e1d5dcfa6ecedc065def", "prompt_hash": "4648f2a0dbd76bfdb2029b7af20247f0ba90200466048b8d902989e7a21e5240", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 242, "doc": {"id": "7-203", "question_stem": "A learned behavior is exhibited when", "choices": {"text": ["squinting in bright light", "inhaling and exhaling during sleep", "blinking and gulping air", "nailing up a picture frame"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "A learned behavior is exhibited when", "arg_1": " squinting in bright light"}, "gen_args_1": {"arg_0": "A learned behavior is exhibited when", "arg_1": " inhaling and exhaling during sleep"}, "gen_args_2": {"arg_0": "A learned behavior is exhibited when", "arg_1": " blinking and gulping air"}, "gen_args_3": {"arg_0": "A learned behavior is exhibited when", "arg_1": " nailing up a picture frame"}}, "resps": [[["-31.5", "False"]], [["-32.75", "False"]], [["-38.5", "False"]], [["-39.5", "False"]]], "filtered_resps": [["-31.5", "False"], ["-32.75", "False"], ["-38.5", "False"], ["-39.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "45dd1ba81440d64a68a8252936422a2a92877a238115f42c3b5f132566e1e2f9", "prompt_hash": "c55d30da9639a6ac38648c628d47fc416ec209af284fffd024f646045c895d68", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 243, "doc": {"id": "745", "question_stem": "Wax can be used similarly to", "choices": {"text": ["wood", "rubber", "water", "metal"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Wax can be used similarly to", "arg_1": " wood"}, "gen_args_1": {"arg_0": "Wax can be used similarly to", "arg_1": " rubber"}, "gen_args_2": {"arg_0": "Wax can be used similarly to", "arg_1": " water"}, "gen_args_3": {"arg_0": "Wax can be used similarly to", "arg_1": " metal"}}, "resps": [[["-3.421875", "False"]], [["-6.65625", "False"]], [["-6.40625", "False"]], [["-4.78125", "False"]]], "filtered_resps": [["-3.421875", "False"], ["-6.65625", "False"], ["-6.40625", "False"], ["-4.78125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a1f8c032741f5a81de259998b37645b83859ceb1352766dbe7484b2f4d297dda", "prompt_hash": "9caa36c5a10074430ec0e28bbf3a04fbf0c1f5c860c9373ef3e5fc05ff254175", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 244, "doc": {"id": "7-902", "question_stem": "Bill planted rapeseed in his field one year and soybeans the next in order to", "choices": {"text": ["get bigger yields", "make things boring", "keep things random", "get smaller yields"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Bill planted rapeseed in his field one year and soybeans the next in order to", "arg_1": " get bigger yields"}, "gen_args_1": {"arg_0": "Bill planted rapeseed in his field one year and soybeans the next in order to", "arg_1": " make things boring"}, "gen_args_2": {"arg_0": "Bill planted rapeseed in his field one year and soybeans the next in order to", "arg_1": " keep things random"}, "gen_args_3": {"arg_0": "Bill planted rapeseed in his field one year and soybeans the next in order to", "arg_1": " get smaller yields"}}, "resps": [[["-10.1875", "False"]], [["-26.0", "False"]], [["-24.0", "False"]], [["-12.9375", "False"]]], "filtered_resps": [["-10.1875", "False"], ["-26.0", "False"], ["-24.0", "False"], ["-12.9375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "c6066c32e94c2f02454a290c1cc81b37a9affced7a5f723b6b28442c924df4ff", "prompt_hash": "0b554bf49878dedd9f6e93f2f7ae554885c04dd9a3d8fa5a7e30b881871764fe", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 245, "doc": {"id": "1095", "question_stem": "What is the primary reason my duck feather filled jacket works well against the snow", "choices": {"text": ["feathers slows heat transfer", "the natural duck wax", "a synthetic thick liner", "small flexible solar panels"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "What is the primary reason my duck feather filled jacket works well against the snow", "arg_1": " feathers slows heat transfer"}, "gen_args_1": {"arg_0": "What is the primary reason my duck feather filled jacket works well against the snow", "arg_1": " the natural duck wax"}, "gen_args_2": {"arg_0": "What is the primary reason my duck feather filled jacket works well against the snow", "arg_1": " a synthetic thick liner"}, "gen_args_3": {"arg_0": "What is the primary reason my duck feather filled jacket works well against the snow", "arg_1": " small flexible solar panels"}}, "resps": [[["-43.25", "False"]], [["-39.0", "False"]], [["-38.5", "False"]], [["-49.75", "False"]]], "filtered_resps": [["-43.25", "False"], ["-39.0", "False"], ["-38.5", "False"], ["-49.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "0251c1ac0839d0b9a64eb49b425ac7d91fb7bc502d2aa708424fec7364116e92", "prompt_hash": "e01b6b3d55134d64b7c41e26be956705766f8fa29f66af2a06195dd5cc74a638", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 246, "doc": {"id": "7-163", "question_stem": "Turbines churning seawater can be used to produce what?", "choices": {"text": ["a charge for appliances", "large quantities of soup", "large schools of fish", "creating some sharp cheese"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Turbines churning seawater can be used to produce what?", "arg_1": " a charge for appliances"}, "gen_args_1": {"arg_0": "Turbines churning seawater can be used to produce what?", "arg_1": " large quantities of soup"}, "gen_args_2": {"arg_0": "Turbines churning seawater can be used to produce what?", "arg_1": " large schools of fish"}, "gen_args_3": {"arg_0": "Turbines churning seawater can be used to produce what?", "arg_1": " creating some sharp cheese"}}, "resps": [[["-44.75", "False"]], [["-30.25", "False"]], [["-28.25", "False"]], [["-49.0", "False"]]], "filtered_resps": [["-44.75", "False"], ["-30.25", "False"], ["-28.25", "False"], ["-49.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2dbf0fc0d1205de69efcc3feb661ef1755ad7056614ef248c8ff5cb08b8dd0e6", "prompt_hash": "831d791fd2bcafb6a85a0c6f935a145304ead098cbdfbc4d89f2de717a2ecb5a", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 247, "doc": {"id": "9-858", "question_stem": "What will increase when a substance absorbs solar energy?", "choices": {"text": ["weight", "height", "hotness", "nutrition"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "What will increase when a substance absorbs solar energy?", "arg_1": " weight"}, "gen_args_1": {"arg_0": "What will increase when a substance absorbs solar energy?", "arg_1": " height"}, "gen_args_2": {"arg_0": "What will increase when a substance absorbs solar energy?", "arg_1": " hotness"}, "gen_args_3": {"arg_0": "What will increase when a substance absorbs solar energy?", "arg_1": " nutrition"}}, "resps": [[["-17.625", "False"]], [["-19.375", "False"]], [["-20.375", "False"]], [["-22.375", "False"]]], "filtered_resps": [["-17.625", "False"], ["-19.375", "False"], ["-20.375", "False"], ["-22.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a8d27114e861802717fe5a6ae90508005cfa6827b76b6a57ec80d9218c00645f", "prompt_hash": "5f26acbbc4936d48903b9d3db1a4ddffdabaee1817e359af5f7c72c9b8f3d0a4", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 248, "doc": {"id": "1530", "question_stem": "Which of these travels through the air?", "choices": {"text": ["planets", "thoughts", "automobile", "music"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Which of these travels through the air?", "arg_1": " planets"}, "gen_args_1": {"arg_0": "Which of these travels through the air?", "arg_1": " thoughts"}, "gen_args_2": {"arg_0": "Which of these travels through the air?", "arg_1": " automobile"}, "gen_args_3": {"arg_0": "Which of these travels through the air?", "arg_1": " music"}}, "resps": [[["-19.0", "False"]], [["-18.0", "False"]], [["-21.0", "False"]], [["-19.875", "False"]]], "filtered_resps": [["-19.0", "False"], ["-18.0", "False"], ["-21.0", "False"], ["-19.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f4d7db9bf55b279c3e6f0b674a8ffcb313738d61b30a52dbe22c077dcc39c74b", "prompt_hash": "a977058a9538784126718f8a81d8ccd851cf7ff0bfa4d4cc996a71978842bf77", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 249, "doc": {"id": "9-993", "question_stem": "A company makes notebooks for college courses, so their main material is", "choices": {"text": ["chips", "water", "grass", "trees"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "A company makes notebooks for college courses, so their main material is", "arg_1": " chips"}, "gen_args_1": {"arg_0": "A company makes notebooks for college courses, so their main material is", "arg_1": " water"}, "gen_args_2": {"arg_0": "A company makes notebooks for college courses, so their main material is", "arg_1": " grass"}, "gen_args_3": {"arg_0": "A company makes notebooks for college courses, so their main material is", "arg_1": " trees"}}, "resps": [[["-13.125", "False"]], [["-10.6875", "False"]], [["-14.75", "False"]], [["-11.5625", "False"]]], "filtered_resps": [["-13.125", "False"], ["-10.6875", "False"], ["-14.75", "False"], ["-11.5625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "b3c0ce365ba54bf413e323772d13b88ac42bdfa42fd1cb98e11e9d727eb12544", "prompt_hash": "3e4cc098eb7835daf32716e86540ec67b6493562c8e07f4b8d3fabb7dd2f03e5", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 250, "doc": {"id": "8-340", "question_stem": "Rain is usually guaranteed when all are present but", "choices": {"text": ["cirrus clouds", "cumulus clouds", "hail stones", "direct sunshine"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Rain is usually guaranteed when all are present but", "arg_1": " cirrus clouds"}, "gen_args_1": {"arg_0": "Rain is usually guaranteed when all are present but", "arg_1": " cumulus clouds"}, "gen_args_2": {"arg_0": "Rain is usually guaranteed when all are present but", "arg_1": " hail stones"}, "gen_args_3": {"arg_0": "Rain is usually guaranteed when all are present but", "arg_1": " direct sunshine"}}, "resps": [[["-16.875", "False"]], [["-15.625", "False"]], [["-18.75", "False"]], [["-17.875", "False"]]], "filtered_resps": [["-16.875", "False"], ["-15.625", "False"], ["-18.75", "False"], ["-17.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "fbacf6c17e57ff0e1ded2b287bcad51f5ae1e9e5d24cfa8b3d41fc1b610389b1", "prompt_hash": "e5c800e8af113d9f22f8679fa715bcff6bea48d9afe1a47c573f09d7d372af46", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 251, "doc": {"id": "3", "question_stem": "Sources of air pollution are", "choices": {"text": ["Walking", "Landfills", "Water", "Chips"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Sources of air pollution are", "arg_1": " Walking"}, "gen_args_1": {"arg_0": "Sources of air pollution are", "arg_1": " Landfills"}, "gen_args_2": {"arg_0": "Sources of air pollution are", "arg_1": " Water"}, "gen_args_3": {"arg_0": "Sources of air pollution are", "arg_1": " Chips"}}, "resps": [[["-21.875", "False"]], [["-21.375", "False"]], [["-20.625", "False"]], [["-26.0", "False"]]], "filtered_resps": [["-21.875", "False"], ["-21.375", "False"], ["-20.625", "False"], ["-26.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "606ef130475fe0487d35bd01fa8cf312a2ae93037a7ed4eb70d172a43dcedcef", "prompt_hash": "dd671623a1a4d0f084d12f5b0084be2cc4f5c6760e030cf44c311b6a4b3fc214", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 252, "doc": {"id": "1074", "question_stem": "What are iron nails made out of?", "choices": {"text": ["wood", "plastic", "metal", "glass"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "What are iron nails made out of?", "arg_1": " wood"}, "gen_args_1": {"arg_0": "What are iron nails made out of?", "arg_1": " plastic"}, "gen_args_2": {"arg_0": "What are iron nails made out of?", "arg_1": " metal"}, "gen_args_3": {"arg_0": "What are iron nails made out of?", "arg_1": " glass"}}, "resps": [[["-19.5", "False"]], [["-20.125", "False"]], [["-16.875", "False"]], [["-19.875", "False"]]], "filtered_resps": [["-19.5", "False"], ["-20.125", "False"], ["-16.875", "False"], ["-19.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "888f476b0bc2581dde6eaf088f445e5171a8531fb4860790d074f5e369765362", "prompt_hash": "380a3d16c71b5aa92196e54703a3f9d58a6c8003b6eda2354e57fbc12bad6527", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 253, "doc": {"id": "9-431", "question_stem": "Putting one kind of soda into the same cup as another kind of soda is doing what to the substances?", "choices": {"text": ["combining", "drinking", "Subtracting", "throwing"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Putting one kind of soda into the same cup as another kind of soda is doing what to the substances?", "arg_1": " combining"}, "gen_args_1": {"arg_0": "Putting one kind of soda into the same cup as another kind of soda is doing what to the substances?", "arg_1": " drinking"}, "gen_args_2": {"arg_0": "Putting one kind of soda into the same cup as another kind of soda is doing what to the substances?", "arg_1": " Subtracting"}, "gen_args_3": {"arg_0": "Putting one kind of soda into the same cup as another kind of soda is doing what to the substances?", "arg_1": " throwing"}}, "resps": [[["-15.75", "False"]], [["-18.125", "False"]], [["-15.9375", "False"]], [["-18.0", "False"]]], "filtered_resps": [["-15.75", "False"], ["-18.125", "False"], ["-15.9375", "False"], ["-18.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "8ddd93e675ee4f2687a0229b7211ab87be83022310e02f08eac231016a983049", "prompt_hash": "37bffb46a284fd457bf49667acb25e56c31263f005e294083005f0918b27e3ca", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 254, "doc": {"id": "9-638", "question_stem": "A moth leaving it's cocoon is the final step in a", "choices": {"text": ["life cycle", "transformation", "recycling process", "chemical reaction"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "A moth leaving it's cocoon is the final step in a", "arg_1": " life cycle"}, "gen_args_1": {"arg_0": "A moth leaving it's cocoon is the final step in a", "arg_1": " transformation"}, "gen_args_2": {"arg_0": "A moth leaving it's cocoon is the final step in a", "arg_1": " recycling process"}, "gen_args_3": {"arg_0": "A moth leaving it's cocoon is the final step in a", "arg_1": " chemical reaction"}}, "resps": [[["-2.765625", "False"]], [["-4.8125", "False"]], [["-13.875", "False"]], [["-11.1875", "False"]]], "filtered_resps": [["-2.765625", "False"], ["-4.8125", "False"], ["-13.875", "False"], ["-11.1875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "fbb789c4dea9bf46ad46f417c95dbb3eab557dbf2fbf5f1056378ab8ca654bef", "prompt_hash": "7344a77a16264088b0ca4a29dbfb8ce0517a92e655413b329e07a7c24b423ce2", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 255, "doc": {"id": "9-352", "question_stem": "Which of these combinations would be desired if someone wanted to make a cutting implement that lasts a long time?", "choices": {"text": ["ice and snow", "sticks and stones", "snow and water", "iron and carbon"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Which of these combinations would be desired if someone wanted to make a cutting implement that lasts a long time?", "arg_1": " ice and snow"}, "gen_args_1": {"arg_0": "Which of these combinations would be desired if someone wanted to make a cutting implement that lasts a long time?", "arg_1": " sticks and stones"}, "gen_args_2": {"arg_0": "Which of these combinations would be desired if someone wanted to make a cutting implement that lasts a long time?", "arg_1": " snow and water"}, "gen_args_3": {"arg_0": "Which of these combinations would be desired if someone wanted to make a cutting implement that lasts a long time?", "arg_1": " iron and carbon"}}, "resps": [[["-32.0", "False"]], [["-28.75", "False"]], [["-32.5", "False"]], [["-23.75", "False"]]], "filtered_resps": [["-32.0", "False"], ["-28.75", "False"], ["-32.5", "False"], ["-23.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "74a6dcb91a7783285a6c16974bf119c1888385b653c0151e961a133326fcd2b3", "prompt_hash": "1f1fc9217d2caaf0667d62d85660b0508fac7f2bcbe7a23acd944f045456ccc3", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 256, "doc": {"id": "226", "question_stem": "Which of the following is powered the same way an electric car is?", "choices": {"text": ["a bicycle", "a motorcycle", "a propane grill", "a blender"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Which of the following is powered the same way an electric car is?", "arg_1": " a bicycle"}, "gen_args_1": {"arg_0": "Which of the following is powered the same way an electric car is?", "arg_1": " a motorcycle"}, "gen_args_2": {"arg_0": "Which of the following is powered the same way an electric car is?", "arg_1": " a propane grill"}, "gen_args_3": {"arg_0": "Which of the following is powered the same way an electric car is?", "arg_1": " a blender"}}, "resps": [[["-26.375", "False"]], [["-27.875", "False"]], [["-32.75", "False"]], [["-30.625", "False"]]], "filtered_resps": [["-26.375", "False"], ["-27.875", "False"], ["-32.75", "False"], ["-30.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "6562763235882f8097e93c8f1ebaac757c163a0859589c167cf50422baffd5da", "prompt_hash": "db63def329877a3e0f668ea24a8b56f086e3dfc5495ddd24a9b16c20e241cbb7", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 257, "doc": {"id": "9-132", "question_stem": "Aluminum is what?", "choices": {"text": ["reprocessable", "plastic", "liquid", "absorbent"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Aluminum is what?", "arg_1": " reprocessable"}, "gen_args_1": {"arg_0": "Aluminum is what?", "arg_1": " plastic"}, "gen_args_2": {"arg_0": "Aluminum is what?", "arg_1": " liquid"}, "gen_args_3": {"arg_0": "Aluminum is what?", "arg_1": " absorbent"}}, "resps": [[["-30.375", "False"]], [["-16.0", "False"]], [["-17.625", "False"]], [["-21.25", "False"]]], "filtered_resps": [["-30.375", "False"], ["-16.0", "False"], ["-17.625", "False"], ["-21.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f19ae3c76a032803dcc1b2b03ba08f396a7f2f2b2055af0944314d9306967c31", "prompt_hash": "3b48f206f9db03ee3028f97ac04de87948f002d6a63a2aca2080c6c0fc72022e", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 258, "doc": {"id": "9-222", "question_stem": "To get warm frogs can", "choices": {"text": ["wear a Christmas sweater", "Drink a hot chocolate", "Go for a run", "sit under a lamp"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "To get warm frogs can", "arg_1": " wear a Christmas sweater"}, "gen_args_1": {"arg_0": "To get warm frogs can", "arg_1": " Drink a hot chocolate"}, "gen_args_2": {"arg_0": "To get warm frogs can", "arg_1": " Go for a run"}, "gen_args_3": {"arg_0": "To get warm frogs can", "arg_1": " sit under a lamp"}}, "resps": [[["-20.625", "False"]], [["-31.875", "False"]], [["-31.5", "False"]], [["-23.0", "False"]]], "filtered_resps": [["-20.625", "False"], ["-31.875", "False"], ["-31.5", "False"], ["-23.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "7fc07c9023aed9da72a8d9887b126844526781d98c261d6ec71e0e4d967dab7c", "prompt_hash": "82e36d029d8f444e5ecdbe1b5bb6d262aefc9837eb3de157a6ea5bb94bf07d86", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 259, "doc": {"id": "9-105", "question_stem": "if you put wine from a jug into a thin bottle, how come it conforms?", "choices": {"text": ["it exhibits absolute rigidity", "it is a solid mass", "all of these", "it is a variable substance"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "if you put wine from a jug into a thin bottle, how come it conforms?", "arg_1": " it exhibits absolute rigidity"}, "gen_args_1": {"arg_0": "if you put wine from a jug into a thin bottle, how come it conforms?", "arg_1": " it is a solid mass"}, "gen_args_2": {"arg_0": "if you put wine from a jug into a thin bottle, how come it conforms?", "arg_1": " all of these"}, "gen_args_3": {"arg_0": "if you put wine from a jug into a thin bottle, how come it conforms?", "arg_1": " it is a variable substance"}}, "resps": [[["-34.25", "False"]], [["-25.125", "False"]], [["-19.75", "False"]], [["-32.0", "False"]]], "filtered_resps": [["-34.25", "False"], ["-25.125", "False"], ["-19.75", "False"], ["-32.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "33ff43095b961c5d85a3a143eae9b45ef03d09395bac2a848306c5528465b65f", "prompt_hash": "e53b12ee6574f2f39252cdc368b19d51bc0f0a423489f230ac578fe23c6445ac", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 260, "doc": {"id": "7-459", "question_stem": "Earthworms create tunnels in", "choices": {"text": ["ice", "dirt", "water", "concrete"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Earthworms create tunnels in", "arg_1": " ice"}, "gen_args_1": {"arg_0": "Earthworms create tunnels in", "arg_1": " dirt"}, "gen_args_2": {"arg_0": "Earthworms create tunnels in", "arg_1": " water"}, "gen_args_3": {"arg_0": "Earthworms create tunnels in", "arg_1": " concrete"}}, "resps": [[["-21.25", "False"]], [["-10.25", "False"]], [["-17.0", "False"]], [["-13.875", "False"]]], "filtered_resps": [["-21.25", "False"], ["-10.25", "False"], ["-17.0", "False"], ["-13.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "3b8cb01be4d805546f37e85c3edc5c64b59338b22bf34f37e524de3e400564d3", "prompt_hash": "fafabef98a967149427bf8c08e97ab71872959db97a63b1baac7d36c11fa70b7", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 261, "doc": {"id": "9-881", "question_stem": "when worms return nutrients from dead organisms to the soil by eating them it is known as", "choices": {"text": ["regurgitation", "decomposition", "recycling", "burial"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "when worms return nutrients from dead organisms to the soil by eating them it is known as", "arg_1": " regurgitation"}, "gen_args_1": {"arg_0": "when worms return nutrients from dead organisms to the soil by eating them it is known as", "arg_1": " decomposition"}, "gen_args_2": {"arg_0": "when worms return nutrients from dead organisms to the soil by eating them it is known as", "arg_1": " recycling"}, "gen_args_3": {"arg_0": "when worms return nutrients from dead organisms to the soil by eating them it is known as", "arg_1": " burial"}}, "resps": [[["-10.8125", "False"]], [["-3.828125", "False"]], [["-9.4375", "False"]], [["-10.6875", "False"]]], "filtered_resps": [["-10.8125", "False"], ["-3.828125", "False"], ["-9.4375", "False"], ["-10.6875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "963945242453894208a55824db83c8f8694c8a82295b6eeefb275fcac2f7f07f", "prompt_hash": "9c786c3d4e051d70b6d37afcb349a888ded847049a8c009e927ef8aa2e552a5b", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 262, "doc": {"id": "280", "question_stem": "The aluminum cans were much hotter than the", "choices": {"text": ["gold jewelry", "wooden fence", "brass doorknob", "steel pole"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "The aluminum cans were much hotter than the", "arg_1": " gold jewelry"}, "gen_args_1": {"arg_0": "The aluminum cans were much hotter than the", "arg_1": " wooden fence"}, "gen_args_2": {"arg_0": "The aluminum cans were much hotter than the", "arg_1": " brass doorknob"}, "gen_args_3": {"arg_0": "The aluminum cans were much hotter than the", "arg_1": " steel pole"}}, "resps": [[["-17.625", "False"]], [["-16.5", "False"]], [["-22.125", "False"]], [["-21.375", "False"]]], "filtered_resps": [["-17.625", "False"], ["-16.5", "False"], ["-22.125", "False"], ["-21.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "38831ad27d53aa23b828368c37187eeb013cc8d199c4e643e401f34c75c8ef8c", "prompt_hash": "2a6c6775f1077d8b491e1843c9e403a1143c77591ff85f7b86156b576f22bd3c", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 263, "doc": {"id": "187", "question_stem": "Which of the following is not an input in photosynthesis?", "choices": {"text": ["sunlight", "oxygen", "water", "carbon dioxide"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Which of the following is not an input in photosynthesis?", "arg_1": " sunlight"}, "gen_args_1": {"arg_0": "Which of the following is not an input in photosynthesis?", "arg_1": " oxygen"}, "gen_args_2": {"arg_0": "Which of the following is not an input in photosynthesis?", "arg_1": " water"}, "gen_args_3": {"arg_0": "Which of the following is not an input in photosynthesis?", "arg_1": " carbon dioxide"}}, "resps": [[["-13.1875", "False"]], [["-22.0", "False"]], [["-14.5625", "False"]], [["-16.5", "False"]]], "filtered_resps": [["-13.1875", "False"], ["-22.0", "False"], ["-14.5625", "False"], ["-16.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "9e24bd6e43a4fd9cc0f1332fb4f517e785fd97a3916e63868062d28ca6e3a2f6", "prompt_hash": "901e9af4eef0f07a9c0e2361faea840e1208f51d56f3a5558039544d84477a28", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 264, "doc": {"id": "8-253", "question_stem": "which of these are you most likely to find in a desert?", "choices": {"text": ["a hammer head shark", "a big tilapia fish", "a prickly horned male lizard", "none of these"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "which of these are you most likely to find in a desert?", "arg_1": " a hammer head shark"}, "gen_args_1": {"arg_0": "which of these are you most likely to find in a desert?", "arg_1": " a big tilapia fish"}, "gen_args_2": {"arg_0": "which of these are you most likely to find in a desert?", "arg_1": " a prickly horned male lizard"}, "gen_args_3": {"arg_0": "which of these are you most likely to find in a desert?", "arg_1": " none of these"}}, "resps": [[["-44.25", "False"]], [["-38.25", "False"]], [["-54.75", "False"]], [["-20.625", "False"]]], "filtered_resps": [["-44.25", "False"], ["-38.25", "False"], ["-54.75", "False"], ["-20.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "bfa9b0a9ba6c1be48b3973ccc36041e030082d6e8e94ee640ca270d38b54d268", "prompt_hash": "5ddda170a061c580ef5b57f670b034528c3fc76de770a57b46a38eeceec6408d", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 265, "doc": {"id": "9-482", "question_stem": "If your dog sits in an oxygen deficient chamber, what happens?", "choices": {"text": ["it will be fine", "it will be happy", "it will be comfortable", "It will pass out"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "If your dog sits in an oxygen deficient chamber, what happens?", "arg_1": " it will be fine"}, "gen_args_1": {"arg_0": "If your dog sits in an oxygen deficient chamber, what happens?", "arg_1": " it will be happy"}, "gen_args_2": {"arg_0": "If your dog sits in an oxygen deficient chamber, what happens?", "arg_1": " it will be comfortable"}, "gen_args_3": {"arg_0": "If your dog sits in an oxygen deficient chamber, what happens?", "arg_1": " It will pass out"}}, "resps": [[["-30.0", "False"]], [["-33.75", "False"]], [["-32.5", "False"]], [["-27.875", "False"]]], "filtered_resps": [["-30.0", "False"], ["-33.75", "False"], ["-32.5", "False"], ["-27.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "193ea59282bfb7d73995af8d2524cf2557e73045fcacddf583c9ee3e822649a4", "prompt_hash": "fc29992826af783e40a2267de6ce37d9a48ce3b97aaa302eb439ceb875e74ebe", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 266, "doc": {"id": "496", "question_stem": "Which of the following can be used to turn on an electrical device?", "choices": {"text": ["solar-rechargeable battery", "a wedge", "a magnet", "pressure gauge"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Which of the following can be used to turn on an electrical device?", "arg_1": " solar-rechargeable battery"}, "gen_args_1": {"arg_0": "Which of the following can be used to turn on an electrical device?", "arg_1": " a wedge"}, "gen_args_2": {"arg_0": "Which of the following can be used to turn on an electrical device?", "arg_1": " a magnet"}, "gen_args_3": {"arg_0": "Which of the following can be used to turn on an electrical device?", "arg_1": " pressure gauge"}}, "resps": [[["-35.25", "False"]], [["-28.0", "False"]], [["-23.875", "False"]], [["-21.0", "False"]]], "filtered_resps": [["-35.25", "False"], ["-28.0", "False"], ["-23.875", "False"], ["-21.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "dedc50d2f5648f4f7a3c7c05ac83d52ae1788b20a602cf699a0706f5c6f38b28", "prompt_hash": "30bf735d2d8590e0248009e06f4ca141fd07c0751a9de05005615bb6959ba497", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 267, "doc": {"id": "630", "question_stem": "Which of the following contains large amounts of salt water?", "choices": {"text": ["The Amazon", "The Nile", "The Indian", "The Mississippi"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Which of the following contains large amounts of salt water?", "arg_1": " The Amazon"}, "gen_args_1": {"arg_0": "Which of the following contains large amounts of salt water?", "arg_1": " The Nile"}, "gen_args_2": {"arg_0": "Which of the following contains large amounts of salt water?", "arg_1": " The Indian"}, "gen_args_3": {"arg_0": "Which of the following contains large amounts of salt water?", "arg_1": " The Mississippi"}}, "resps": [[["-19.5", "False"]], [["-16.875", "False"]], [["-19.125", "False"]], [["-19.0", "False"]]], "filtered_resps": [["-19.5", "False"], ["-16.875", "False"], ["-19.125", "False"], ["-19.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "cdea19f55f92af329a6640458775445032b7da90607afe1267e6c7f55e41767a", "prompt_hash": "b64adba1c105b108ea0c31136a9597e99a89c467b456ac3e490c2024bd8b08a2", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 268, "doc": {"id": "9-16", "question_stem": "The nimbleness of this animal is a key adaption that allows it to escape attacks from predators:", "choices": {"text": ["the butterfly", "the sloth", "the praying mantis", "the antelope"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "The nimbleness of this animal is a key adaption that allows it to escape attacks from predators:", "arg_1": " the butterfly"}, "gen_args_1": {"arg_0": "The nimbleness of this animal is a key adaption that allows it to escape attacks from predators:", "arg_1": " the sloth"}, "gen_args_2": {"arg_0": "The nimbleness of this animal is a key adaption that allows it to escape attacks from predators:", "arg_1": " the praying mantis"}, "gen_args_3": {"arg_0": "The nimbleness of this animal is a key adaption that allows it to escape attacks from predators:", "arg_1": " the antelope"}}, "resps": [[["-13.9375", "False"]], [["-10.25", "False"]], [["-12.0", "False"]], [["-11.625", "False"]]], "filtered_resps": [["-13.9375", "False"], ["-10.25", "False"], ["-12.0", "False"], ["-11.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "7148ad9e32956ba97175cfab18477e8180bab0b41f83df905c150dc3fdaf13c6", "prompt_hash": "7e19090d61a8b5834d4fdf3316f9cbe9dcb0b6de525904ae35ac27ed151aefa0", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 269, "doc": {"id": "7-986", "question_stem": "A person speaks English as her first language because", "choices": {"text": ["media is mainly in English", "school is in English", "she was genetically predisposed", "she watched her parents speak"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "A person speaks English as her first language because", "arg_1": " media is mainly in English"}, "gen_args_1": {"arg_0": "A person speaks English as her first language because", "arg_1": " school is in English"}, "gen_args_2": {"arg_0": "A person speaks English as her first language because", "arg_1": " she was genetically predisposed"}, "gen_args_3": {"arg_0": "A person speaks English as her first language because", "arg_1": " she watched her parents speak"}}, "resps": [[["-27.125", "False"]], [["-22.125", "False"]], [["-17.125", "False"]], [["-21.125", "False"]]], "filtered_resps": [["-27.125", "False"], ["-22.125", "False"], ["-17.125", "False"], ["-21.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "39599a79f086fed3b8ce7d41317740965eb6527c3a12f681f71801a756254273", "prompt_hash": "b5f89ee340788f45238ed0aab61a8a927494b7313732766a80118cdecc57b852", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 270, "doc": {"id": "7-787", "question_stem": "what are eaten by honey producing insects?", "choices": {"text": ["they consume plants", "they eat cows", "plant reproduction parts", "they eat flowers"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "what are eaten by honey producing insects?", "arg_1": " they consume plants"}, "gen_args_1": {"arg_0": "what are eaten by honey producing insects?", "arg_1": " they eat cows"}, "gen_args_2": {"arg_0": "what are eaten by honey producing insects?", "arg_1": " plant reproduction parts"}, "gen_args_3": {"arg_0": "what are eaten by honey producing insects?", "arg_1": " they eat flowers"}}, "resps": [[["-23.375", "False"]], [["-32.0", "False"]], [["-32.5", "False"]], [["-20.25", "False"]]], "filtered_resps": [["-23.375", "False"], ["-32.0", "False"], ["-32.5", "False"], ["-20.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "70ad481fe75fd0e1c21b7122bffdfc2ce21f7510693c6d2643f5be0e9d08d7ef", "prompt_hash": "714b382f68659619533ebcee0c044c4c0b01e39aacffccb57b6f6c24936a83cb", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 271, "doc": {"id": "9-181", "question_stem": "The Earth's closest heat source is", "choices": {"text": ["our celestial fireball", "solar flares", "gamma rays", "big bang"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "The Earth's closest heat source is", "arg_1": " our celestial fireball"}, "gen_args_1": {"arg_0": "The Earth's closest heat source is", "arg_1": " solar flares"}, "gen_args_2": {"arg_0": "The Earth's closest heat source is", "arg_1": " gamma rays"}, "gen_args_3": {"arg_0": "The Earth's closest heat source is", "arg_1": " big bang"}}, "resps": [[["-28.375", "False"]], [["-16.125", "False"]], [["-13.3125", "False"]], [["-21.5", "False"]]], "filtered_resps": [["-28.375", "False"], ["-16.125", "False"], ["-13.3125", "False"], ["-21.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "669ec3690e0a58d09c4803f3433ac5cf92652c9012b8e2bbf15fce6068a4da38", "prompt_hash": "96f316683719d3e850fd227317b54cc51de4d983bfe254ff51314980c7496b0e", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 272, "doc": {"id": "1240", "question_stem": "How can we see how wind effects sand?", "choices": {"text": ["sand is always moving", "sandstorms create ripples in sand", "sand is easy to move through", "beaches often have waves in the sand"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "How can we see how wind effects sand?", "arg_1": " sand is always moving"}, "gen_args_1": {"arg_0": "How can we see how wind effects sand?", "arg_1": " sandstorms create ripples in sand"}, "gen_args_2": {"arg_0": "How can we see how wind effects sand?", "arg_1": " sand is easy to move through"}, "gen_args_3": {"arg_0": "How can we see how wind effects sand?", "arg_1": " beaches often have waves in the sand"}}, "resps": [[["-19.75", "False"]], [["-34.25", "False"]], [["-29.0", "False"]], [["-45.0", "False"]]], "filtered_resps": [["-19.75", "False"], ["-34.25", "False"], ["-29.0", "False"], ["-45.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a2eb32855f793899cf11b116050ae4b2ae5428bfdbc9329726ece2c4af45c3e9", "prompt_hash": "152dc2fe084830dfd7ac9e13688773fae34fa0c3b7318ec02b1375133a7ac8f8", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 273, "doc": {"id": "474", "question_stem": "Fruit comes from what source", "choices": {"text": ["an organism that releases carbon dioxide", "an organism that absorbs water through it's branches", "an organism that absorbs oxygen", "an organism that absorbs water through it's roots"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Fruit comes from what source", "arg_1": " an organism that releases carbon dioxide"}, "gen_args_1": {"arg_0": "Fruit comes from what source", "arg_1": " an organism that absorbs water through it's branches"}, "gen_args_2": {"arg_0": "Fruit comes from what source", "arg_1": " an organism that absorbs oxygen"}, "gen_args_3": {"arg_0": "Fruit comes from what source", "arg_1": " an organism that absorbs water through it's roots"}}, "resps": [[["-35.75", "False"]], [["-45.0", "False"]], [["-31.625", "False"]], [["-36.5", "False"]]], "filtered_resps": [["-35.75", "False"], ["-45.0", "False"], ["-31.625", "False"], ["-36.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "42095197877ab46398264bfeb9ffe0f0bd4af17b94e80f280e6ec4c62c6f8102", "prompt_hash": "154c5ae236e97bf63e2d56908b2c6a62ed57879dce34ef001ef5d256f2d75ad4", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 274, "doc": {"id": "1274", "question_stem": "In which location would a groundhog hide from a wolf?", "choices": {"text": ["beside a tree", "in the grass", "on a stump", "under the ground"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "In which location would a groundhog hide from a wolf?", "arg_1": " beside a tree"}, "gen_args_1": {"arg_0": "In which location would a groundhog hide from a wolf?", "arg_1": " in the grass"}, "gen_args_2": {"arg_0": "In which location would a groundhog hide from a wolf?", "arg_1": " on a stump"}, "gen_args_3": {"arg_0": "In which location would a groundhog hide from a wolf?", "arg_1": " under the ground"}}, "resps": [[["-24.5", "False"]], [["-21.0", "False"]], [["-21.125", "False"]], [["-18.75", "False"]]], "filtered_resps": [["-24.5", "False"], ["-21.0", "False"], ["-21.125", "False"], ["-18.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f28505c9b3ae63a4dc179ec2e48f5022330fa9f302fac8ecc5c27203f76bdb8a", "prompt_hash": "3ab8c134b0e4eb109813642b8492db6a132b31655a2647f7182ed3bc8d0dd945", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 275, "doc": {"id": "1531", "question_stem": "Cutting down trees in a forest", "choices": {"text": ["leads to more habitats for animals", "decreases the chance of erosion", "increases the number of trees in the forest", "leads to less habitats for animals"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Cutting down trees in a forest", "arg_1": " leads to more habitats for animals"}, "gen_args_1": {"arg_0": "Cutting down trees in a forest", "arg_1": " decreases the chance of erosion"}, "gen_args_2": {"arg_0": "Cutting down trees in a forest", "arg_1": " increases the number of trees in the forest"}, "gen_args_3": {"arg_0": "Cutting down trees in a forest", "arg_1": " leads to less habitats for animals"}}, "resps": [[["-27.0", "False"]], [["-28.375", "False"]], [["-22.375", "False"]], [["-27.625", "False"]]], "filtered_resps": [["-27.0", "False"], ["-28.375", "False"], ["-22.375", "False"], ["-27.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "81668cbd8a6b8ac2e17e5974bbc4989795dd20202c6aebd44db75f143900105e", "prompt_hash": "1f14cba3cc21c52718d4d1896c8c8b6cb816b71cada8629149d4ac18eefb5e36", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 276, "doc": {"id": "8-321", "question_stem": "I chipped away at a toy doll and the surface became really rough, when I rub it against a piece of wood that will create an increase in", "choices": {"text": ["animals", "resistance", "water", "sunshine"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "I chipped away at a toy doll and the surface became really rough, when I rub it against a piece of wood that will create an increase in", "arg_1": " animals"}, "gen_args_1": {"arg_0": "I chipped away at a toy doll and the surface became really rough, when I rub it against a piece of wood that will create an increase in", "arg_1": " resistance"}, "gen_args_2": {"arg_0": "I chipped away at a toy doll and the surface became really rough, when I rub it against a piece of wood that will create an increase in", "arg_1": " water"}, "gen_args_3": {"arg_0": "I chipped away at a toy doll and the surface became really rough, when I rub it against a piece of wood that will create an increase in", "arg_1": " sunshine"}}, "resps": [[["-15.4375", "False"]], [["-4.46875", "False"]], [["-7.96875", "False"]], [["-15.25", "False"]]], "filtered_resps": [["-15.4375", "False"], ["-4.46875", "False"], ["-7.96875", "False"], ["-15.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d5566bf7eaa9995b656d7b4488e625956e068d63f5a0fd68df1049dbe6911b49", "prompt_hash": "548e97cde9904cce27cec5544b7cb0cb9a686105f6d9729d00f09c56b3f14dc0", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 277, "doc": {"id": "1321", "question_stem": "The arctic is white in coloring", "choices": {"text": ["because it's overpopulated with polar bears", "because it's covered in white lilies", "because it's blanketed in crystalline ice water", "because it's gets so little sunlight"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "The arctic is white in coloring", "arg_1": " because it's overpopulated with polar bears"}, "gen_args_1": {"arg_0": "The arctic is white in coloring", "arg_1": " because it's covered in white lilies"}, "gen_args_2": {"arg_0": "The arctic is white in coloring", "arg_1": " because it's blanketed in crystalline ice water"}, "gen_args_3": {"arg_0": "The arctic is white in coloring", "arg_1": " because it's gets so little sunlight"}}, "resps": [[["-28.875", "False"]], [["-34.75", "False"]], [["-41.75", "False"]], [["-32.0", "False"]]], "filtered_resps": [["-28.875", "False"], ["-34.75", "False"], ["-41.75", "False"], ["-32.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "dd90b40eca52b94a49193b59bf0cc3a7e36153150a2dbc218ed899a96f8d6664", "prompt_hash": "0fb3988cb38a798acf1e0aa90dc5c7f18d0738b25a857cf313c92a804dd5dbd8", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 278, "doc": {"id": "9-51", "question_stem": "What would help to ensure that your dog remains free from hypothermia in January in Alaska?", "choices": {"text": ["Lots of meat", "Lots of love", "Vitamin supplements", "An insulated room"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "What would help to ensure that your dog remains free from hypothermia in January in Alaska?", "arg_1": " Lots of meat"}, "gen_args_1": {"arg_0": "What would help to ensure that your dog remains free from hypothermia in January in Alaska?", "arg_1": " Lots of love"}, "gen_args_2": {"arg_0": "What would help to ensure that your dog remains free from hypothermia in January in Alaska?", "arg_1": " Vitamin supplements"}, "gen_args_3": {"arg_0": "What would help to ensure that your dog remains free from hypothermia in January in Alaska?", "arg_1": " An insulated room"}}, "resps": [[["-29.25", "False"]], [["-27.25", "False"]], [["-21.25", "False"]], [["-36.5", "False"]]], "filtered_resps": [["-29.25", "False"], ["-27.25", "False"], ["-21.25", "False"], ["-36.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "fcb5956ec5f9c1c01b9423bacaf5331c85dfa4122c437b785572eb65c9ee38b5", "prompt_hash": "8f033f7ab62a336aa0357dc5ccb726fdd84bc903ece7a78484a849a8b996f3e4", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 279, "doc": {"id": "7-685", "question_stem": "The majority of a lizard's diet consists of", "choices": {"text": ["fleas", "crawlies", "gummy worms", "berries"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "The majority of a lizard's diet consists of", "arg_1": " fleas"}, "gen_args_1": {"arg_0": "The majority of a lizard's diet consists of", "arg_1": " crawlies"}, "gen_args_2": {"arg_0": "The majority of a lizard's diet consists of", "arg_1": " gummy worms"}, "gen_args_3": {"arg_0": "The majority of a lizard's diet consists of", "arg_1": " berries"}}, "resps": [[["-16.375", "False"]], [["-28.5", "False"]], [["-22.25", "False"]], [["-16.75", "False"]]], "filtered_resps": [["-16.375", "False"], ["-28.5", "False"], ["-22.25", "False"], ["-16.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "150adeda43f1c517003e3209429093ac849e31b7fcf06dc72a817e3b5ed34809", "prompt_hash": "bb4f3977800c3c27903b5ff7ddf3d12103ae9d3fd79ecc2b82816a9548c931f4", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 280, "doc": {"id": "7-59", "question_stem": "What food production happens in a leaf?", "choices": {"text": ["nutrient making process", "the breathing", "the respiration", "the digestion"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "What food production happens in a leaf?", "arg_1": " nutrient making process"}, "gen_args_1": {"arg_0": "What food production happens in a leaf?", "arg_1": " the breathing"}, "gen_args_2": {"arg_0": "What food production happens in a leaf?", "arg_1": " the respiration"}, "gen_args_3": {"arg_0": "What food production happens in a leaf?", "arg_1": " the digestion"}}, "resps": [[["-27.25", "False"]], [["-27.0", "False"]], [["-22.25", "False"]], [["-21.75", "False"]]], "filtered_resps": [["-27.25", "False"], ["-27.0", "False"], ["-22.25", "False"], ["-21.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "6dee3885fd360da2965a677b25ff4f0865ef5247f2226d2c64ac6d330ea84fa9", "prompt_hash": "0d60f4b3d8011c4cf29cb30e4e28d19d90fd44975adc91eb6adcb1429fe73d08", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 281, "doc": {"id": "7-270", "question_stem": "Plants are unable to grow if they have zero access to", "choices": {"text": ["a nice cool breeze", "fresh soil with manure", "a regular source of saltwater", "needs required for creating chlorophyll"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Plants are unable to grow if they have zero access to", "arg_1": " a nice cool breeze"}, "gen_args_1": {"arg_0": "Plants are unable to grow if they have zero access to", "arg_1": " fresh soil with manure"}, "gen_args_2": {"arg_0": "Plants are unable to grow if they have zero access to", "arg_1": " a regular source of saltwater"}, "gen_args_3": {"arg_0": "Plants are unable to grow if they have zero access to", "arg_1": " needs required for creating chlorophyll"}}, "resps": [[["-29.75", "False"]], [["-37.0", "False"]], [["-29.625", "False"]], [["-36.25", "False"]]], "filtered_resps": [["-29.75", "False"], ["-37.0", "False"], ["-29.625", "False"], ["-36.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f9370669a56f581693c311f2cac117be18210fb89dcad885bc3c908b103937b3", "prompt_hash": "1a99fa7a529f63e33cab999c0f42000a7af244b4b2cdf0b6ede035db5c3bcffc", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 282, "doc": {"id": "7-736", "question_stem": "Which characteristic did a person inherit?", "choices": {"text": ["length of hair", "number of friends", "number of nails", "length of shirt"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Which characteristic did a person inherit?", "arg_1": " length of hair"}, "gen_args_1": {"arg_0": "Which characteristic did a person inherit?", "arg_1": " number of friends"}, "gen_args_2": {"arg_0": "Which characteristic did a person inherit?", "arg_1": " number of nails"}, "gen_args_3": {"arg_0": "Which characteristic did a person inherit?", "arg_1": " length of shirt"}}, "resps": [[["-23.75", "False"]], [["-34.5", "False"]], [["-36.25", "False"]], [["-36.0", "False"]]], "filtered_resps": [["-23.75", "False"], ["-34.5", "False"], ["-36.25", "False"], ["-36.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f6c79720d6e993a4abea31111c143f24a7b27072c5e16fbd4248075dd3570624", "prompt_hash": "71eb4fda7fdf83204189fff999214fc1f339c5df752cc06253884afd6c4dcb2e", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 283, "doc": {"id": "8-186", "question_stem": "Selective deforestation has a negative impact on", "choices": {"text": ["rain clouds and ozone layer", "lakes, ponds and shellfish", "greenhouse gases and algae", "living organisms in ecosystem"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Selective deforestation has a negative impact on", "arg_1": " rain clouds and ozone layer"}, "gen_args_1": {"arg_0": "Selective deforestation has a negative impact on", "arg_1": " lakes, ponds and shellfish"}, "gen_args_2": {"arg_0": "Selective deforestation has a negative impact on", "arg_1": " greenhouse gases and algae"}, "gen_args_3": {"arg_0": "Selective deforestation has a negative impact on", "arg_1": " living organisms in ecosystem"}}, "resps": [[["-33.25", "False"]], [["-41.75", "False"]], [["-24.5", "False"]], [["-21.75", "False"]]], "filtered_resps": [["-33.25", "False"], ["-41.75", "False"], ["-24.5", "False"], ["-21.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "bdc0c78bb85c0b8faea7e613ec43b58c8626f4538cb609fec1d9dfb157edeb25", "prompt_hash": "449b3aaf5007371f9c012e154e4fb366760356fe9da34412db0992878902032c", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 284, "doc": {"id": "224", "question_stem": "Where would you find a mine?", "choices": {"text": ["in a tree", "under a mountain", "in the air", "in the water"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Where would you find a mine?", "arg_1": " in a tree"}, "gen_args_1": {"arg_0": "Where would you find a mine?", "arg_1": " under a mountain"}, "gen_args_2": {"arg_0": "Where would you find a mine?", "arg_1": " in the air"}, "gen_args_3": {"arg_0": "Where would you find a mine?", "arg_1": " in the water"}}, "resps": [[["-17.25", "False"]], [["-16.0", "False"]], [["-16.375", "False"]], [["-15.25", "False"]]], "filtered_resps": [["-17.25", "False"], ["-16.0", "False"], ["-16.375", "False"], ["-15.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "19e369dacd6a58698b89c88a5a9aeebd11df7ba8f796cc92d6e85e2c4edccfb2", "prompt_hash": "987e6ab16349eb9e02eab9f5dd18c1aed7ae8a09d1e418a8d69cacab29379d9d", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 285, "doc": {"id": "8-206", "question_stem": "What can cause people to crash their car?", "choices": {"text": ["Seeing a solar eclipse", "Using their turn signals", "Driving the speed limit", "Keeping their eyes on the road"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "What can cause people to crash their car?", "arg_1": " Seeing a solar eclipse"}, "gen_args_1": {"arg_0": "What can cause people to crash their car?", "arg_1": " Using their turn signals"}, "gen_args_2": {"arg_0": "What can cause people to crash their car?", "arg_1": " Driving the speed limit"}, "gen_args_3": {"arg_0": "What can cause people to crash their car?", "arg_1": " Keeping their eyes on the road"}}, "resps": [[["-28.5", "False"]], [["-27.875", "False"]], [["-23.25", "False"]], [["-28.5", "False"]]], "filtered_resps": [["-28.5", "False"], ["-27.875", "False"], ["-23.25", "False"], ["-28.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "0440020a9e698fa340ee01af9d0c5157b764b4850471f8ef911d695075b9d4c3", "prompt_hash": "2642275b1cc7babf67263eca5cf4c60b482c4bc51f3aeaa1fbef8e0a791a1dea", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 286, "doc": {"id": "8-190", "question_stem": "Coral grows in", "choices": {"text": ["frigid waters", "tepid seas", "glacial environments", "jungle forests"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Coral grows in", "arg_1": " frigid waters"}, "gen_args_1": {"arg_0": "Coral grows in", "arg_1": " tepid seas"}, "gen_args_2": {"arg_0": "Coral grows in", "arg_1": " glacial environments"}, "gen_args_3": {"arg_0": "Coral grows in", "arg_1": " jungle forests"}}, "resps": [[["-10.9375", "False"]], [["-20.25", "False"]], [["-14.75", "False"]], [["-19.5", "False"]]], "filtered_resps": [["-10.9375", "False"], ["-20.25", "False"], ["-14.75", "False"], ["-19.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "caad304958fce263ba3f5c510899b1567d6e99de2d38856f141a0c476fe5d247", "prompt_hash": "83d52b4f930aa2e606ec60afaf56581156beda3156df7cb4d969894f63813c69", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 287, "doc": {"id": "7-334", "question_stem": "A Punnett square is used to identify the percent chance of a trait being passed down from a parent to its offspring, so", "choices": {"text": ["certain things may be featured", "certain features may be predicted", "certain traits may be given", "certain features may be guaranteed"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "A Punnett square is used to identify the percent chance of a trait being passed down from a parent to its offspring, so", "arg_1": " certain things may be featured"}, "gen_args_1": {"arg_0": "A Punnett square is used to identify the percent chance of a trait being passed down from a parent to its offspring, so", "arg_1": " certain features may be predicted"}, "gen_args_2": {"arg_0": "A Punnett square is used to identify the percent chance of a trait being passed down from a parent to its offspring, so", "arg_1": " certain traits may be given"}, "gen_args_3": {"arg_0": "A Punnett square is used to identify the percent chance of a trait being passed down from a parent to its offspring, so", "arg_1": " certain features may be guaranteed"}}, "resps": [[["-30.625", "False"]], [["-27.375", "False"]], [["-22.75", "False"]], [["-30.75", "False"]]], "filtered_resps": [["-30.625", "False"], ["-27.375", "False"], ["-22.75", "False"], ["-30.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2634183a2e1ef2c4f4549f0e81bffefc7bc76c36c21228dd2c1f41031ead6dab", "prompt_hash": "f978f56937ecaa22e625252e0e9d3ed5c513cc00a49d00c0551276435357fea5", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 288, "doc": {"id": "9-853", "question_stem": "A deer is eating in a field, and wants more food. Regardless of how hard the deer tries, the deer is unable to produce", "choices": {"text": ["longer antlers", "food for itself", "baby deer", "urine"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "A deer is eating in a field, and wants more food. Regardless of how hard the deer tries, the deer is unable to produce", "arg_1": " longer antlers"}, "gen_args_1": {"arg_0": "A deer is eating in a field, and wants more food. Regardless of how hard the deer tries, the deer is unable to produce", "arg_1": " food for itself"}, "gen_args_2": {"arg_0": "A deer is eating in a field, and wants more food. Regardless of how hard the deer tries, the deer is unable to produce", "arg_1": " baby deer"}, "gen_args_3": {"arg_0": "A deer is eating in a field, and wants more food. Regardless of how hard the deer tries, the deer is unable to produce", "arg_1": " urine"}}, "resps": [[["-17.5", "False"]], [["-6.78125", "False"]], [["-15.25", "False"]], [["-16.75", "False"]]], "filtered_resps": [["-17.5", "False"], ["-6.78125", "False"], ["-15.25", "False"], ["-16.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f9d3a81b4acfbd376aa2ba5b1ae773302b65ce28a7b7bc4ef8df48e9cd12768a", "prompt_hash": "7546afb1dc853cae56ff56d8eff47cf994d42738adc283a4eb5f0ac6eb1dc61f", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 289, "doc": {"id": "8-367", "question_stem": "Building new areas to dispose of refuse may lead to", "choices": {"text": ["community concerns over environmental impact", "better air and soil quality", "higher value on land parcels", "improvement in water supply"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Building new areas to dispose of refuse may lead to", "arg_1": " community concerns over environmental impact"}, "gen_args_1": {"arg_0": "Building new areas to dispose of refuse may lead to", "arg_1": " better air and soil quality"}, "gen_args_2": {"arg_0": "Building new areas to dispose of refuse may lead to", "arg_1": " higher value on land parcels"}, "gen_args_3": {"arg_0": "Building new areas to dispose of refuse may lead to", "arg_1": " improvement in water supply"}}, "resps": [[["-17.0", "False"]], [["-18.25", "False"]], [["-29.875", "False"]], [["-21.5", "False"]]], "filtered_resps": [["-17.0", "False"], ["-18.25", "False"], ["-29.875", "False"], ["-21.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "7cfd38ee8d4ae4f81fbc449d8dfe5ba07c4ab07bb577d168bc8458f19e92d42f", "prompt_hash": "88b95071a1c40b9cbd4820e9db95e5f1ea4e6c756d4a821207c47cf770cf3b10", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 290, "doc": {"id": "1047", "question_stem": "Evaporation", "choices": {"text": ["causes puddles to become dried out mud", "causes fields of crops to grow faster", "causes flowers to bloom abundantly", "fills up irrigation ponds"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Evaporation", "arg_1": " causes puddles to become dried out mud"}, "gen_args_1": {"arg_0": "Evaporation", "arg_1": " causes fields of crops to grow faster"}, "gen_args_2": {"arg_0": "Evaporation", "arg_1": " causes flowers to bloom abundantly"}, "gen_args_3": {"arg_0": "Evaporation", "arg_1": " fills up irrigation ponds"}}, "resps": [[["-47.75", "False"]], [["-42.25", "False"]], [["-33.5", "False"]], [["-33.5", "False"]]], "filtered_resps": [["-47.75", "False"], ["-42.25", "False"], ["-33.5", "False"], ["-33.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a601390e0411bd0f0689c7643b261ccc499759b1f18e925cc6ec6e3cde523f13", "prompt_hash": "5bbc10ae437da3b951d1ab2308e7b6e8f7877adfccfb3b4967765ca8aa9e9c94", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 291, "doc": {"id": "9-454", "question_stem": "A field begins to bloom and blossom and plants need to be pollinated. In order to spread seeds, plants will most rely on", "choices": {"text": ["pythons", "salmon", "robins", "craters"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A field begins to bloom and blossom and plants need to be pollinated. In order to spread seeds, plants will most rely on", "arg_1": " pythons"}, "gen_args_1": {"arg_0": "A field begins to bloom and blossom and plants need to be pollinated. In order to spread seeds, plants will most rely on", "arg_1": " salmon"}, "gen_args_2": {"arg_0": "A field begins to bloom and blossom and plants need to be pollinated. In order to spread seeds, plants will most rely on", "arg_1": " robins"}, "gen_args_3": {"arg_0": "A field begins to bloom and blossom and plants need to be pollinated. In order to spread seeds, plants will most rely on", "arg_1": " craters"}}, "resps": [[["-16.875", "False"]], [["-15.0625", "False"]], [["-14.6875", "False"]], [["-18.75", "False"]]], "filtered_resps": [["-16.875", "False"], ["-15.0625", "False"], ["-14.6875", "False"], ["-18.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "6576efea39bb6897ffaa90428e15b425b524a67085375b3a481114e120f97587", "prompt_hash": "9100aded1f75de1ec31c58389c2744f4180b2c45ee5b4ba6592adce8864633e8", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 292, "doc": {"id": "1572", "question_stem": "Which item urinates?", "choices": {"text": ["airplane", "car", "mammal", "boat"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Which item urinates?", "arg_1": " airplane"}, "gen_args_1": {"arg_0": "Which item urinates?", "arg_1": " car"}, "gen_args_2": {"arg_0": "Which item urinates?", "arg_1": " mammal"}, "gen_args_3": {"arg_0": "Which item urinates?", "arg_1": " boat"}}, "resps": [[["-21.25", "False"]], [["-17.625", "False"]], [["-19.0", "False"]], [["-19.875", "False"]]], "filtered_resps": [["-21.25", "False"], ["-17.625", "False"], ["-19.0", "False"], ["-19.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "983740f8648a789d8e7eb38cecddab8e6745faca0d75e089adb3fd49bf747f7b", "prompt_hash": "3153f63f020b8188097eed7e5ce895e3cbaf370786fde1b6901172510406e9a0", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 293, "doc": {"id": "8-373", "question_stem": "All of the following are examples of evaporation apart from", "choices": {"text": ["Warm breath fogging up a mirror", "Morning dew drying on the grass", "The water level in a glass decreasing", "Sweat drying on skin"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "All of the following are examples of evaporation apart from", "arg_1": " Warm breath fogging up a mirror"}, "gen_args_1": {"arg_0": "All of the following are examples of evaporation apart from", "arg_1": " Morning dew drying on the grass"}, "gen_args_2": {"arg_0": "All of the following are examples of evaporation apart from", "arg_1": " The water level in a glass decreasing"}, "gen_args_3": {"arg_0": "All of the following are examples of evaporation apart from", "arg_1": " Sweat drying on skin"}}, "resps": [[["-42.75", "False"]], [["-38.0", "False"]], [["-30.625", "False"]], [["-31.375", "False"]]], "filtered_resps": [["-42.75", "False"], ["-38.0", "False"], ["-30.625", "False"], ["-31.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "9e342e7f8780bd5cd5b5232acacb5f46630f927439f53808cf8e253dff8d01fd", "prompt_hash": "3fa37b5b9a040e945b4c15073a4789a782e7cd766c7c617bd11399557e0a0370", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 294, "doc": {"id": "9-772", "question_stem": "The lunar cycle also changes", "choices": {"text": ["water", "colors", "the sun", "planets"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "The lunar cycle also changes", "arg_1": " water"}, "gen_args_1": {"arg_0": "The lunar cycle also changes", "arg_1": " colors"}, "gen_args_2": {"arg_0": "The lunar cycle also changes", "arg_1": " the sun"}, "gen_args_3": {"arg_0": "The lunar cycle also changes", "arg_1": " planets"}}, "resps": [[["-8.875", "False"]], [["-8.6875", "False"]], [["-7.8125", "False"]], [["-12.1875", "False"]]], "filtered_resps": [["-8.875", "False"], ["-8.6875", "False"], ["-7.8125", "False"], ["-12.1875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "fa5d0127ca83044409730e8ed2feb3bfa36e7fd1664166cf18c0eb5ae9c06ce6", "prompt_hash": "c4d5e42b7948d5de8f41ff19378555edd378beb708a598cc3f38faad1ccd5121", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 295, "doc": {"id": "1852", "question_stem": "An organism that makes food for itself", "choices": {"text": ["is nutritionally self sustaining", "will die faster than other organisms", "will need help sustaining strength", "is reliant on other organisms for assistance"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "An organism that makes food for itself", "arg_1": " is nutritionally self sustaining"}, "gen_args_1": {"arg_0": "An organism that makes food for itself", "arg_1": " will die faster than other organisms"}, "gen_args_2": {"arg_0": "An organism that makes food for itself", "arg_1": " will need help sustaining strength"}, "gen_args_3": {"arg_0": "An organism that makes food for itself", "arg_1": " is reliant on other organisms for assistance"}}, "resps": [[["-32.25", "False"]], [["-32.75", "False"]], [["-43.0", "False"]], [["-28.5", "False"]]], "filtered_resps": [["-32.25", "False"], ["-32.75", "False"], ["-43.0", "False"], ["-28.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "530a692aa474bdcd4482d51debbdfd01f16ce6f9ac5e9ba21cee23a78ac2ccfd", "prompt_hash": "1577219561c67b42c8be47b3084cc447e320e89431cb2a97218e6cac9277e6d8", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 296, "doc": {"id": "9-1090", "question_stem": "What does the respiratory system transfer to the circulatory system?", "choices": {"text": ["food", "water", "nutrients", "O"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "What does the respiratory system transfer to the circulatory system?", "arg_1": " food"}, "gen_args_1": {"arg_0": "What does the respiratory system transfer to the circulatory system?", "arg_1": " water"}, "gen_args_2": {"arg_0": "What does the respiratory system transfer to the circulatory system?", "arg_1": " nutrients"}, "gen_args_3": {"arg_0": "What does the respiratory system transfer to the circulatory system?", "arg_1": " O"}}, "resps": [[["-19.375", "False"]], [["-15.75", "False"]], [["-16.375", "False"]], [["-10.6875", "False"]]], "filtered_resps": [["-19.375", "False"], ["-15.75", "False"], ["-16.375", "False"], ["-10.6875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "ca430076e2676e8a6e73f0c58144507477815abb3256c836bd450b1e02724aae", "prompt_hash": "d485a9064a240a4d6eeb398a86151aaee8e65d9c4dc46952cf518146a79545b3", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 297, "doc": {"id": "7-769", "question_stem": "In a closed circuit, electricity will", "choices": {"text": ["burn out", "charge itself", "loop endlessly", "resist flow"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "In a closed circuit, electricity will", "arg_1": " burn out"}, "gen_args_1": {"arg_0": "In a closed circuit, electricity will", "arg_1": " charge itself"}, "gen_args_2": {"arg_0": "In a closed circuit, electricity will", "arg_1": " loop endlessly"}, "gen_args_3": {"arg_0": "In a closed circuit, electricity will", "arg_1": " resist flow"}}, "resps": [[["-17.875", "False"]], [["-19.75", "False"]], [["-16.25", "False"]], [["-13.625", "False"]]], "filtered_resps": [["-17.875", "False"], ["-19.75", "False"], ["-16.25", "False"], ["-13.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "ba8dd95e5ca725ab316f1d2259811c3dbfee46e6f57cf9c05bcdbfb75365c054", "prompt_hash": "072c660a01d325bc4a994150aeced743191b4ff492d683a1e7db872c66d2eca0", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 298, "doc": {"id": "9-478", "question_stem": "A Punnett square can be used to calculate the chance of a trait being passed to someone's", "choices": {"text": ["mother", "grandfather", "daughter", "father"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A Punnett square can be used to calculate the chance of a trait being passed to someone's", "arg_1": " mother"}, "gen_args_1": {"arg_0": "A Punnett square can be used to calculate the chance of a trait being passed to someone's", "arg_1": " grandfather"}, "gen_args_2": {"arg_0": "A Punnett square can be used to calculate the chance of a trait being passed to someone's", "arg_1": " daughter"}, "gen_args_3": {"arg_0": "A Punnett square can be used to calculate the chance of a trait being passed to someone's", "arg_1": " father"}}, "resps": [[["-18.875", "False"]], [["-21.375", "False"]], [["-10.4375", "False"]], [["-17.375", "False"]]], "filtered_resps": [["-18.875", "False"], ["-21.375", "False"], ["-10.4375", "False"], ["-17.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "5b274f378f8bc4ab798311d4d0f4c590e0128375cc1c7047a346d9a6833b9591", "prompt_hash": "965120b9d7b83f2faca07e23cacb637379595b3ce8e6c79dbf8e2fd9e9815d14", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 299, "doc": {"id": "448", "question_stem": "They looked where the log decayed to garden as it would leave the earth", "choices": {"text": ["richer", "dryer", "sandy", "harder"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "They looked where the log decayed to garden as it would leave the earth", "arg_1": " richer"}, "gen_args_1": {"arg_0": "They looked where the log decayed to garden as it would leave the earth", "arg_1": " dryer"}, "gen_args_2": {"arg_0": "They looked where the log decayed to garden as it would leave the earth", "arg_1": " sandy"}, "gen_args_3": {"arg_0": "They looked where the log decayed to garden as it would leave the earth", "arg_1": " harder"}}, "resps": [[["-10.3125", "False"]], [["-12.6875", "False"]], [["-10.0", "False"]], [["-12.125", "False"]]], "filtered_resps": [["-10.3125", "False"], ["-12.6875", "False"], ["-10.0", "False"], ["-12.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4c3b17dfa2428fe1685b3e8bb5afe3d71d546281f5083a531fb92c0091849f45", "prompt_hash": "46cdfda048782265840ab32e66f451ad0d938f9472fb0a248603d7a927bc7cac", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 300, "doc": {"id": "7-417", "question_stem": "what kind of temperature causes fur shedding?", "choices": {"text": ["in freezing cold", "a high temperature", "in any temperature", "a low temperature"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "what kind of temperature causes fur shedding?", "arg_1": " in freezing cold"}, "gen_args_1": {"arg_0": "what kind of temperature causes fur shedding?", "arg_1": " a high temperature"}, "gen_args_2": {"arg_0": "what kind of temperature causes fur shedding?", "arg_1": " in any temperature"}, "gen_args_3": {"arg_0": "what kind of temperature causes fur shedding?", "arg_1": " a low temperature"}}, "resps": [[["-24.375", "False"]], [["-16.875", "False"]], [["-25.75", "False"]], [["-15.75", "False"]]], "filtered_resps": [["-24.375", "False"], ["-16.875", "False"], ["-25.75", "False"], ["-15.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e7f9cf34551138cc1a652a830cd4e26638db620d0e5f3a937cfae01f315e431f", "prompt_hash": "36243e86f816069d64e6c89da8997b6ad4f6f60d236278e06f17da17dc7a7abb", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 301, "doc": {"id": "7-108", "question_stem": "exposure to fire could result in", "choices": {"text": ["wet items", "cold items", "none of these", "combusted items"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "exposure to fire could result in", "arg_1": " wet items"}, "gen_args_1": {"arg_0": "exposure to fire could result in", "arg_1": " cold items"}, "gen_args_2": {"arg_0": "exposure to fire could result in", "arg_1": " none of these"}, "gen_args_3": {"arg_0": "exposure to fire could result in", "arg_1": " combusted items"}}, "resps": [[["-26.875", "False"]], [["-24.625", "False"]], [["-17.25", "False"]], [["-20.875", "False"]]], "filtered_resps": [["-26.875", "False"], ["-24.625", "False"], ["-17.25", "False"], ["-20.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "263a42f935aafc6e225a333d7f5801c78ba3c69aa45b9caf500c5a8ad37e937b", "prompt_hash": "321cd60604205c3b0576800966cf6e106a278d64a6c0cbc0f7afde1f91ae4d2e", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 302, "doc": {"id": "1506", "question_stem": "What is an example of clear weather meaning sunny weather?", "choices": {"text": ["more stars are visible on clear nights", "cloud cover protects from sunburn", "clear days will be warmer", "fewer clouds allow for more sunlight"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "What is an example of clear weather meaning sunny weather?", "arg_1": " more stars are visible on clear nights"}, "gen_args_1": {"arg_0": "What is an example of clear weather meaning sunny weather?", "arg_1": " cloud cover protects from sunburn"}, "gen_args_2": {"arg_0": "What is an example of clear weather meaning sunny weather?", "arg_1": " clear days will be warmer"}, "gen_args_3": {"arg_0": "What is an example of clear weather meaning sunny weather?", "arg_1": " fewer clouds allow for more sunlight"}}, "resps": [[["-39.75", "False"]], [["-34.5", "False"]], [["-36.25", "False"]], [["-35.75", "False"]]], "filtered_resps": [["-39.75", "False"], ["-34.5", "False"], ["-36.25", "False"], ["-35.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "b8e7828562b4da7a82c42a285df4aa3bb537a71952e4cfe1170683db1cb84488", "prompt_hash": "3d1b4e6bf4a0d606e21d867cba62c8d7f5a6723b0a69c3d895de85cf38f356e3", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 303, "doc": {"id": "1712", "question_stem": "The special tissues in plants that transport minerals throughout the plant are similar to", "choices": {"text": ["a wick", "a funnel", "a knife", "a whisk"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "The special tissues in plants that transport minerals throughout the plant are similar to", "arg_1": " a wick"}, "gen_args_1": {"arg_0": "The special tissues in plants that transport minerals throughout the plant are similar to", "arg_1": " a funnel"}, "gen_args_2": {"arg_0": "The special tissues in plants that transport minerals throughout the plant are similar to", "arg_1": " a knife"}, "gen_args_3": {"arg_0": "The special tissues in plants that transport minerals throughout the plant are similar to", "arg_1": " a whisk"}}, "resps": [[["-20.0", "False"]], [["-15.875", "False"]], [["-20.75", "False"]], [["-18.0", "False"]]], "filtered_resps": [["-20.0", "False"], ["-15.875", "False"], ["-20.75", "False"], ["-18.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e1e8f07cc1ea9c1dd21989ca5b84d2f4689ad2d764518cfbfdd6c85aa86863a6", "prompt_hash": "52127b77cf79594c35ab3f33a66fb80b199f41ba6b4c4caece64735bd64a3178", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 304, "doc": {"id": "8-312", "question_stem": "a compass is a kind of tool for determining direction by pointing", "choices": {"text": ["to western Canada shoreline", "to the lower pole", "to the upper pole", "directly to the equator"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "a compass is a kind of tool for determining direction by pointing", "arg_1": " to western Canada shoreline"}, "gen_args_1": {"arg_0": "a compass is a kind of tool for determining direction by pointing", "arg_1": " to the lower pole"}, "gen_args_2": {"arg_0": "a compass is a kind of tool for determining direction by pointing", "arg_1": " to the upper pole"}, "gen_args_3": {"arg_0": "a compass is a kind of tool for determining direction by pointing", "arg_1": " directly to the equator"}}, "resps": [[["-47.75", "False"]], [["-22.75", "False"]], [["-21.875", "False"]], [["-23.625", "False"]]], "filtered_resps": [["-47.75", "False"], ["-22.75", "False"], ["-21.875", "False"], ["-23.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "b8da004ac924f37fda009ab59a9122d8bc7514a2b720f976b5a4d70df085c851", "prompt_hash": "44c0a78139793b0b507585087499afb2635b44d6b4887da2720721fb98e443a6", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 305, "doc": {"id": "9-776", "question_stem": "Which of these saws will last longer?", "choices": {"text": ["iron saw", "aluminium saw", "plastic saw", "wooden saw"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Which of these saws will last longer?", "arg_1": " iron saw"}, "gen_args_1": {"arg_0": "Which of these saws will last longer?", "arg_1": " aluminium saw"}, "gen_args_2": {"arg_0": "Which of these saws will last longer?", "arg_1": " plastic saw"}, "gen_args_3": {"arg_0": "Which of these saws will last longer?", "arg_1": " wooden saw"}}, "resps": [[["-21.125", "False"]], [["-25.75", "False"]], [["-22.25", "False"]], [["-24.25", "False"]]], "filtered_resps": [["-21.125", "False"], ["-25.75", "False"], ["-22.25", "False"], ["-24.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "71b928c6061c5679113acb1455f41ec9ad55c56ba2843f67584a6ae009bb917d", "prompt_hash": "8361c5da6b10277aa0439cfd026cb509275803938f211f4fe365a5bcdd23c782", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 306, "doc": {"id": "8-279", "question_stem": "although there are many stars visible in the night sky, which is most visible in the day?", "choices": {"text": ["the single moon close to us", "the orion star cluster", "the sun that shines all day", "all of these"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "although there are many stars visible in the night sky, which is most visible in the day?", "arg_1": " the single moon close to us"}, "gen_args_1": {"arg_0": "although there are many stars visible in the night sky, which is most visible in the day?", "arg_1": " the orion star cluster"}, "gen_args_2": {"arg_0": "although there are many stars visible in the night sky, which is most visible in the day?", "arg_1": " the sun that shines all day"}, "gen_args_3": {"arg_0": "although there are many stars visible in the night sky, which is most visible in the day?", "arg_1": " all of these"}}, "resps": [[["-37.5", "False"]], [["-19.875", "False"]], [["-23.125", "False"]], [["-13.125", "False"]]], "filtered_resps": [["-37.5", "False"], ["-19.875", "False"], ["-23.125", "False"], ["-13.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f2e7c33b31df678aeff85a449d3aecb4151d6158d3cfbdc95f2af32b65692e97", "prompt_hash": "30f91f07ea6a841db05b8b56f2047a7de834d2401b8e1418e1ece94f3cf6e813", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 307, "doc": {"id": "9-621", "question_stem": "The moon is known for having what feature?", "choices": {"text": ["frozen streams of water", "large bowl shaped cavities", "caves formed by solar winds", "groups of large trees"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "The moon is known for having what feature?", "arg_1": " frozen streams of water"}, "gen_args_1": {"arg_0": "The moon is known for having what feature?", "arg_1": " large bowl shaped cavities"}, "gen_args_2": {"arg_0": "The moon is known for having what feature?", "arg_1": " caves formed by solar winds"}, "gen_args_3": {"arg_0": "The moon is known for having what feature?", "arg_1": " groups of large trees"}}, "resps": [[["-29.625", "False"]], [["-35.25", "False"]], [["-38.0", "False"]], [["-36.5", "False"]]], "filtered_resps": [["-29.625", "False"], ["-35.25", "False"], ["-38.0", "False"], ["-36.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "9c11baf412fe941b59277ae15365c57d249e2fc96fcc2bab5575bfe06eb1fc2d", "prompt_hash": "31337423b77a1f0aab3de4cb488fdb2cd4e74b08505f0783f0323b94cfa205d5", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 308, "doc": {"id": "1823", "question_stem": "Cellular respiration's trash is", "choices": {"text": ["a bug's treasure", "a cow's treasure", "a plant's treasure", "everyone's trash"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Cellular respiration's trash is", "arg_1": " a bug's treasure"}, "gen_args_1": {"arg_0": "Cellular respiration's trash is", "arg_1": " a cow's treasure"}, "gen_args_2": {"arg_0": "Cellular respiration's trash is", "arg_1": " a plant's treasure"}, "gen_args_3": {"arg_0": "Cellular respiration's trash is", "arg_1": " everyone's trash"}}, "resps": [[["-24.125", "False"]], [["-25.75", "False"]], [["-18.125", "False"]], [["-19.625", "False"]]], "filtered_resps": [["-24.125", "False"], ["-25.75", "False"], ["-18.125", "False"], ["-19.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "7fd13944d6e509bb2a51dcaab4e74ea82b1844240693a4b18fc78bcff974f341", "prompt_hash": "463b46188f8e13824ee19226bcb3c6582f70f4d7a0836b44a328f4d53bdeba5f", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 309, "doc": {"id": "9-735", "question_stem": "Which of the following human activities can lead to a change in the local ecosystem?", "choices": {"text": ["swimming in a lake", "building a new subdivision", "dancing in a field", "going for a hike"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Which of the following human activities can lead to a change in the local ecosystem?", "arg_1": " swimming in a lake"}, "gen_args_1": {"arg_0": "Which of the following human activities can lead to a change in the local ecosystem?", "arg_1": " building a new subdivision"}, "gen_args_2": {"arg_0": "Which of the following human activities can lead to a change in the local ecosystem?", "arg_1": " dancing in a field"}, "gen_args_3": {"arg_0": "Which of the following human activities can lead to a change in the local ecosystem?", "arg_1": " going for a hike"}}, "resps": [[["-31.125", "False"]], [["-27.875", "False"]], [["-40.0", "False"]], [["-28.375", "False"]]], "filtered_resps": [["-31.125", "False"], ["-27.875", "False"], ["-40.0", "False"], ["-28.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d4e3779270e3adb41e8cc45c1ec12b0e4a03ab916a5e4bf783a095b86518caa4", "prompt_hash": "6da94cd654293b1d940ab5d37acc8be713510787707d0934483e7e001c5ed178", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 310, "doc": {"id": "7-1170", "question_stem": "A bird that takes off flying is", "choices": {"text": ["using heat to produce motion", "using calories to produce motion", "using wings to produce heat", "using calories to produce energy"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "A bird that takes off flying is", "arg_1": " using heat to produce motion"}, "gen_args_1": {"arg_0": "A bird that takes off flying is", "arg_1": " using calories to produce motion"}, "gen_args_2": {"arg_0": "A bird that takes off flying is", "arg_1": " using wings to produce heat"}, "gen_args_3": {"arg_0": "A bird that takes off flying is", "arg_1": " using calories to produce energy"}}, "resps": [[["-28.0", "False"]], [["-35.5", "False"]], [["-30.0", "False"]], [["-30.75", "False"]]], "filtered_resps": [["-28.0", "False"], ["-35.5", "False"], ["-30.0", "False"], ["-30.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "0e2e888eaa40104a20b97ab96aeeb8084fd8db8f8ebdbc76e1ed8cbaef5fd52a", "prompt_hash": "3f4e105853bf5b5680e2c6495ef605dddd5c75758ddab23aa4fd3b957f111652", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 311, "doc": {"id": "1500", "question_stem": "The leading cause of soil and rock erosion is", "choices": {"text": ["H2O", "CO2", "NaCl", "Fe"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "The leading cause of soil and rock erosion is", "arg_1": " H2O"}, "gen_args_1": {"arg_0": "The leading cause of soil and rock erosion is", "arg_1": " CO2"}, "gen_args_2": {"arg_0": "The leading cause of soil and rock erosion is", "arg_1": " NaCl"}, "gen_args_3": {"arg_0": "The leading cause of soil and rock erosion is", "arg_1": " Fe"}}, "resps": [[["-16.0", "False"]], [["-14.8125", "False"]], [["-20.625", "False"]], [["-20.5", "False"]]], "filtered_resps": [["-16.0", "False"], ["-14.8125", "False"], ["-20.625", "False"], ["-20.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "dd064f842f1f93863f2aae994b8584b8cf1b86ef55ae705e3064324133c6f1a0", "prompt_hash": "74d3ea477d4526686cb80dbc7dada342644f002cf3d73865551598157fe059f7", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 312, "doc": {"id": "342", "question_stem": "Which of these foods might have a negative impact on humans?", "choices": {"text": ["Organic corn", "Conventional corn", "Organic potato", "Organic Apples"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Which of these foods might have a negative impact on humans?", "arg_1": " Organic corn"}, "gen_args_1": {"arg_0": "Which of these foods might have a negative impact on humans?", "arg_1": " Conventional corn"}, "gen_args_2": {"arg_0": "Which of these foods might have a negative impact on humans?", "arg_1": " Organic potato"}, "gen_args_3": {"arg_0": "Which of these foods might have a negative impact on humans?", "arg_1": " Organic Apples"}}, "resps": [[["-23.75", "False"]], [["-27.75", "False"]], [["-28.875", "False"]], [["-24.875", "False"]]], "filtered_resps": [["-23.75", "False"], ["-27.75", "False"], ["-28.875", "False"], ["-24.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "cc2f2bc75df9aba08b5a279d6ff9344ca099fe8147941eb3101537696d9f9046", "prompt_hash": "de83b766f11c36358eea0901b1139400c3a575c18220a3dd800096cb299a8eb6", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 313, "doc": {"id": "7-356", "question_stem": "What kind of implement is a compass?", "choices": {"text": ["to test heat", "for wind speed", "it measures distance", "it shows direction"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "What kind of implement is a compass?", "arg_1": " to test heat"}, "gen_args_1": {"arg_0": "What kind of implement is a compass?", "arg_1": " for wind speed"}, "gen_args_2": {"arg_0": "What kind of implement is a compass?", "arg_1": " it measures distance"}, "gen_args_3": {"arg_0": "What kind of implement is a compass?", "arg_1": " it shows direction"}}, "resps": [[["-33.0", "False"]], [["-28.625", "False"]], [["-23.0", "False"]], [["-19.25", "False"]]], "filtered_resps": [["-33.0", "False"], ["-28.625", "False"], ["-23.0", "False"], ["-19.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "64352662bacbf180a3f3e58406c7676cfdd0598c07ac3bdbd7203fcbe6adfbcd", "prompt_hash": "a0711d833840f267997e03fb8878c033fabe2dfa5c734bb38a871743cef71dc4", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 314, "doc": {"id": "78", "question_stem": "Nectar is taken to", "choices": {"text": ["flowers", "a hive", "a stream", "a nest"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Nectar is taken to", "arg_1": " flowers"}, "gen_args_1": {"arg_0": "Nectar is taken to", "arg_1": " a hive"}, "gen_args_2": {"arg_0": "Nectar is taken to", "arg_1": " a stream"}, "gen_args_3": {"arg_0": "Nectar is taken to", "arg_1": " a nest"}}, "resps": [[["-10.5625", "False"]], [["-9.125", "False"]], [["-11.75", "False"]], [["-10.6875", "False"]]], "filtered_resps": [["-10.5625", "False"], ["-9.125", "False"], ["-11.75", "False"], ["-10.6875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2bc3dafc6975e729dc31ca97c013a288e26671aa59d7f71136cd892a98128d24", "prompt_hash": "7e805bfa768d0de0e10da794d07d6bc3cfcddc466a8171b8d1e5aa472d15a7b1", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 315, "doc": {"id": "9-520", "question_stem": "what is the closest source of plasma to our planet?", "choices": {"text": ["all of these", "the cloud in the sky", "the nearest star sulfur burning heavenly body", "the bare moon surface"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "what is the closest source of plasma to our planet?", "arg_1": " all of these"}, "gen_args_1": {"arg_0": "what is the closest source of plasma to our planet?", "arg_1": " the cloud in the sky"}, "gen_args_2": {"arg_0": "what is the closest source of plasma to our planet?", "arg_1": " the nearest star sulfur burning heavenly body"}, "gen_args_3": {"arg_0": "what is the closest source of plasma to our planet?", "arg_1": " the bare moon surface"}}, "resps": [[["-16.0", "False"]], [["-28.25", "False"]], [["-63.75", "False"]], [["-38.5", "False"]]], "filtered_resps": [["-16.0", "False"], ["-28.25", "False"], ["-63.75", "False"], ["-38.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e1c320d7f47b3d47edfa99243368007097b2d69093f2ebf501ff560c10469c3c", "prompt_hash": "da438d872adf2a8de6e2b7862ab78e819ed2e1ea6a31e0776271a4689349d588", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 316, "doc": {"id": "7-653", "question_stem": "A sailor needs to navigate to the shore, and does this by", "choices": {"text": ["closing the sails quickly", "setting out to sea", "making an adjustment to the rudder", "taking the afternoon off"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A sailor needs to navigate to the shore, and does this by", "arg_1": " closing the sails quickly"}, "gen_args_1": {"arg_0": "A sailor needs to navigate to the shore, and does this by", "arg_1": " setting out to sea"}, "gen_args_2": {"arg_0": "A sailor needs to navigate to the shore, and does this by", "arg_1": " making an adjustment to the rudder"}, "gen_args_3": {"arg_0": "A sailor needs to navigate to the shore, and does this by", "arg_1": " taking the afternoon off"}}, "resps": [[["-20.875", "False"]], [["-10.125", "False"]], [["-16.5", "False"]], [["-21.0", "False"]]], "filtered_resps": [["-20.875", "False"], ["-10.125", "False"], ["-16.5", "False"], ["-21.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2ab1f2ec7ff58f2e0a26344890e7e2f15e10e98f3fd00c7af25cfe470d94242b", "prompt_hash": "707c466d7066b1589bf15e2f23cbd6715b8deffed3390220cdce4c3bb0872d27", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 317, "doc": {"id": "1112", "question_stem": "A bat starts its life similarly to a", "choices": {"text": ["chicken", "pig", "butterfly", "duck"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "A bat starts its life similarly to a", "arg_1": " chicken"}, "gen_args_1": {"arg_0": "A bat starts its life similarly to a", "arg_1": " pig"}, "gen_args_2": {"arg_0": "A bat starts its life similarly to a", "arg_1": " butterfly"}, "gen_args_3": {"arg_0": "A bat starts its life similarly to a", "arg_1": " duck"}}, "resps": [[["-5.375", "False"]], [["-7.375", "False"]], [["-5.4375", "False"]], [["-7.125", "False"]]], "filtered_resps": [["-5.375", "False"], ["-7.375", "False"], ["-5.4375", "False"], ["-7.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "32cbbba72dab8064501d51b7f85736801ca4a3608cd4b1affeb03a4b8dc62f26", "prompt_hash": "6ea5f2525a41fe906cdf3aedf1c999f895e804270a8d0cff187a716d61426f4e", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 318, "doc": {"id": "9-152", "question_stem": "In order to catch a rabbit, a predator must be", "choices": {"text": ["big", "quick", "slow", "small"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "In order to catch a rabbit, a predator must be", "arg_1": " big"}, "gen_args_1": {"arg_0": "In order to catch a rabbit, a predator must be", "arg_1": " quick"}, "gen_args_2": {"arg_0": "In order to catch a rabbit, a predator must be", "arg_1": " slow"}, "gen_args_3": {"arg_0": "In order to catch a rabbit, a predator must be", "arg_1": " small"}}, "resps": [[["-10.75", "False"]], [["-6.4375", "False"]], [["-8.0625", "False"]], [["-8.6875", "False"]]], "filtered_resps": [["-10.75", "False"], ["-6.4375", "False"], ["-8.0625", "False"], ["-8.6875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "c2f3e2a84e7ff732e343032cd9d2fd9ee04d6f905f33d73fe25b14d3d43912ee", "prompt_hash": "d56f24179bc20beb4e120ecea1d032f39ecb9b2c0776d762882ef349fe255644", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 319, "doc": {"id": "9-552", "question_stem": "If a bird is a carnivore, then it is likely a(n)", "choices": {"text": ["prey", "predator", "herbivore", "canary"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "If a bird is a carnivore, then it is likely a(n)", "arg_1": " prey"}, "gen_args_1": {"arg_0": "If a bird is a carnivore, then it is likely a(n)", "arg_1": " predator"}, "gen_args_2": {"arg_0": "If a bird is a carnivore, then it is likely a(n)", "arg_1": " herbivore"}, "gen_args_3": {"arg_0": "If a bird is a carnivore, then it is likely a(n)", "arg_1": " canary"}}, "resps": [[["-7.09375", "False"]], [["-5.90625", "False"]], [["-10.9375", "False"]], [["-11.6875", "False"]]], "filtered_resps": [["-7.09375", "False"], ["-5.90625", "False"], ["-10.9375", "False"], ["-11.6875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "9fdba08d0e86b4f40b11598a8bb5ad3839162bfb34163d0d820f4d22293f290f", "prompt_hash": "ef2391371417bd5f798d5707c5c9d24041f6c8e59ec9e8d2a58b4ee20daf004d", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 320, "doc": {"id": "7-262", "question_stem": "A warm-weather organism can be found in", "choices": {"text": ["the Sahara", "the mountains", "the ocean", "the sewers"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "A warm-weather organism can be found in", "arg_1": " the Sahara"}, "gen_args_1": {"arg_0": "A warm-weather organism can be found in", "arg_1": " the mountains"}, "gen_args_2": {"arg_0": "A warm-weather organism can be found in", "arg_1": " the ocean"}, "gen_args_3": {"arg_0": "A warm-weather organism can be found in", "arg_1": " the sewers"}}, "resps": [[["-6.0", "False"]], [["-7.4375", "False"]], [["-4.21875", "False"]], [["-12.5625", "False"]]], "filtered_resps": [["-6.0", "False"], ["-7.4375", "False"], ["-4.21875", "False"], ["-12.5625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a9dfe9a9a0638aca5a43510e99731dd97c34e4e88706a6e0f5639cea3db5f471", "prompt_hash": "6d34554eb4d29f8f716d642267aa519fed2ab3af7c12b357a8783c8a027b06c9", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 321, "doc": {"id": "7-683", "question_stem": "When approaching an elephant from a great distance,", "choices": {"text": ["it stays large", "it grows larger", "it gets bigger", "it looks bigger"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "When approaching an elephant from a great distance,", "arg_1": " it stays large"}, "gen_args_1": {"arg_0": "When approaching an elephant from a great distance,", "arg_1": " it grows larger"}, "gen_args_2": {"arg_0": "When approaching an elephant from a great distance,", "arg_1": " it gets bigger"}, "gen_args_3": {"arg_0": "When approaching an elephant from a great distance,", "arg_1": " it looks bigger"}}, "resps": [[["-26.375", "False"]], [["-17.25", "False"]], [["-22.0", "False"]], [["-17.25", "False"]]], "filtered_resps": [["-26.375", "False"], ["-17.25", "False"], ["-22.0", "False"], ["-17.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f8d625b6258ba6c26736a406391a937d9dd6ee5328421bda1741316ca1f6b744", "prompt_hash": "b9f050ac9e705e743b7832b2384fdd0457313ef4779a22954980d89dc569091e", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 322, "doc": {"id": "276", "question_stem": "What would cause a human to grow?", "choices": {"text": ["light waves", "eating wheat", "photosynthesis", "marching"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "What would cause a human to grow?", "arg_1": " light waves"}, "gen_args_1": {"arg_0": "What would cause a human to grow?", "arg_1": " eating wheat"}, "gen_args_2": {"arg_0": "What would cause a human to grow?", "arg_1": " photosynthesis"}, "gen_args_3": {"arg_0": "What would cause a human to grow?", "arg_1": " marching"}}, "resps": [[["-28.5", "False"]], [["-25.625", "False"]], [["-22.5", "False"]], [["-22.75", "False"]]], "filtered_resps": [["-28.5", "False"], ["-25.625", "False"], ["-22.5", "False"], ["-22.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "3f5b4f992421551a46e8f1ceb5695944e7f3259379ea5a55a035725c210b5988", "prompt_hash": "899afdf53094eabbc2d58eb5a63f6b4a1b2538149bb185b9705800733f3e2281", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 323, "doc": {"id": "7-855", "question_stem": "A saguaro has adaptations for an environment with", "choices": {"text": ["lots of snow", "many people", "less water", "more water"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A saguaro has adaptations for an environment with", "arg_1": " lots of snow"}, "gen_args_1": {"arg_0": "A saguaro has adaptations for an environment with", "arg_1": " many people"}, "gen_args_2": {"arg_0": "A saguaro has adaptations for an environment with", "arg_1": " less water"}, "gen_args_3": {"arg_0": "A saguaro has adaptations for an environment with", "arg_1": " more water"}}, "resps": [[["-11.625", "False"]], [["-13.375", "False"]], [["-9.4375", "False"]], [["-12.75", "False"]]], "filtered_resps": [["-11.625", "False"], ["-13.375", "False"], ["-9.4375", "False"], ["-12.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "fa840d07471d260a3385ac0ffbe75eb9cc2e11e3b8d0f22d290a9467ff0c338c", "prompt_hash": "8bdccec8eb2c4c6305cd05c538cff1d04ff91545a6ab06dfc9aca0a1706c0ee9", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 324, "doc": {"id": "664", "question_stem": "There are less hummingbirds by this house than before because of", "choices": {"text": ["a feeder at this house", "the birds no longer like feeders", "the size of the feeder", "a feeder at another house"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "There are less hummingbirds by this house than before because of", "arg_1": " a feeder at this house"}, "gen_args_1": {"arg_0": "There are less hummingbirds by this house than before because of", "arg_1": " the birds no longer like feeders"}, "gen_args_2": {"arg_0": "There are less hummingbirds by this house than before because of", "arg_1": " the size of the feeder"}, "gen_args_3": {"arg_0": "There are less hummingbirds by this house than before because of", "arg_1": " a feeder at another house"}}, "resps": [[["-25.5", "False"]], [["-30.625", "False"]], [["-16.0", "False"]], [["-28.375", "False"]]], "filtered_resps": [["-25.5", "False"], ["-30.625", "False"], ["-16.0", "False"], ["-28.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2c497a824a4a5beec1bafc1bf424ed2a650a5df39430ad660a0c1a7bfdad3ff0", "prompt_hash": "18a82fb2133ea5aa2dc3254e4c78e11c92a95ea0a56bc657097023dc23006789", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 325, "doc": {"id": "9-883", "question_stem": "the  oceans are full of", "choices": {"text": ["water lilies", "guppies", "sea life", "fresh water"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "the  oceans are full of", "arg_1": " water lilies"}, "gen_args_1": {"arg_0": "the  oceans are full of", "arg_1": " guppies"}, "gen_args_2": {"arg_0": "the  oceans are full of", "arg_1": " sea life"}, "gen_args_3": {"arg_0": "the  oceans are full of", "arg_1": " fresh water"}}, "resps": [[["-19.75", "False"]], [["-14.5", "False"]], [["-9.0625", "False"]], [["-11.0625", "False"]]], "filtered_resps": [["-19.75", "False"], ["-14.5", "False"], ["-9.0625", "False"], ["-11.0625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a8acbbe4080e3c5adddd624d8db45468fe9599333ebb9c23daf0139778f0f7fb", "prompt_hash": "5312689290527ea8929766c6e8941a92127d6a0517771426b373186a4575e21e", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 326, "doc": {"id": "9-550", "question_stem": "A light was off because the cord was", "choices": {"text": ["sitting on the table", "attached to the wall", "attached to an extension cord", "attached to a battery pack"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "A light was off because the cord was", "arg_1": " sitting on the table"}, "gen_args_1": {"arg_0": "A light was off because the cord was", "arg_1": " attached to the wall"}, "gen_args_2": {"arg_0": "A light was off because the cord was", "arg_1": " attached to an extension cord"}, "gen_args_3": {"arg_0": "A light was off because the cord was", "arg_1": " attached to a battery pack"}}, "resps": [[["-18.125", "False"]], [["-12.6875", "False"]], [["-21.75", "False"]], [["-23.0", "False"]]], "filtered_resps": [["-18.125", "False"], ["-12.6875", "False"], ["-21.75", "False"], ["-23.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4920f9641fe8ac3173de31de93323fb47fbc8de3b6cb1db664340e500b2e5c21", "prompt_hash": "cafdf9b34306ed5bdb49a49477be58544ebc226324cde0670df1292f4770c526", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 327, "doc": {"id": "8-493", "question_stem": "In the hottest months in the hottest desert, creatures such as birds may find water to drink", "choices": {"text": ["in sticks", "in pebbles", "in sand", "in spiked plants"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "In the hottest months in the hottest desert, creatures such as birds may find water to drink", "arg_1": " in sticks"}, "gen_args_1": {"arg_0": "In the hottest months in the hottest desert, creatures such as birds may find water to drink", "arg_1": " in pebbles"}, "gen_args_2": {"arg_0": "In the hottest months in the hottest desert, creatures such as birds may find water to drink", "arg_1": " in sand"}, "gen_args_3": {"arg_0": "In the hottest months in the hottest desert, creatures such as birds may find water to drink", "arg_1": " in spiked plants"}}, "resps": [[["-19.5", "False"]], [["-14.5", "False"]], [["-10.375", "False"]], [["-23.25", "False"]]], "filtered_resps": [["-19.5", "False"], ["-14.5", "False"], ["-10.375", "False"], ["-23.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "9b56a3b0081a9661fbb35ca1a68902ea1432647643c82af34503d88ff59f8d05", "prompt_hash": "62e0d405f876d4e0af97f46a742ae5f0a4033d7099d116384b5784206669f3f1", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 328, "doc": {"id": "9-257", "question_stem": "Why might a polar bear grow white hair?", "choices": {"text": ["look fancy", "random", "blend in", "stand out"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Why might a polar bear grow white hair?", "arg_1": " look fancy"}, "gen_args_1": {"arg_0": "Why might a polar bear grow white hair?", "arg_1": " random"}, "gen_args_2": {"arg_0": "Why might a polar bear grow white hair?", "arg_1": " blend in"}, "gen_args_3": {"arg_0": "Why might a polar bear grow white hair?", "arg_1": " stand out"}}, "resps": [[["-40.25", "False"]], [["-22.375", "False"]], [["-28.75", "False"]], [["-27.375", "False"]]], "filtered_resps": [["-40.25", "False"], ["-22.375", "False"], ["-28.75", "False"], ["-27.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "060330aadf523d47edccc2b61bafc60397b6b65eda9d280fd816e091b8395902", "prompt_hash": "af1060d383f67b728de5ce3749b5b878d80460f21f5844f1fc1ce0c1d6a8435a", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 329, "doc": {"id": "1239", "question_stem": "Xylem", "choices": {"text": ["discourages pests from landing on leaves", "allows plants to move carbon dioxide from root to stems", "carries seedlings from roots to leaves", "allows plants to move rain thru their systems"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Xylem", "arg_1": " discourages pests from landing on leaves"}, "gen_args_1": {"arg_0": "Xylem", "arg_1": " allows plants to move carbon dioxide from root to stems"}, "gen_args_2": {"arg_0": "Xylem", "arg_1": " carries seedlings from roots to leaves"}, "gen_args_3": {"arg_0": "Xylem", "arg_1": " allows plants to move rain thru their systems"}}, "resps": [[["-44.0", "False"]], [["-50.5", "False"]], [["-39.25", "False"]], [["-57.75", "False"]]], "filtered_resps": [["-44.0", "False"], ["-50.5", "False"], ["-39.25", "False"], ["-57.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "01c93edf5d81ff54581b381a04135eed3f72eaadc27a906c5d511d6267791e32", "prompt_hash": "b78476d7637a563abf1ce36e1b8eefa7628be66814e229a289d6bbdb978f0303", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 330, "doc": {"id": "869", "question_stem": "A food that is a source of heat is", "choices": {"text": ["ramen", "salad", "ice cream", "sushi"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "A food that is a source of heat is", "arg_1": " ramen"}, "gen_args_1": {"arg_0": "A food that is a source of heat is", "arg_1": " salad"}, "gen_args_2": {"arg_0": "A food that is a source of heat is", "arg_1": " ice cream"}, "gen_args_3": {"arg_0": "A food that is a source of heat is", "arg_1": " sushi"}}, "resps": [[["-13.8125", "False"]], [["-14.5625", "False"]], [["-13.5", "False"]], [["-14.625", "False"]]], "filtered_resps": [["-13.8125", "False"], ["-14.5625", "False"], ["-13.5", "False"], ["-14.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "20c6c2f9ec25606f267daee3b058a72f30af1da5fae135076add3c05ee8a9710", "prompt_hash": "c25859fb8de4f48d3d87397dfbe1e055632198c4914047abfa1ac74d04202b48", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 331, "doc": {"id": "7-1105", "question_stem": "When heat is added to something", "choices": {"text": ["contaminates may be destroyed", "bacterial can grow more rapidly", "viruses may be picked up", "the thing loses energy"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "When heat is added to something", "arg_1": " contaminates may be destroyed"}, "gen_args_1": {"arg_0": "When heat is added to something", "arg_1": " bacterial can grow more rapidly"}, "gen_args_2": {"arg_0": "When heat is added to something", "arg_1": " viruses may be picked up"}, "gen_args_3": {"arg_0": "When heat is added to something", "arg_1": " the thing loses energy"}}, "resps": [[["-40.25", "False"]], [["-40.5", "False"]], [["-43.0", "False"]], [["-21.25", "False"]]], "filtered_resps": [["-40.25", "False"], ["-40.5", "False"], ["-43.0", "False"], ["-21.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a8cd81357e55a55e8a2e99043082ae37e03f6776eef3ce75b33eef4ba14caf0d", "prompt_hash": "501d04916822fbdace4e873721851f135af423d02e87a36044e38cc4fd6d9c87", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 332, "doc": {"id": "597", "question_stem": "The heart is an example of", "choices": {"text": ["a part of the nervous system", "an organ that filters toxins", "a self-healing protector from germs", "something protected by the skeletal system"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "The heart is an example of", "arg_1": " a part of the nervous system"}, "gen_args_1": {"arg_0": "The heart is an example of", "arg_1": " an organ that filters toxins"}, "gen_args_2": {"arg_0": "The heart is an example of", "arg_1": " a self-healing protector from germs"}, "gen_args_3": {"arg_0": "The heart is an example of", "arg_1": " something protected by the skeletal system"}}, "resps": [[["-12.6875", "False"]], [["-18.375", "False"]], [["-46.5", "False"]], [["-29.25", "False"]]], "filtered_resps": [["-12.6875", "False"], ["-18.375", "False"], ["-46.5", "False"], ["-29.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "c90978d6063b96419359ce51d7e8b38479e887258934e673b078034f3ee7af94", "prompt_hash": "89d035f304e4a1a9c7bdfab2ac5dc76f6e2505c9f077abe819bcfec07f529a14", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 333, "doc": {"id": "385", "question_stem": "Prey are eaten by", "choices": {"text": ["an animal herded by sheep dogs", "the animal with a starring role in Bambi", "animals known for their memory", "the fastest mammal with four legs"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Prey are eaten by", "arg_1": " an animal herded by sheep dogs"}, "gen_args_1": {"arg_0": "Prey are eaten by", "arg_1": " the animal with a starring role in Bambi"}, "gen_args_2": {"arg_0": "Prey are eaten by", "arg_1": " animals known for their memory"}, "gen_args_3": {"arg_0": "Prey are eaten by", "arg_1": " the fastest mammal with four legs"}}, "resps": [[["-48.25", "False"]], [["-44.25", "False"]], [["-27.0", "False"]], [["-35.0", "False"]]], "filtered_resps": [["-48.25", "False"], ["-44.25", "False"], ["-27.0", "False"], ["-35.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "88d193af7c507c8b43e090873bd6214c74fa5e059f3b7879bf0e14ddea8d3597", "prompt_hash": "aa30e4132af401b5bfabb4c854c6fc598d3986dd1e3e0db8116f40a1c6a77805", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 334, "doc": {"id": "1301", "question_stem": "A dog is warm-blooded just like", "choices": {"text": ["a snake", "a cardinal", "a spider", "a scorpion"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "A dog is warm-blooded just like", "arg_1": " a snake"}, "gen_args_1": {"arg_0": "A dog is warm-blooded just like", "arg_1": " a cardinal"}, "gen_args_2": {"arg_0": "A dog is warm-blooded just like", "arg_1": " a spider"}, "gen_args_3": {"arg_0": "A dog is warm-blooded just like", "arg_1": " a scorpion"}}, "resps": [[["-7.90625", "False"]], [["-19.125", "False"]], [["-15.8125", "False"]], [["-16.0", "False"]]], "filtered_resps": [["-7.90625", "False"], ["-19.125", "False"], ["-15.8125", "False"], ["-16.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e673412399563e3d29ddc83d76f4585a5e10c05e9deacfa7baaf78665d354bf3", "prompt_hash": "b81938c9fb1494135862c49abf437b9f0844ba49012ba1c946a9fd571bbaf5b6", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 335, "doc": {"id": "9-893", "question_stem": "A flashlight will need this in order to radiate photons:", "choices": {"text": ["radiation", "acoustic energy", "vibrations", "electron flow"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "A flashlight will need this in order to radiate photons:", "arg_1": " radiation"}, "gen_args_1": {"arg_0": "A flashlight will need this in order to radiate photons:", "arg_1": " acoustic energy"}, "gen_args_2": {"arg_0": "A flashlight will need this in order to radiate photons:", "arg_1": " vibrations"}, "gen_args_3": {"arg_0": "A flashlight will need this in order to radiate photons:", "arg_1": " electron flow"}}, "resps": [[["-11.1875", "False"]], [["-18.75", "False"]], [["-16.375", "False"]], [["-13.5", "False"]]], "filtered_resps": [["-11.1875", "False"], ["-18.75", "False"], ["-16.375", "False"], ["-13.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "726837cacc8bf39ed03c623d38410a7d9e9c69ede98b7ddba0e6a265d60faa41", "prompt_hash": "a622c6c80ffd03bd7a72379b825a4ff8b192c3ec83911bdce564e6d7fd3671af", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 336, "doc": {"id": "9-369", "question_stem": "to find out how fast you are going you first need to know", "choices": {"text": ["where you're going", "distance traveled", "distance to travel", "home location"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "to find out how fast you are going you first need to know", "arg_1": " where you're going"}, "gen_args_1": {"arg_0": "to find out how fast you are going you first need to know", "arg_1": " distance traveled"}, "gen_args_2": {"arg_0": "to find out how fast you are going you first need to know", "arg_1": " distance to travel"}, "gen_args_3": {"arg_0": "to find out how fast you are going you first need to know", "arg_1": " home location"}}, "resps": [[["-7.03125", "False"]], [["-13.25", "False"]], [["-16.25", "False"]], [["-22.75", "False"]]], "filtered_resps": [["-7.03125", "False"], ["-13.25", "False"], ["-16.25", "False"], ["-22.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "5c17834ec426209fef0cba4c50db105b75af1547a6ee7c3fe3794ce961c02129", "prompt_hash": "9b5a719948fef9182b4d35f05e5015f9471fa6aa20e7fb8da8301da72f175801", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 337, "doc": {"id": "9-1026", "question_stem": "A small lamb, two days old, is walking with its mother. The mother feels ill, so refuses food, which dries up her milk production. The lack of lactation causes the lamb to", "choices": {"text": ["weaken", "strengthen", "coexist", "thrive"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "A small lamb, two days old, is walking with its mother. The mother feels ill, so refuses food, which dries up her milk production. The lack of lactation causes the lamb to", "arg_1": " weaken"}, "gen_args_1": {"arg_0": "A small lamb, two days old, is walking with its mother. The mother feels ill, so refuses food, which dries up her milk production. The lack of lactation causes the lamb to", "arg_1": " strengthen"}, "gen_args_2": {"arg_0": "A small lamb, two days old, is walking with its mother. The mother feels ill, so refuses food, which dries up her milk production. The lack of lactation causes the lamb to", "arg_1": " coexist"}, "gen_args_3": {"arg_0": "A small lamb, two days old, is walking with its mother. The mother feels ill, so refuses food, which dries up her milk production. The lack of lactation causes the lamb to", "arg_1": " thrive"}}, "resps": [[["-2.953125", "False"]], [["-10.4375", "False"]], [["-22.5", "False"]], [["-11.5", "False"]]], "filtered_resps": [["-2.953125", "False"], ["-10.4375", "False"], ["-22.5", "False"], ["-11.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "cfa9ea6b1640a6afd5d218ded5546af3a4314db1d6b8b516fbf1d838f76bbb38", "prompt_hash": "40ec999e968cc5f95deb8f826fc34bad7b4ea10f409ae5c183726369a949c3db", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 338, "doc": {"id": "7-424", "question_stem": "When trying to find fresh clams for dinner, a hungry person would don", "choices": {"text": ["a dinner jacket", "a diving suit", "a warm coat", "a dress suit"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "When trying to find fresh clams for dinner, a hungry person would don", "arg_1": " a dinner jacket"}, "gen_args_1": {"arg_0": "When trying to find fresh clams for dinner, a hungry person would don", "arg_1": " a diving suit"}, "gen_args_2": {"arg_0": "When trying to find fresh clams for dinner, a hungry person would don", "arg_1": " a warm coat"}, "gen_args_3": {"arg_0": "When trying to find fresh clams for dinner, a hungry person would don", "arg_1": " a dress suit"}}, "resps": [[["-15.75", "False"]], [["-3.46875", "False"]], [["-8.5", "False"]], [["-18.25", "False"]]], "filtered_resps": [["-15.75", "False"], ["-3.46875", "False"], ["-8.5", "False"], ["-18.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a412f174396535d5f4288204dfb962c0dfb6d07aca192da7f7284fb25e8316a7", "prompt_hash": "769273f5eb6943b3e2decb288389d881c5603ae976b2f9e6a3df6c3d43745ff6", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 339, "doc": {"id": "9-259", "question_stem": "How do polar bears survive the cold?", "choices": {"text": ["B and D", "Double Fur Coats", "Cold blooded", "Compact ears"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "How do polar bears survive the cold?", "arg_1": " B and D"}, "gen_args_1": {"arg_0": "How do polar bears survive the cold?", "arg_1": " Double Fur Coats"}, "gen_args_2": {"arg_0": "How do polar bears survive the cold?", "arg_1": " Cold blooded"}, "gen_args_3": {"arg_0": "How do polar bears survive the cold?", "arg_1": " Compact ears"}}, "resps": [[["-27.875", "False"]], [["-39.25", "False"]], [["-22.5", "False"]], [["-31.0", "False"]]], "filtered_resps": [["-27.875", "False"], ["-39.25", "False"], ["-22.5", "False"], ["-31.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "baf0982d054aa90c7804a4684291435fbb1ee67393ad8833175f5df0a8202724", "prompt_hash": "8443de118db4791cd1a6c35e771772b4f174369e687ff5972fd50fc14d4ee0d3", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 340, "doc": {"id": "9-783", "question_stem": "A solid is likely to form in extreme", "choices": {"text": ["floods", "wind", "chill", "rain"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A solid is likely to form in extreme", "arg_1": " floods"}, "gen_args_1": {"arg_0": "A solid is likely to form in extreme", "arg_1": " wind"}, "gen_args_2": {"arg_0": "A solid is likely to form in extreme", "arg_1": " chill"}, "gen_args_3": {"arg_0": "A solid is likely to form in extreme", "arg_1": " rain"}}, "resps": [[["-14.375", "False"]], [["-10.0625", "False"]], [["-13.875", "False"]], [["-10.875", "False"]]], "filtered_resps": [["-14.375", "False"], ["-10.0625", "False"], ["-13.875", "False"], ["-10.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "93ea698621be2725a860c169582cfae75c2ea671b803e6fbbc093172538a23e4", "prompt_hash": "3248deed3f896eeb749117af050836dd389dbfe791f445537681bb2a23a0517c", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 341, "doc": {"id": "1088", "question_stem": "What are the feet of Dendrocygna autumnalis designed for?", "choices": {"text": ["catching prey", "aquatic speed", "flying", "walking"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "What are the feet of Dendrocygna autumnalis designed for?", "arg_1": " catching prey"}, "gen_args_1": {"arg_0": "What are the feet of Dendrocygna autumnalis designed for?", "arg_1": " aquatic speed"}, "gen_args_2": {"arg_0": "What are the feet of Dendrocygna autumnalis designed for?", "arg_1": " flying"}, "gen_args_3": {"arg_0": "What are the feet of Dendrocygna autumnalis designed for?", "arg_1": " walking"}}, "resps": [[["-19.625", "False"]], [["-31.625", "False"]], [["-19.25", "False"]], [["-18.0", "False"]]], "filtered_resps": [["-19.625", "False"], ["-31.625", "False"], ["-19.25", "False"], ["-18.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2f22238d2c336c6688fc1874130f606539be8c249cb343ed72fe2a157830b638", "prompt_hash": "340b92cd04bb3f87ec288dcdef90fab96497361faab5c5e41ca2ff7abe69510e", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 342, "doc": {"id": "1387", "question_stem": "What is an example of fire giving off light?", "choices": {"text": ["an oven is preheated and the pilot light is lit", "a match is lit to light a cigarette", "a lit candle in a window signalling to someone", "a fire that was put out to send smoke signals"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "What is an example of fire giving off light?", "arg_1": " an oven is preheated and the pilot light is lit"}, "gen_args_1": {"arg_0": "What is an example of fire giving off light?", "arg_1": " a match is lit to light a cigarette"}, "gen_args_2": {"arg_0": "What is an example of fire giving off light?", "arg_1": " a lit candle in a window signalling to someone"}, "gen_args_3": {"arg_0": "What is an example of fire giving off light?", "arg_1": " a fire that was put out to send smoke signals"}}, "resps": [[["-43.25", "False"]], [["-48.0", "False"]], [["-61.0", "False"]], [["-56.5", "False"]]], "filtered_resps": [["-43.25", "False"], ["-48.0", "False"], ["-61.0", "False"], ["-56.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d4296f9013245bfec3e4e1228fdc750dd4a6a7dfe53c5f7befa54c8c46bfde3b", "prompt_hash": "eafd3e456949c7e19536dded5b1b17516d3a290aa921b01e4bfce7258e377171", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 343, "doc": {"id": "7-1062", "question_stem": "The respiratory system works by", "choices": {"text": ["directing oxygen from lungs to other organs", "pushing air through lungs", "moving air in a room", "making air quality better"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "The respiratory system works by", "arg_1": " directing oxygen from lungs to other organs"}, "gen_args_1": {"arg_0": "The respiratory system works by", "arg_1": " pushing air through lungs"}, "gen_args_2": {"arg_0": "The respiratory system works by", "arg_1": " moving air in a room"}, "gen_args_3": {"arg_0": "The respiratory system works by", "arg_1": " making air quality better"}}, "resps": [[["-37.75", "False"]], [["-13.875", "False"]], [["-23.25", "False"]], [["-21.5", "False"]]], "filtered_resps": [["-37.75", "False"], ["-13.875", "False"], ["-23.25", "False"], ["-21.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "5b2f860828152411b9a98278385621027e202b1ea72f32301817e71f1237e115", "prompt_hash": "3418dbcb57ef31ffde053c3fe6772d1ad72417169b9b74fd57f8b9a078ae2038", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 344, "doc": {"id": "676", "question_stem": "Animals have more fat", "choices": {"text": ["in the ocean", "in human homes", "in landfills", "in polar areas"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Animals have more fat", "arg_1": " in the ocean"}, "gen_args_1": {"arg_0": "Animals have more fat", "arg_1": " in human homes"}, "gen_args_2": {"arg_0": "Animals have more fat", "arg_1": " in landfills"}, "gen_args_3": {"arg_0": "Animals have more fat", "arg_1": " in polar areas"}}, "resps": [[["-15.0625", "False"]], [["-23.0", "False"]], [["-16.875", "False"]], [["-20.5", "False"]]], "filtered_resps": [["-15.0625", "False"], ["-23.0", "False"], ["-16.875", "False"], ["-20.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f8ac310aaea04a44688e759026035beedc3ffb73f3710b71a9dfef611d0b6670", "prompt_hash": "0609c6393ac6f6410dffb8767795c3fa27ce7ecad5c77480d6a4779bc7f3e462", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 345, "doc": {"id": "1998", "question_stem": "A stick bug uses what to protect itself from predators?", "choices": {"text": ["poison", "its appearance", "speed", "hearing"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "A stick bug uses what to protect itself from predators?", "arg_1": " poison"}, "gen_args_1": {"arg_0": "A stick bug uses what to protect itself from predators?", "arg_1": " its appearance"}, "gen_args_2": {"arg_0": "A stick bug uses what to protect itself from predators?", "arg_1": " speed"}, "gen_args_3": {"arg_0": "A stick bug uses what to protect itself from predators?", "arg_1": " hearing"}}, "resps": [[["-15.4375", "False"]], [["-21.25", "False"]], [["-17.375", "False"]], [["-19.0", "False"]]], "filtered_resps": [["-15.4375", "False"], ["-21.25", "False"], ["-17.375", "False"], ["-19.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "cacd09957e10548fb000420d2132fc8b4ddd5ae9a1936988ea4662ce376b9dca", "prompt_hash": "18314d8ef2e1aed47d43228eccbf5e7a3162ad00d989b0a8726604cb0bd49188", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 346, "doc": {"id": "1698", "question_stem": "Corn is sometimes used to make", "choices": {"text": ["a simple alcohol", "water", "glass", "milk"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Corn is sometimes used to make", "arg_1": " a simple alcohol"}, "gen_args_1": {"arg_0": "Corn is sometimes used to make", "arg_1": " water"}, "gen_args_2": {"arg_0": "Corn is sometimes used to make", "arg_1": " glass"}, "gen_args_3": {"arg_0": "Corn is sometimes used to make", "arg_1": " milk"}}, "resps": [[["-12.875", "False"]], [["-7.5", "False"]], [["-7.59375", "False"]], [["-6.09375", "False"]]], "filtered_resps": [["-12.875", "False"], ["-7.5", "False"], ["-7.59375", "False"], ["-6.09375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "0ba1a63fbc0e1b29f4278b71425d5d08032721ec3f34e18f76411a9026434903", "prompt_hash": "80c6a1dacfa5ec1c62ae8b40ef5f9084b2511574f28bfb9603b4d516de26b01e", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 347, "doc": {"id": "490", "question_stem": "The inside of the Thanksgiving turkey is white instead of pink because of", "choices": {"text": ["heat energy", "light energy", "color energy", "color transfusion"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "The inside of the Thanksgiving turkey is white instead of pink because of", "arg_1": " heat energy"}, "gen_args_1": {"arg_0": "The inside of the Thanksgiving turkey is white instead of pink because of", "arg_1": " light energy"}, "gen_args_2": {"arg_0": "The inside of the Thanksgiving turkey is white instead of pink because of", "arg_1": " color energy"}, "gen_args_3": {"arg_0": "The inside of the Thanksgiving turkey is white instead of pink because of", "arg_1": " color transfusion"}}, "resps": [[["-22.5", "False"]], [["-21.75", "False"]], [["-27.375", "False"]], [["-27.0", "False"]]], "filtered_resps": [["-22.5", "False"], ["-21.75", "False"], ["-27.375", "False"], ["-27.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f8a0c022d9b80ff22d5048ef282a943b15fc47c7426dc2046770b05c48aa988e", "prompt_hash": "40ce72b357b309be06c064052ac972ff13985e3330ee10302dbe25b64a6fcdc8", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 348, "doc": {"id": "844", "question_stem": "Little puppies are a result of:", "choices": {"text": ["reproduction ?", "pet store sale", "a begging child", "evolution"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Little puppies are a result of:", "arg_1": " reproduction ?"}, "gen_args_1": {"arg_0": "Little puppies are a result of:", "arg_1": " pet store sale"}, "gen_args_2": {"arg_0": "Little puppies are a result of:", "arg_1": " a begging child"}, "gen_args_3": {"arg_0": "Little puppies are a result of:", "arg_1": " evolution"}}, "resps": [[["-27.25", "False"]], [["-25.875", "False"]], [["-34.25", "False"]], [["-16.25", "False"]]], "filtered_resps": [["-27.25", "False"], ["-25.875", "False"], ["-34.25", "False"], ["-16.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4973b0ae78358bce6df6db22dc753420da1981770ba73f99801e2d32e346316c", "prompt_hash": "b909fade5113a153fd2b8fd29ba4225df85f865e903fbab91a59e8696013c7e2", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 349, "doc": {"id": "1795", "question_stem": "What would a Jersey most likely be fed?", "choices": {"text": ["hamburger", "moles", "alfalfa", "cow"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "What would a Jersey most likely be fed?", "arg_1": " hamburger"}, "gen_args_1": {"arg_0": "What would a Jersey most likely be fed?", "arg_1": " moles"}, "gen_args_2": {"arg_0": "What would a Jersey most likely be fed?", "arg_1": " alfalfa"}, "gen_args_3": {"arg_0": "What would a Jersey most likely be fed?", "arg_1": " cow"}}, "resps": [[["-18.25", "False"]], [["-18.75", "False"]], [["-16.125", "False"]], [["-15.3125", "False"]]], "filtered_resps": [["-18.25", "False"], ["-18.75", "False"], ["-16.125", "False"], ["-15.3125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "92f26e7a52e24b108757a37c090f675437ad672b53222be50068a60cf21ad649", "prompt_hash": "0a2ebbb70a8ed5eb99905173dc31026991478a682f4636bae587cfec8328d531", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 350, "doc": {"id": "1508", "question_stem": "Which of these energy sources generates the least amount of pollution?", "choices": {"text": ["coal", "windmill", "lithium batteries", "gasoline"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Which of these energy sources generates the least amount of pollution?", "arg_1": " coal"}, "gen_args_1": {"arg_0": "Which of these energy sources generates the least amount of pollution?", "arg_1": " windmill"}, "gen_args_2": {"arg_0": "Which of these energy sources generates the least amount of pollution?", "arg_1": " lithium batteries"}, "gen_args_3": {"arg_0": "Which of these energy sources generates the least amount of pollution?", "arg_1": " gasoline"}}, "resps": [[["-15.125", "False"]], [["-21.375", "False"]], [["-24.875", "False"]], [["-22.0", "False"]]], "filtered_resps": [["-15.125", "False"], ["-21.375", "False"], ["-24.875", "False"], ["-22.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "b716b9f62f68e0f7f30de9df15b1828c248dd9e34b8c756c6fd0bede3e3eeb5f", "prompt_hash": "0cf8d6a091ef8b1cae9f2f77cad71aeabb60914d4b0e888f180161cda86f021e", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 351, "doc": {"id": "9-289", "question_stem": "Grass snakes live in what?", "choices": {"text": ["trees", "mountains", "lakes", "turf"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Grass snakes live in what?", "arg_1": " trees"}, "gen_args_1": {"arg_0": "Grass snakes live in what?", "arg_1": " mountains"}, "gen_args_2": {"arg_0": "Grass snakes live in what?", "arg_1": " lakes"}, "gen_args_3": {"arg_0": "Grass snakes live in what?", "arg_1": " turf"}}, "resps": [[["-16.75", "False"]], [["-16.625", "False"]], [["-18.375", "False"]], [["-19.875", "False"]]], "filtered_resps": [["-16.75", "False"], ["-16.625", "False"], ["-18.375", "False"], ["-19.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "db6ce62fd8af89c16c51638dadd4c147476ce238a305f6cfdf3656308e623e80", "prompt_hash": "cf8293ca9e51a3b8b8de69e27d8b907c7f4eedeb67f1c43877f0a497e551854a", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 352, "doc": {"id": "9-668", "question_stem": "Cephalopod ink is by octopuses to", "choices": {"text": ["mate", "feed", "hide", "play"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Cephalopod ink is by octopuses to", "arg_1": " mate"}, "gen_args_1": {"arg_0": "Cephalopod ink is by octopuses to", "arg_1": " feed"}, "gen_args_2": {"arg_0": "Cephalopod ink is by octopuses to", "arg_1": " hide"}, "gen_args_3": {"arg_0": "Cephalopod ink is by octopuses to", "arg_1": " play"}}, "resps": [[["-8.5", "False"]], [["-6.34375", "False"]], [["-8.4375", "False"]], [["-7.96875", "False"]]], "filtered_resps": [["-8.5", "False"], ["-6.34375", "False"], ["-8.4375", "False"], ["-7.96875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "56f1d447a2a049b8ca401f1f2391595595e3638665b48ad15c2135c2a8b68cd9", "prompt_hash": "9c0af9d39a8c0b783aa67924feceefefc490e04b5086404d8a6dc98ad889756b", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 353, "doc": {"id": "7-364", "question_stem": "Tapping a drumstick to a drum will", "choices": {"text": ["reverberate when touched together", "vibrate when next to each other", "shake around when near", "put each other down"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Tapping a drumstick to a drum will", "arg_1": " reverberate when touched together"}, "gen_args_1": {"arg_0": "Tapping a drumstick to a drum will", "arg_1": " vibrate when next to each other"}, "gen_args_2": {"arg_0": "Tapping a drumstick to a drum will", "arg_1": " shake around when near"}, "gen_args_3": {"arg_0": "Tapping a drumstick to a drum will", "arg_1": " put each other down"}}, "resps": [[["-29.375", "False"]], [["-39.25", "False"]], [["-35.0", "False"]], [["-32.5", "False"]]], "filtered_resps": [["-29.375", "False"], ["-39.25", "False"], ["-35.0", "False"], ["-32.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "7b73ebfd07d48a1d43aba51b7d37053ac99a27d6b8625d94ccd5bf34feb3e286", "prompt_hash": "4a7568acd98f31f9a982ab3810a7dd68037a1c4d25f446f548ed6f5e8e579221", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 354, "doc": {"id": "1271", "question_stem": "Snow is more likely to fall two months before", "choices": {"text": ["June", "March", "September", "December"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Snow is more likely to fall two months before", "arg_1": " June"}, "gen_args_1": {"arg_0": "Snow is more likely to fall two months before", "arg_1": " March"}, "gen_args_2": {"arg_0": "Snow is more likely to fall two months before", "arg_1": " September"}, "gen_args_3": {"arg_0": "Snow is more likely to fall two months before", "arg_1": " December"}}, "resps": [[["-11.625", "False"]], [["-12.875", "False"]], [["-11.125", "False"]], [["-9.4375", "False"]]], "filtered_resps": [["-11.625", "False"], ["-12.875", "False"], ["-11.125", "False"], ["-9.4375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "bd42436cfd29c054b2500a752ec1d0768734285eefc39b47754e762a5f96b8c8", "prompt_hash": "eddb33645b13a7b27500bc3f3bedac3390a72e9dda06790b33567360d52dc6f1", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 355, "doc": {"id": "9-1117", "question_stem": "If I want to avoid being dinner for some type of frog what should I reincarnate as?", "choices": {"text": ["Scorpion", "House Fly", "Cricket", "Moth"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "If I want to avoid being dinner for some type of frog what should I reincarnate as?", "arg_1": " Scorpion"}, "gen_args_1": {"arg_0": "If I want to avoid being dinner for some type of frog what should I reincarnate as?", "arg_1": " House Fly"}, "gen_args_2": {"arg_0": "If I want to avoid being dinner for some type of frog what should I reincarnate as?", "arg_1": " Cricket"}, "gen_args_3": {"arg_0": "If I want to avoid being dinner for some type of frog what should I reincarnate as?", "arg_1": " Moth"}}, "resps": [[["-20.25", "False"]], [["-27.75", "False"]], [["-19.875", "False"]], [["-18.25", "False"]]], "filtered_resps": [["-20.25", "False"], ["-27.75", "False"], ["-19.875", "False"], ["-18.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "b1f2e9f41bcecb2611d11542a018977eca3efc3e33b1756346cc1701f4a3c6da", "prompt_hash": "70d44145657b98d4ff9f61df80684c9fa21ef5aa68577c4044dca584c1f5d8c1", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 356, "doc": {"id": "35", "question_stem": "Dead plants are easier to find in", "choices": {"text": ["January", "July", "May", "September"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Dead plants are easier to find in", "arg_1": " January"}, "gen_args_1": {"arg_0": "Dead plants are easier to find in", "arg_1": " July"}, "gen_args_2": {"arg_0": "Dead plants are easier to find in", "arg_1": " May"}, "gen_args_3": {"arg_0": "Dead plants are easier to find in", "arg_1": " September"}}, "resps": [[["-9.125", "False"]], [["-9.9375", "False"]], [["-12.5", "False"]], [["-9.75", "False"]]], "filtered_resps": [["-9.125", "False"], ["-9.9375", "False"], ["-12.5", "False"], ["-9.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "19fe379892ded057ca8cc23f462ccd86a4cc9250eab2bfe6818c698717d950a3", "prompt_hash": "57b37b4916d36142a4520f20aba8c77f64079980a7c33b444303bb274470ee4e", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 357, "doc": {"id": "1660", "question_stem": "The harder a child pushes a toy car", "choices": {"text": ["decreases the distance it will travel", "the further it will roll across the floor", "the quicker the child will want to play with another toy", "determines how long the child with play with it"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "The harder a child pushes a toy car", "arg_1": " decreases the distance it will travel"}, "gen_args_1": {"arg_0": "The harder a child pushes a toy car", "arg_1": " the further it will roll across the floor"}, "gen_args_2": {"arg_0": "The harder a child pushes a toy car", "arg_1": " the quicker the child will want to play with another toy"}, "gen_args_3": {"arg_0": "The harder a child pushes a toy car", "arg_1": " determines how long the child with play with it"}}, "resps": [[["-25.375", "False"]], [["-30.75", "False"]], [["-40.0", "False"]], [["-49.5", "False"]]], "filtered_resps": [["-25.375", "False"], ["-30.75", "False"], ["-40.0", "False"], ["-49.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "97af09f182617f6e5a29dbf47ae2bc6bd00a64384c0272dd651b7139c1622a23", "prompt_hash": "f57b85d46760b746668244144c7f2181d826349be18879f86152fe3312cafadd", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 358, "doc": {"id": "7-710", "question_stem": "Fossil fuels", "choices": {"text": ["come from old age", "come from expired life", "take two years to create", "are created in a year"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Fossil fuels", "arg_1": " come from old age"}, "gen_args_1": {"arg_0": "Fossil fuels", "arg_1": " come from expired life"}, "gen_args_2": {"arg_0": "Fossil fuels", "arg_1": " take two years to create"}, "gen_args_3": {"arg_0": "Fossil fuels", "arg_1": " are created in a year"}}, "resps": [[["-25.25", "False"]], [["-30.125", "False"]], [["-26.875", "False"]], [["-28.625", "False"]]], "filtered_resps": [["-25.25", "False"], ["-30.125", "False"], ["-26.875", "False"], ["-28.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a786d0a3c681927e236b1775da5515adc9f131ac2d58746e8e5e7502378129ad", "prompt_hash": "23aa57b287735b906e179d121aa8cb509ea2b920c5739b29b5bf0584864b18d9", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 359, "doc": {"id": "8-52", "question_stem": "A star, burning far, far away, has enormous pressure and temperature. This allows for", "choices": {"text": ["a room to have overhead lights", "night on Earth to be dimly lit", "plastic stars to decorate a ceiling", "a person to be the star of a show"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "A star, burning far, far away, has enormous pressure and temperature. This allows for", "arg_1": " a room to have overhead lights"}, "gen_args_1": {"arg_0": "A star, burning far, far away, has enormous pressure and temperature. This allows for", "arg_1": " night on Earth to be dimly lit"}, "gen_args_2": {"arg_0": "A star, burning far, far away, has enormous pressure and temperature. This allows for", "arg_1": " plastic stars to decorate a ceiling"}, "gen_args_3": {"arg_0": "A star, burning far, far away, has enormous pressure and temperature. This allows for", "arg_1": " a person to be the star of a show"}}, "resps": [[["-37.0", "False"]], [["-40.75", "False"]], [["-45.0", "False"]], [["-35.75", "False"]]], "filtered_resps": [["-37.0", "False"], ["-40.75", "False"], ["-45.0", "False"], ["-35.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "5634a06204580cea7f859bb696d7b90c2611b422e752a0663fd184524926a2bf", "prompt_hash": "e318cf03a1a9723be92ba0d6a65da8dada18be597ad9d9d5ad0a80490d4446cd", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 360, "doc": {"id": "9-1167", "question_stem": "Erosion is caused by different kinds of", "choices": {"text": ["soil", "fish", "rocks", "weather"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Erosion is caused by different kinds of", "arg_1": " soil"}, "gen_args_1": {"arg_0": "Erosion is caused by different kinds of", "arg_1": " fish"}, "gen_args_2": {"arg_0": "Erosion is caused by different kinds of", "arg_1": " rocks"}, "gen_args_3": {"arg_0": "Erosion is caused by different kinds of", "arg_1": " weather"}}, "resps": [[["-9.125", "False"]], [["-22.875", "False"]], [["-7.875", "False"]], [["-5.1875", "False"]]], "filtered_resps": [["-9.125", "False"], ["-22.875", "False"], ["-7.875", "False"], ["-5.1875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e09eacadeb42987e9e0504d496eac5f669acf1a038ba325cb370ef97cc67c1d4", "prompt_hash": "e3c36ae7b5abe345f79cf646e8ccd821c98411ce2714b4e37247d4930de54fc2", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 361, "doc": {"id": "8-43", "question_stem": "Respiration is a", "choices": {"text": ["happens for some species", "happens for only land dwelling mammals", "occurs for only sea creatures", "commonality among all animals"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Respiration is a", "arg_1": " happens for some species"}, "gen_args_1": {"arg_0": "Respiration is a", "arg_1": " happens for only land dwelling mammals"}, "gen_args_2": {"arg_0": "Respiration is a", "arg_1": " occurs for only sea creatures"}, "gen_args_3": {"arg_0": "Respiration is a", "arg_1": " commonality among all animals"}}, "resps": [[["-39.25", "False"]], [["-53.75", "False"]], [["-47.75", "False"]], [["-26.375", "False"]]], "filtered_resps": [["-39.25", "False"], ["-53.75", "False"], ["-47.75", "False"], ["-26.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "342de6d2a76a052ff8313831db251cb5e7d284fbcca882e0aa4551c2b54b36b8", "prompt_hash": "ad27ea7e72076d80cb5826f8359951f94dc13ea17a8ec9b046af0f577f0d9b5d", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 362, "doc": {"id": "9-57", "question_stem": "In order for your computer to operate, it must have an electrical path that is what?", "choices": {"text": ["magical", "closed", "broken", "open"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "In order for your computer to operate, it must have an electrical path that is what?", "arg_1": " magical"}, "gen_args_1": {"arg_0": "In order for your computer to operate, it must have an electrical path that is what?", "arg_1": " closed"}, "gen_args_2": {"arg_0": "In order for your computer to operate, it must have an electrical path that is what?", "arg_1": " broken"}, "gen_args_3": {"arg_0": "In order for your computer to operate, it must have an electrical path that is what?", "arg_1": " open"}}, "resps": [[["-18.625", "False"]], [["-17.25", "False"]], [["-17.25", "False"]], [["-15.25", "False"]]], "filtered_resps": [["-18.625", "False"], ["-17.25", "False"], ["-17.25", "False"], ["-15.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4316505bfcd6b74c7808c27cd447f6128c8fa18a161b92e309cca631ed942279", "prompt_hash": "9b16de73a3088198c754be886002d96ee3eef306851e60135eb91c6588eaed56", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 363, "doc": {"id": "1411", "question_stem": "Polar bears live in", "choices": {"text": ["frosty environments", "tepid environments", "warm environments", "tropical environments"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Polar bears live in", "arg_1": " frosty environments"}, "gen_args_1": {"arg_0": "Polar bears live in", "arg_1": " tepid environments"}, "gen_args_2": {"arg_0": "Polar bears live in", "arg_1": " warm environments"}, "gen_args_3": {"arg_0": "Polar bears live in", "arg_1": " tropical environments"}}, "resps": [[["-14.6875", "False"]], [["-22.125", "False"]], [["-15.625", "False"]], [["-16.625", "False"]]], "filtered_resps": [["-14.6875", "False"], ["-22.125", "False"], ["-15.625", "False"], ["-16.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "ee3aa7ae904cfd2ba264d6231666dba9318d9225306b5037337c4979ac3b5741", "prompt_hash": "d1fad3c3117ea46db8e0373270e563367558b95d208af0d877be1f6f4867c8b2", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 364, "doc": {"id": "9-206", "question_stem": "What would be more likely to attract a magnet?", "choices": {"text": ["a plastic zipper", "flowing water", "a car engine", "A wooden desk"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "What would be more likely to attract a magnet?", "arg_1": " a plastic zipper"}, "gen_args_1": {"arg_0": "What would be more likely to attract a magnet?", "arg_1": " flowing water"}, "gen_args_2": {"arg_0": "What would be more likely to attract a magnet?", "arg_1": " a car engine"}, "gen_args_3": {"arg_0": "What would be more likely to attract a magnet?", "arg_1": " A wooden desk"}}, "resps": [[["-31.25", "False"]], [["-17.0", "False"]], [["-21.625", "False"]], [["-23.75", "False"]]], "filtered_resps": [["-31.25", "False"], ["-17.0", "False"], ["-21.625", "False"], ["-23.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "532818b8eade16e32851151157d69216922becdd865055dff7c0111fcf5393f3", "prompt_hash": "eb712aaaa65594c760875ac32453ff15251a6d8c409e2c8a3145383c95c8301f", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 365, "doc": {"id": "7-740", "question_stem": "What constitutes a frog's diet?", "choices": {"text": ["it eats all plants", "it will eat dogs", "it only eats burgers", "it chomps on insects"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "What constitutes a frog's diet?", "arg_1": " it eats all plants"}, "gen_args_1": {"arg_0": "What constitutes a frog's diet?", "arg_1": " it will eat dogs"}, "gen_args_2": {"arg_0": "What constitutes a frog's diet?", "arg_1": " it only eats burgers"}, "gen_args_3": {"arg_0": "What constitutes a frog's diet?", "arg_1": " it chomps on insects"}}, "resps": [[["-38.0", "False"]], [["-42.75", "False"]], [["-38.75", "False"]], [["-31.375", "False"]]], "filtered_resps": [["-38.0", "False"], ["-42.75", "False"], ["-38.75", "False"], ["-31.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4314d24aa2b896de0db78be561e501012ff71ead3fa963fa5e8392d6bf393554", "prompt_hash": "d796ff1151551e0802c7524050b98f248a19b5946781a206c3f15c91ab52bcaa", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 366, "doc": {"id": "1774", "question_stem": "What is an example of the digestive system digesting food for the body?", "choices": {"text": ["a man eating nachos then getting food poisoning", "a baby drinking formula then needing a diaper change", "a cat eating food then throwing it up", "a horse licking a salt lick"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "What is an example of the digestive system digesting food for the body?", "arg_1": " a man eating nachos then getting food poisoning"}, "gen_args_1": {"arg_0": "What is an example of the digestive system digesting food for the body?", "arg_1": " a baby drinking formula then needing a diaper change"}, "gen_args_2": {"arg_0": "What is an example of the digestive system digesting food for the body?", "arg_1": " a cat eating food then throwing it up"}, "gen_args_3": {"arg_0": "What is an example of the digestive system digesting food for the body?", "arg_1": " a horse licking a salt lick"}}, "resps": [[["-64.5", "False"]], [["-65.5", "False"]], [["-50.5", "False"]], [["-49.0", "False"]]], "filtered_resps": [["-64.5", "False"], ["-65.5", "False"], ["-50.5", "False"], ["-49.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "beeae6ca45db7f4b8ec2dd6070dad9a3c702e33d121d5cb0a6b4d8eb13aaf3c0", "prompt_hash": "f3ba472e00de8eb4712d02d4ab876eddcc8ba5274e399fa2d728173212299087", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 367, "doc": {"id": "7-93", "question_stem": "The body is negatively impacted by", "choices": {"text": ["white blood cells", "vitamins", "rotavirus", "nasal decongestants"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "The body is negatively impacted by", "arg_1": " white blood cells"}, "gen_args_1": {"arg_0": "The body is negatively impacted by", "arg_1": " vitamins"}, "gen_args_2": {"arg_0": "The body is negatively impacted by", "arg_1": " rotavirus"}, "gen_args_3": {"arg_0": "The body is negatively impacted by", "arg_1": " nasal decongestants"}}, "resps": [[["-15.0625", "False"]], [["-10.4375", "False"]], [["-15.6875", "False"]], [["-17.125", "False"]]], "filtered_resps": [["-15.0625", "False"], ["-10.4375", "False"], ["-15.6875", "False"], ["-17.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "165dc951610e78313e677cce4657da2ac0e5e503fa6876014423d48717151575", "prompt_hash": "575bfc7d3be545a554de459641dff49902ffcecdb98775dd754b1c27371a05a7", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 368, "doc": {"id": "8-97", "question_stem": "Someone wants their electromagnets to work, but is having difficulty powering them. In order to make them work, they need to", "choices": {"text": ["run wire through currants", "run a continuous current", "run around the wire", "currently run wire through"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Someone wants their electromagnets to work, but is having difficulty powering them. In order to make them work, they need to", "arg_1": " run wire through currants"}, "gen_args_1": {"arg_0": "Someone wants their electromagnets to work, but is having difficulty powering them. In order to make them work, they need to", "arg_1": " run a continuous current"}, "gen_args_2": {"arg_0": "Someone wants their electromagnets to work, but is having difficulty powering them. In order to make them work, they need to", "arg_1": " run around the wire"}, "gen_args_3": {"arg_0": "Someone wants their electromagnets to work, but is having difficulty powering them. In order to make them work, they need to", "arg_1": " currently run wire through"}}, "resps": [[["-35.75", "False"]], [["-12.125", "False"]], [["-20.0", "False"]], [["-28.625", "False"]]], "filtered_resps": [["-35.75", "False"], ["-12.125", "False"], ["-20.0", "False"], ["-28.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4dc1799a7cd1d6de0bbb8ba2da7394440204fcaeb55b5ed2d98baf83b6f92e04", "prompt_hash": "c0d2cad88c3e5cd8eba0593b53193f5b8063a1bee08ae5b009489cb01f1c26ce", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 369, "doc": {"id": "9-813", "question_stem": "if a place has experienced flooding, what could be responsible?", "choices": {"text": ["all of these", "there has been excess condensed water vapor", "the water lacks oxygen", "the local deities are angry"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "if a place has experienced flooding, what could be responsible?", "arg_1": " all of these"}, "gen_args_1": {"arg_0": "if a place has experienced flooding, what could be responsible?", "arg_1": " there has been excess condensed water vapor"}, "gen_args_2": {"arg_0": "if a place has experienced flooding, what could be responsible?", "arg_1": " the water lacks oxygen"}, "gen_args_3": {"arg_0": "if a place has experienced flooding, what could be responsible?", "arg_1": " the local deities are angry"}}, "resps": [[["-16.375", "False"]], [["-49.75", "False"]], [["-28.25", "False"]], [["-40.0", "False"]]], "filtered_resps": [["-16.375", "False"], ["-49.75", "False"], ["-28.25", "False"], ["-40.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d57e0c0484f13b70c8f8d59cd853f7171d56ba56c00d7c32f0d3ac1ecdef792e", "prompt_hash": "c8183ee289f146af139025f29ae6b285ce9fa9002de585dc863ae4dd4adfebe6", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 370, "doc": {"id": "9-686", "question_stem": "What is an example of reproduction?", "choices": {"text": ["farming", "egg depositing", "flying", "walking"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "What is an example of reproduction?", "arg_1": " farming"}, "gen_args_1": {"arg_0": "What is an example of reproduction?", "arg_1": " egg depositing"}, "gen_args_2": {"arg_0": "What is an example of reproduction?", "arg_1": " flying"}, "gen_args_3": {"arg_0": "What is an example of reproduction?", "arg_1": " walking"}}, "resps": [[["-21.375", "False"]], [["-30.625", "False"]], [["-20.25", "False"]], [["-18.75", "False"]]], "filtered_resps": [["-21.375", "False"], ["-30.625", "False"], ["-20.25", "False"], ["-18.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "48809389f8274f840158cc68c7103cd1d2e5b74d38a9bc82b9dc0916e226b45a", "prompt_hash": "852f0c84471147323de6a5e19998340de9bc4a1269f342273c993fcc20cae7a1", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 371, "doc": {"id": "9-799", "question_stem": "A place that is snowy has a large amount of", "choices": {"text": ["wind", "storms", "frozen water", "rain"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A place that is snowy has a large amount of", "arg_1": " wind"}, "gen_args_1": {"arg_0": "A place that is snowy has a large amount of", "arg_1": " storms"}, "gen_args_2": {"arg_0": "A place that is snowy has a large amount of", "arg_1": " frozen water"}, "gen_args_3": {"arg_0": "A place that is snowy has a large amount of", "arg_1": " rain"}}, "resps": [[["-4.0625", "False"]], [["-11.25", "False"]], [["-5.5", "False"]], [["-6.5", "False"]]], "filtered_resps": [["-4.0625", "False"], ["-11.25", "False"], ["-5.5", "False"], ["-6.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "308f877972231a9f41b590e2a12c45e4c6a4635c7777264f36fe8781c739bcd0", "prompt_hash": "983a0a80bb58a89b54efe86761a4f9ec3c4385dad9203a331d4a44724ebbd06e", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 372, "doc": {"id": "1179", "question_stem": "An ideal abode for crickets is", "choices": {"text": ["a small potted plant in a house", "a green and lush tree and plant packed area", "a briny and warm body of water", "a area surrounded by spider webs"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "An ideal abode for crickets is", "arg_1": " a small potted plant in a house"}, "gen_args_1": {"arg_0": "An ideal abode for crickets is", "arg_1": " a green and lush tree and plant packed area"}, "gen_args_2": {"arg_0": "An ideal abode for crickets is", "arg_1": " a briny and warm body of water"}, "gen_args_3": {"arg_0": "An ideal abode for crickets is", "arg_1": " a area surrounded by spider webs"}}, "resps": [[["-28.625", "False"]], [["-48.0", "False"]], [["-29.75", "False"]], [["-27.625", "False"]]], "filtered_resps": [["-28.625", "False"], ["-48.0", "False"], ["-29.75", "False"], ["-27.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "02387252abd0c6f5e6bd56cd92ae845def7d5fdc6b79c3f967c8a2b747979ebc", "prompt_hash": "d086ab11a4012ce5988be8a4b0be7b237074cc4cc0d3269ce71b878240d80c0c", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 373, "doc": {"id": "1954", "question_stem": "Carbon steel is always what?", "choices": {"text": ["attractive to various objects that contain iron", "pleasant with a magnetic personality", "made up of iron and pieces of magnets", "hard as a magnetizing rod"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Carbon steel is always what?", "arg_1": " attractive to various objects that contain iron"}, "gen_args_1": {"arg_0": "Carbon steel is always what?", "arg_1": " pleasant with a magnetic personality"}, "gen_args_2": {"arg_0": "Carbon steel is always what?", "arg_1": " made up of iron and pieces of magnets"}, "gen_args_3": {"arg_0": "Carbon steel is always what?", "arg_1": " hard as a magnetizing rod"}}, "resps": [[["-51.5", "False"]], [["-46.25", "False"]], [["-55.0", "False"]], [["-44.75", "False"]]], "filtered_resps": [["-51.5", "False"], ["-46.25", "False"], ["-55.0", "False"], ["-44.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "afb14940cee6467d039665b320087ec181ac8a0cf9a50e9139aae30597cc9800", "prompt_hash": "75d2b3504ad5723fca8fe37484a99c021142ea41247e0aed66bad96a80bc4b21", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 374, "doc": {"id": "8-403", "question_stem": "if a bat delivers a live offspring, what does this tell us?", "choices": {"text": ["it is a mammal", "calling it a bird is wrong", "all of these", "it is capable of reproducing"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "if a bat delivers a live offspring, what does this tell us?", "arg_1": " it is a mammal"}, "gen_args_1": {"arg_0": "if a bat delivers a live offspring, what does this tell us?", "arg_1": " calling it a bird is wrong"}, "gen_args_2": {"arg_0": "if a bat delivers a live offspring, what does this tell us?", "arg_1": " all of these"}, "gen_args_3": {"arg_0": "if a bat delivers a live offspring, what does this tell us?", "arg_1": " it is capable of reproducing"}}, "resps": [[["-20.75", "False"]], [["-35.0", "False"]], [["-21.125", "False"]], [["-24.875", "False"]]], "filtered_resps": [["-20.75", "False"], ["-35.0", "False"], ["-21.125", "False"], ["-24.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "36cd1ccd248cc834465ca421b0b99ae2d45e456a862a187bb96d5da3352e37b1", "prompt_hash": "392f6aa6136ba91d48d2e3460d99136668d212b9e2977f72b1581bd4f7ad8d30", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 375, "doc": {"id": "9-576", "question_stem": "If you find something smooth and hard on the ground, it is probably made of what?", "choices": {"text": ["minerals", "mist", "clouds", "water"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "If you find something smooth and hard on the ground, it is probably made of what?", "arg_1": " minerals"}, "gen_args_1": {"arg_0": "If you find something smooth and hard on the ground, it is probably made of what?", "arg_1": " mist"}, "gen_args_2": {"arg_0": "If you find something smooth and hard on the ground, it is probably made of what?", "arg_1": " clouds"}, "gen_args_3": {"arg_0": "If you find something smooth and hard on the ground, it is probably made of what?", "arg_1": " water"}}, "resps": [[["-17.0", "False"]], [["-20.375", "False"]], [["-18.875", "False"]], [["-17.375", "False"]]], "filtered_resps": [["-17.0", "False"], ["-20.375", "False"], ["-18.875", "False"], ["-17.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "45c55c1d32c786cf3e25988a912bbc704acc591bf6f859dc645f9299a03d1a3b", "prompt_hash": "574038ed74249d75f7e2e36ed66d2b277e485a3a724d8a5bd1661a0cccb728e2", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 376, "doc": {"id": "9-866", "question_stem": "as you get closer to something it begins to", "choices": {"text": ["shrinks down to nothing", "grow in size visually", "show a large shadow", "rotate in a clockwise direction"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "as you get closer to something it begins to", "arg_1": " shrinks down to nothing"}, "gen_args_1": {"arg_0": "as you get closer to something it begins to", "arg_1": " grow in size visually"}, "gen_args_2": {"arg_0": "as you get closer to something it begins to", "arg_1": " show a large shadow"}, "gen_args_3": {"arg_0": "as you get closer to something it begins to", "arg_1": " rotate in a clockwise direction"}}, "resps": [[["-25.5", "False"]], [["-20.625", "False"]], [["-20.25", "False"]], [["-18.125", "False"]]], "filtered_resps": [["-25.5", "False"], ["-20.625", "False"], ["-20.25", "False"], ["-18.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a16820a9e99835d84f4b0c2126d86de73c099b92bef1bd2058df1a70cbb85d92", "prompt_hash": "777e1b8a89908299f7b795c968496ae3846e35344d70c228f5f96b91e490b603", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 377, "doc": {"id": "7-208", "question_stem": "After a storm", "choices": {"text": ["ponds may dry out", "flowers will wilt and wither", "creek beds may be spilling over", "drinking water will be in short supply"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "After a storm", "arg_1": " ponds may dry out"}, "gen_args_1": {"arg_0": "After a storm", "arg_1": " flowers will wilt and wither"}, "gen_args_2": {"arg_0": "After a storm", "arg_1": " creek beds may be spilling over"}, "gen_args_3": {"arg_0": "After a storm", "arg_1": " drinking water will be in short supply"}}, "resps": [[["-44.25", "False"]], [["-42.75", "False"]], [["-44.0", "False"]], [["-34.0", "False"]]], "filtered_resps": [["-44.25", "False"], ["-42.75", "False"], ["-44.0", "False"], ["-34.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "8a790d2867d3eb7da32a3a540c0ea1da9263ed61af9ad481b505d49f6d6cbff7", "prompt_hash": "c4e5fc37ec16b0b99105d27685d8958037eb2689094f9efe33257e9330b5aacc", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 378, "doc": {"id": "9-771", "question_stem": "What could I use as biofuel", "choices": {"text": ["Gold", "Car", "Diamonds", "Pine Needles"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "What could I use as biofuel", "arg_1": " Gold"}, "gen_args_1": {"arg_0": "What could I use as biofuel", "arg_1": " Car"}, "gen_args_2": {"arg_0": "What could I use as biofuel", "arg_1": " Diamonds"}, "gen_args_3": {"arg_0": "What could I use as biofuel", "arg_1": " Pine Needles"}}, "resps": [[["-23.75", "False"]], [["-19.5", "False"]], [["-25.75", "False"]], [["-26.375", "False"]]], "filtered_resps": [["-23.75", "False"], ["-19.5", "False"], ["-25.75", "False"], ["-26.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "ab8901f7448f462a0cb024be56f8ed644f170e0b28d25d2ac03220b1c9259414", "prompt_hash": "a4a60db7e05468d6eabf9a0bb63709be1d0638e84c9d17151afcce87a40cecd8", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 379, "doc": {"id": "998", "question_stem": "Which animal is hiding from a predator?", "choices": {"text": ["a tadpole losing its tail as it grows", "an angler fish using its Esca to lure another fish", "an octopus mimicking the color and texture of a rocky outcrop", "a great white shark breaching the water's surface"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Which animal is hiding from a predator?", "arg_1": " a tadpole losing its tail as it grows"}, "gen_args_1": {"arg_0": "Which animal is hiding from a predator?", "arg_1": " an angler fish using its Esca to lure another fish"}, "gen_args_2": {"arg_0": "Which animal is hiding from a predator?", "arg_1": " an octopus mimicking the color and texture of a rocky outcrop"}, "gen_args_3": {"arg_0": "Which animal is hiding from a predator?", "arg_1": " a great white shark breaching the water's surface"}}, "resps": [[["-62.0", "False"]], [["-82.0", "False"]], [["-53.75", "False"]], [["-49.0", "False"]]], "filtered_resps": [["-62.0", "False"], ["-82.0", "False"], ["-53.75", "False"], ["-49.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "3306aa8b0524224de21997d151822f55a60bdbaca208d8b9f4ae222582385800", "prompt_hash": "c81089eea64a5804e5930bf65a1a612b5e9b1b87490e0578296a768fa7574fcb", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 380, "doc": {"id": "433", "question_stem": "Which best demonstrates the concept of force causing an increase in speed?", "choices": {"text": ["skating on a rough surface", "a full bag swung in circles", "a computer powering on", "a baker stirring batter"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Which best demonstrates the concept of force causing an increase in speed?", "arg_1": " skating on a rough surface"}, "gen_args_1": {"arg_0": "Which best demonstrates the concept of force causing an increase in speed?", "arg_1": " a full bag swung in circles"}, "gen_args_2": {"arg_0": "Which best demonstrates the concept of force causing an increase in speed?", "arg_1": " a computer powering on"}, "gen_args_3": {"arg_0": "Which best demonstrates the concept of force causing an increase in speed?", "arg_1": " a baker stirring batter"}}, "resps": [[["-38.25", "False"]], [["-69.0", "False"]], [["-49.5", "False"]], [["-45.75", "False"]]], "filtered_resps": [["-38.25", "False"], ["-69.0", "False"], ["-49.5", "False"], ["-45.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "0afc2808fb98ce77a1d448ac0de79a3360648204c552c6cdf05b185ebf1cc7c3", "prompt_hash": "d6ceee0ed775d698d84c77e6310fc8889eddc7c81aeec57cf2ef42de1e4ddeb8", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 381, "doc": {"id": "9-508", "question_stem": "the night sky shows very far away what", "choices": {"text": ["clumps of flaming gas", "tidal waves washing over beaches", "aircraft falling towards collision", "party balloons tied to houses"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "the night sky shows very far away what", "arg_1": " clumps of flaming gas"}, "gen_args_1": {"arg_0": "the night sky shows very far away what", "arg_1": " tidal waves washing over beaches"}, "gen_args_2": {"arg_0": "the night sky shows very far away what", "arg_1": " aircraft falling towards collision"}, "gen_args_3": {"arg_0": "the night sky shows very far away what", "arg_1": " party balloons tied to houses"}}, "resps": [[["-34.25", "False"]], [["-32.5", "False"]], [["-42.75", "False"]], [["-41.0", "False"]]], "filtered_resps": [["-34.25", "False"], ["-32.5", "False"], ["-42.75", "False"], ["-41.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "606c43b2da84d118a71c2eb338c689028d36ede30fb2c22029fe2a351f3d9fec", "prompt_hash": "a468c17c881338b166a9400bbdaaf1d86971bdcbd3010abfde83ecfd92975764", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 382, "doc": {"id": "7-561", "question_stem": "Which would you likely find inside a beach ball?", "choices": {"text": ["cheese", "steam", "water", "air"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Which would you likely find inside a beach ball?", "arg_1": " cheese"}, "gen_args_1": {"arg_0": "Which would you likely find inside a beach ball?", "arg_1": " steam"}, "gen_args_2": {"arg_0": "Which would you likely find inside a beach ball?", "arg_1": " water"}, "gen_args_3": {"arg_0": "Which would you likely find inside a beach ball?", "arg_1": " air"}}, "resps": [[["-19.5", "False"]], [["-21.0", "False"]], [["-14.9375", "False"]], [["-15.25", "False"]]], "filtered_resps": [["-19.5", "False"], ["-21.0", "False"], ["-14.9375", "False"], ["-15.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "33b5c5885e483a7a8e5e112086ad6dd2a654dcbee5e631b68f0c77a71fb467c5", "prompt_hash": "6193f17968d2786e1ca6b3c442fe86f045f5525acc396fed77224ab4c574f7b1", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 383, "doc": {"id": "7-976", "question_stem": "Two fridge decorations when touched back to back", "choices": {"text": ["shove each other away", "are attracted to each other", "have very little reaction", "are reflective when together"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Two fridge decorations when touched back to back", "arg_1": " shove each other away"}, "gen_args_1": {"arg_0": "Two fridge decorations when touched back to back", "arg_1": " are attracted to each other"}, "gen_args_2": {"arg_0": "Two fridge decorations when touched back to back", "arg_1": " have very little reaction"}, "gen_args_3": {"arg_0": "Two fridge decorations when touched back to back", "arg_1": " are reflective when together"}}, "resps": [[["-22.75", "False"]], [["-10.0", "False"]], [["-19.875", "False"]], [["-28.75", "False"]]], "filtered_resps": [["-22.75", "False"], ["-10.0", "False"], ["-19.875", "False"], ["-28.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "97259b81361d975ccd31df1a564176f4d1a92b1bab37e6cda41c9f7027028055", "prompt_hash": "b39f81e3d174ad80d75b883c7e2d1a99e82e9289f91a39c5b60e7a0b8341230f", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 384, "doc": {"id": "1635", "question_stem": "Runoff happens because of", "choices": {"text": ["birds", "cattails", "people", "fish"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Runoff happens because of", "arg_1": " birds"}, "gen_args_1": {"arg_0": "Runoff happens because of", "arg_1": " cattails"}, "gen_args_2": {"arg_0": "Runoff happens because of", "arg_1": " people"}, "gen_args_3": {"arg_0": "Runoff happens because of", "arg_1": " fish"}}, "resps": [[["-12.75", "False"]], [["-17.625", "False"]], [["-11.9375", "False"]], [["-12.75", "False"]]], "filtered_resps": [["-12.75", "False"], ["-17.625", "False"], ["-11.9375", "False"], ["-12.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "32ced37898228adbfee44cf7388a581db443114c5bcca6b88f0c8ba8b2df440d", "prompt_hash": "8769fd2b2d6ba22b318d914040a929f1f66c51b873390eb2ba5be7b6f5a5269b", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 385, "doc": {"id": "7-875", "question_stem": "Desert environments are generally", "choices": {"text": ["sweltering", "arctic like", "lush", "frigid"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Desert environments are generally", "arg_1": " sweltering"}, "gen_args_1": {"arg_0": "Desert environments are generally", "arg_1": " arctic like"}, "gen_args_2": {"arg_0": "Desert environments are generally", "arg_1": " lush"}, "gen_args_3": {"arg_0": "Desert environments are generally", "arg_1": " frigid"}}, "resps": [[["-16.375", "False"]], [["-25.75", "False"]], [["-11.125", "False"]], [["-12.375", "False"]]], "filtered_resps": [["-16.375", "False"], ["-25.75", "False"], ["-11.125", "False"], ["-12.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e7e7cf3d6a65be036c29cd6604a9a6f09d64fa389e05d11da406a7e38d17bf8a", "prompt_hash": "f29a9327b8ab9afbfdf184741bfb252341b2e269d0d9d838f7e5b8d06cac04b8", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 386, "doc": {"id": "7-1053", "question_stem": "Are deserts characterized by high sunshine?", "choices": {"text": ["they get low sunlight", "deserts get surplus sun", "deserts get little sun", "deserts are always cloudy"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Are deserts characterized by high sunshine?", "arg_1": " they get low sunlight"}, "gen_args_1": {"arg_0": "Are deserts characterized by high sunshine?", "arg_1": " deserts get surplus sun"}, "gen_args_2": {"arg_0": "Are deserts characterized by high sunshine?", "arg_1": " deserts get little sun"}, "gen_args_3": {"arg_0": "Are deserts characterized by high sunshine?", "arg_1": " deserts are always cloudy"}}, "resps": [[["-37.0", "False"]], [["-36.5", "False"]], [["-33.25", "False"]], [["-26.375", "False"]]], "filtered_resps": [["-37.0", "False"], ["-36.5", "False"], ["-33.25", "False"], ["-26.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a2a85716e8c0fc3f8fb306db23ea798c8186a93ceef4e6a701dcffd6fb734f66", "prompt_hash": "91b56ad9cd884e1a2263dd7bf736385a5e86ca598a8e70c9fcaf585b4ff8f881", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 387, "doc": {"id": "9-957", "question_stem": "Water conservation could be a survival tactic in", "choices": {"text": ["The Appalachian Mountains", "New York City", "The Amazon", "The Gobi Desert"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Water conservation could be a survival tactic in", "arg_1": " The Appalachian Mountains"}, "gen_args_1": {"arg_0": "Water conservation could be a survival tactic in", "arg_1": " New York City"}, "gen_args_2": {"arg_0": "Water conservation could be a survival tactic in", "arg_1": " The Amazon"}, "gen_args_3": {"arg_0": "Water conservation could be a survival tactic in", "arg_1": " The Gobi Desert"}}, "resps": [[["-25.25", "False"]], [["-11.5", "False"]], [["-21.75", "False"]], [["-27.875", "False"]]], "filtered_resps": [["-25.25", "False"], ["-11.5", "False"], ["-21.75", "False"], ["-27.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "967c1f2e7a14bd4291a8eb67e8bf365511a23a2b23fea88295596ac2f5b98aae", "prompt_hash": "02907f3172c08c5a1a5bcd1e3fd2c1d4aa6afb6c0bd6950021ac4e764d4e1346", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 388, "doc": {"id": "1150", "question_stem": "Objects used to hold sheets of paper together are often", "choices": {"text": ["large", "wooden", "ferromagnetic", "electronic"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Objects used to hold sheets of paper together are often", "arg_1": " large"}, "gen_args_1": {"arg_0": "Objects used to hold sheets of paper together are often", "arg_1": " wooden"}, "gen_args_2": {"arg_0": "Objects used to hold sheets of paper together are often", "arg_1": " ferromagnetic"}, "gen_args_3": {"arg_0": "Objects used to hold sheets of paper together are often", "arg_1": " electronic"}}, "resps": [[["-7.125", "False"]], [["-6.96875", "False"]], [["-12.625", "False"]], [["-10.8125", "False"]]], "filtered_resps": [["-7.125", "False"], ["-6.96875", "False"], ["-12.625", "False"], ["-10.8125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a6fab29808b26c8f4655944e225dbac25b70fc1953abbeca0bd4b6070e14b800", "prompt_hash": "c54ffeea37d466a6f77179f8f263609dace82371fa39fb342563faa4d2fabf41", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 389, "doc": {"id": "8-240", "question_stem": "In order for plants and animals to grow, they need to consume food and water for", "choices": {"text": ["energy", "fun", "taste", "soil"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "In order for plants and animals to grow, they need to consume food and water for", "arg_1": " energy"}, "gen_args_1": {"arg_0": "In order for plants and animals to grow, they need to consume food and water for", "arg_1": " fun"}, "gen_args_2": {"arg_0": "In order for plants and animals to grow, they need to consume food and water for", "arg_1": " taste"}, "gen_args_3": {"arg_0": "In order for plants and animals to grow, they need to consume food and water for", "arg_1": " soil"}}, "resps": [[["-0.162109375", "True"]], [["-14.1875", "False"]], [["-21.125", "False"]], [["-17.5", "False"]]], "filtered_resps": [["-0.162109375", "True"], ["-14.1875", "False"], ["-21.125", "False"], ["-17.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e3b3e0397ba6ce56bb788c77c631118469a1f8d3985dc9477057c35caada9fb8", "prompt_hash": "d99cb6f233abe8617773f88447879881b61b7d069c45e7cc59db9177eecaae04", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 390, "doc": {"id": "9-554", "question_stem": "A bear cub learns to stay away from unknown bears because", "choices": {"text": ["they are much bigger than the cub", "the other bears look like its mother", "their mother teaches them to keep their distance", "the unknown bears look harmless"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A bear cub learns to stay away from unknown bears because", "arg_1": " they are much bigger than the cub"}, "gen_args_1": {"arg_0": "A bear cub learns to stay away from unknown bears because", "arg_1": " the other bears look like its mother"}, "gen_args_2": {"arg_0": "A bear cub learns to stay away from unknown bears because", "arg_1": " their mother teaches them to keep their distance"}, "gen_args_3": {"arg_0": "A bear cub learns to stay away from unknown bears because", "arg_1": " the unknown bears look harmless"}}, "resps": [[["-18.75", "False"]], [["-25.125", "False"]], [["-19.5", "False"]], [["-21.75", "False"]]], "filtered_resps": [["-18.75", "False"], ["-25.125", "False"], ["-19.5", "False"], ["-21.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "c940b7a1d868eefb5a403813edafbab6ec305a1572b9ef46ab2f22d571e56260", "prompt_hash": "538b8252d5a065472cecbe248755975350fa65bb9ea2695f3ee66bf7461f955b", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 391, "doc": {"id": "9-135", "question_stem": "a person driving to work in which of these is most likely to lose control?", "choices": {"text": ["a dry cobblestone road", "a sleet covered highway", "a dry paved road", "a dry gravel road"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "a person driving to work in which of these is most likely to lose control?", "arg_1": " a dry cobblestone road"}, "gen_args_1": {"arg_0": "a person driving to work in which of these is most likely to lose control?", "arg_1": " a sleet covered highway"}, "gen_args_2": {"arg_0": "a person driving to work in which of these is most likely to lose control?", "arg_1": " a dry paved road"}, "gen_args_3": {"arg_0": "a person driving to work in which of these is most likely to lose control?", "arg_1": " a dry gravel road"}}, "resps": [[["-33.0", "False"]], [["-40.5", "False"]], [["-31.0", "False"]], [["-31.5", "False"]]], "filtered_resps": [["-33.0", "False"], ["-40.5", "False"], ["-31.0", "False"], ["-31.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "673f601c68b2f0449ab505d6bf00443140b281d71b0351644d373e959bff811f", "prompt_hash": "6a01d80952f69f28336474c5da01a9857cfa6baf4822ef0f8205706fe4c79d04", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 392, "doc": {"id": "7-1096", "question_stem": "The only creature with offspring that is hatched, of these, is the", "choices": {"text": ["squirrel", "swallow", "mink", "bat"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "The only creature with offspring that is hatched, of these, is the", "arg_1": " squirrel"}, "gen_args_1": {"arg_0": "The only creature with offspring that is hatched, of these, is the", "arg_1": " swallow"}, "gen_args_2": {"arg_0": "The only creature with offspring that is hatched, of these, is the", "arg_1": " mink"}, "gen_args_3": {"arg_0": "The only creature with offspring that is hatched, of these, is the", "arg_1": " bat"}}, "resps": [[["-7.5", "False"]], [["-8.3125", "False"]], [["-9.0625", "False"]], [["-6.3125", "False"]]], "filtered_resps": [["-7.5", "False"], ["-8.3125", "False"], ["-9.0625", "False"], ["-6.3125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "3ec82d8f878c827e16e3e4249811fb8a346487d28c76f2abdb61530f4faab51c", "prompt_hash": "56acdfa7618dd9acf72253308522425cc54c01b28a1710e5119d31c418416d38", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 393, "doc": {"id": "841", "question_stem": "What element is prevalent in a plateau?", "choices": {"text": ["helium", "krypton", "silicon", "neon"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "What element is prevalent in a plateau?", "arg_1": " helium"}, "gen_args_1": {"arg_0": "What element is prevalent in a plateau?", "arg_1": " krypton"}, "gen_args_2": {"arg_0": "What element is prevalent in a plateau?", "arg_1": " silicon"}, "gen_args_3": {"arg_0": "What element is prevalent in a plateau?", "arg_1": " neon"}}, "resps": [[["-18.375", "False"]], [["-22.625", "False"]], [["-17.5", "False"]], [["-20.5", "False"]]], "filtered_resps": [["-18.375", "False"], ["-22.625", "False"], ["-17.5", "False"], ["-20.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "7cdf5ccf103dd7ff73820c376af55f9552a496117a59866135b18ec88d7380a8", "prompt_hash": "fc6e44e43e036f2da47cc8e3c3aee6ddadf2b88896981ed18ccd1bcf0ab271de", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 394, "doc": {"id": "7-146", "question_stem": "When a city tears down a park in a city, the park", "choices": {"text": ["is removed", "is renewed", "is retrieved", "is restored"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "When a city tears down a park in a city, the park", "arg_1": " is removed"}, "gen_args_1": {"arg_0": "When a city tears down a park in a city, the park", "arg_1": " is renewed"}, "gen_args_2": {"arg_0": "When a city tears down a park in a city, the park", "arg_1": " is retrieved"}, "gen_args_3": {"arg_0": "When a city tears down a park in a city, the park", "arg_1": " is restored"}}, "resps": [[["-6.34375", "False"]], [["-11.5", "False"]], [["-11.1875", "False"]], [["-9.6875", "False"]]], "filtered_resps": [["-6.34375", "False"], ["-11.5", "False"], ["-11.1875", "False"], ["-9.6875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "cf11530c4a6ed9da8cb2ce2db6e1718eb95ba1f3e4e08ca1eb1983f7e7046443", "prompt_hash": "c0d66b823d125013f0b093f3cc20fe74c54be99830bf74b638687466cdc25596", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 395, "doc": {"id": "1554", "question_stem": "Seeds", "choices": {"text": ["are useless shells that need to be discarded", "store extra bits of chlorophyll", "need to be mashed to grow", "aid in feeding what grows from them"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Seeds", "arg_1": " are useless shells that need to be discarded"}, "gen_args_1": {"arg_0": "Seeds", "arg_1": " store extra bits of chlorophyll"}, "gen_args_2": {"arg_0": "Seeds", "arg_1": " need to be mashed to grow"}, "gen_args_3": {"arg_0": "Seeds", "arg_1": " aid in feeding what grows from them"}}, "resps": [[["-35.75", "False"]], [["-46.25", "False"]], [["-32.75", "False"]], [["-49.25", "False"]]], "filtered_resps": [["-35.75", "False"], ["-46.25", "False"], ["-32.75", "False"], ["-49.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "ca0da45aae4e9dc250763a110a661e346aa1d1b52a34e542b74b636b1cf31a50", "prompt_hash": "45a195454fe20751e264b4423a6e45d6f61dbb3ec87f0907b5b38acf867fa463", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 396, "doc": {"id": "9-731", "question_stem": "If a grizzly bear eats a salmon, what is the grizzly bear demonstrating?", "choices": {"text": ["consumption", "cinematography", "direction", "production"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "If a grizzly bear eats a salmon, what is the grizzly bear demonstrating?", "arg_1": " consumption"}, "gen_args_1": {"arg_0": "If a grizzly bear eats a salmon, what is the grizzly bear demonstrating?", "arg_1": " cinematography"}, "gen_args_2": {"arg_0": "If a grizzly bear eats a salmon, what is the grizzly bear demonstrating?", "arg_1": " direction"}, "gen_args_3": {"arg_0": "If a grizzly bear eats a salmon, what is the grizzly bear demonstrating?", "arg_1": " production"}}, "resps": [[["-24.75", "False"]], [["-28.875", "False"]], [["-23.75", "False"]], [["-25.625", "False"]]], "filtered_resps": [["-24.75", "False"], ["-28.875", "False"], ["-23.75", "False"], ["-25.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f0f6dd001bed23bde37af72cccf1dcdc61c584a3ed14723deb4a92a2e673f75b", "prompt_hash": "f07f87fb3e681da95365c387c8250a966b70bbd7cd769b31c814fffbe8455f04", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 397, "doc": {"id": "1780", "question_stem": "The spring season brings", "choices": {"text": ["Bees", "Snow", "More Oxygen", "Dust"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "The spring season brings", "arg_1": " Bees"}, "gen_args_1": {"arg_0": "The spring season brings", "arg_1": " Snow"}, "gen_args_2": {"arg_0": "The spring season brings", "arg_1": " More Oxygen"}, "gen_args_3": {"arg_0": "The spring season brings", "arg_1": " Dust"}}, "resps": [[["-23.625", "False"]], [["-23.875", "False"]], [["-39.0", "False"]], [["-24.0", "False"]]], "filtered_resps": [["-23.625", "False"], ["-23.875", "False"], ["-39.0", "False"], ["-24.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d135bfc51ec05165e8c5f2b4cb88535bbf82a430c85226b8a63a384321298eb3", "prompt_hash": "0badd4008f3fe8020114a7a2b1ce94344c36ff65b08e3d107d7d03b10f8836ac", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 398, "doc": {"id": "7-1077", "question_stem": "Kinetic energy can be found in objects that move, such as", "choices": {"text": ["flower pots on a wagon", "cars that are in a lot", "kids that are sleeping soundly", "skateboards that are ridden all day"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Kinetic energy can be found in objects that move, such as", "arg_1": " flower pots on a wagon"}, "gen_args_1": {"arg_0": "Kinetic energy can be found in objects that move, such as", "arg_1": " cars that are in a lot"}, "gen_args_2": {"arg_0": "Kinetic energy can be found in objects that move, such as", "arg_1": " kids that are sleeping soundly"}, "gen_args_3": {"arg_0": "Kinetic energy can be found in objects that move, such as", "arg_1": " skateboards that are ridden all day"}}, "resps": [[["-36.5", "False"]], [["-30.25", "False"]], [["-47.0", "False"]], [["-38.5", "False"]]], "filtered_resps": [["-36.5", "False"], ["-30.25", "False"], ["-47.0", "False"], ["-38.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "6d12b4c004219f1be6c6e1ffabb4a80eb688f6baa3ffd82584047201f679b803", "prompt_hash": "2a105b049f3cf0332aa32796fba682f64a629f5d72fbc2ea14ae055ae63231c4", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 399, "doc": {"id": "8-494", "question_stem": "which of these is the quickest to go visiting from our world?", "choices": {"text": ["none of these", "a trip to mars", "a trip to the moon", "a trip to the northern star"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "which of these is the quickest to go visiting from our world?", "arg_1": " none of these"}, "gen_args_1": {"arg_0": "which of these is the quickest to go visiting from our world?", "arg_1": " a trip to mars"}, "gen_args_2": {"arg_0": "which of these is the quickest to go visiting from our world?", "arg_1": " a trip to the moon"}, "gen_args_3": {"arg_0": "which of these is the quickest to go visiting from our world?", "arg_1": " a trip to the northern star"}}, "resps": [[["-15.9375", "False"]], [["-21.125", "False"]], [["-18.25", "False"]], [["-29.5", "False"]]], "filtered_resps": [["-15.9375", "False"], ["-21.125", "False"], ["-18.25", "False"], ["-29.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e388a32784f80633a5ffe0be946cb428f89c29b89096d78aa83e4ffe3ad9a324", "prompt_hash": "60b1710c797d388b92fd5f1a08bf161f2d989cf55b53299c90582c2581d6f904", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 400, "doc": {"id": "936", "question_stem": "Animals died after the removal of a", "choices": {"text": ["bush", "street", "house", "city"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Animals died after the removal of a", "arg_1": " bush"}, "gen_args_1": {"arg_0": "Animals died after the removal of a", "arg_1": " street"}, "gen_args_2": {"arg_0": "Animals died after the removal of a", "arg_1": " house"}, "gen_args_3": {"arg_0": "Animals died after the removal of a", "arg_1": " city"}}, "resps": [[["-7.625", "False"]], [["-8.5625", "False"]], [["-7.71875", "False"]], [["-11.3125", "False"]]], "filtered_resps": [["-7.625", "False"], ["-8.5625", "False"], ["-7.71875", "False"], ["-11.3125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "c1d1226bb16c76b100a889aa8e11d48912d16ece026052b15d09a69649482210", "prompt_hash": "ede131be6e67be77b9f97dba8a941bd1bed30255defd03df2a3ecada77c94106", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 401, "doc": {"id": "8-478", "question_stem": "If I want to go running at night, what can I use as a reflector?", "choices": {"text": ["A black shirt", "Kitchen foil", "Sunglasses", "A megaphone"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "If I want to go running at night, what can I use as a reflector?", "arg_1": " A black shirt"}, "gen_args_1": {"arg_0": "If I want to go running at night, what can I use as a reflector?", "arg_1": " Kitchen foil"}, "gen_args_2": {"arg_0": "If I want to go running at night, what can I use as a reflector?", "arg_1": " Sunglasses"}, "gen_args_3": {"arg_0": "If I want to go running at night, what can I use as a reflector?", "arg_1": " A megaphone"}}, "resps": [[["-16.25", "False"]], [["-25.625", "False"]], [["-14.5", "False"]], [["-22.5", "False"]]], "filtered_resps": [["-16.25", "False"], ["-25.625", "False"], ["-14.5", "False"], ["-22.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "99284baf6bac41bdfa65b365d55f12fdc60152871ad9a7a0679694d0500fcede", "prompt_hash": "10b0a03daf4bd0d18a5dea2914c31687407ee7dc9aad123d8f479b5a3e3a38a5", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 402, "doc": {"id": "9-669", "question_stem": "the closest star to our planet delivers solar energy to the planet", "choices": {"text": ["maybe", "all of these", "this is sure", "this is uncertain"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "the closest star to our planet delivers solar energy to the planet", "arg_1": " maybe"}, "gen_args_1": {"arg_0": "the closest star to our planet delivers solar energy to the planet", "arg_1": " all of these"}, "gen_args_2": {"arg_0": "the closest star to our planet delivers solar energy to the planet", "arg_1": " this is sure"}, "gen_args_3": {"arg_0": "the closest star to our planet delivers solar energy to the planet", "arg_1": " this is uncertain"}}, "resps": [[["-11.75", "False"]], [["-14.5", "False"]], [["-22.0", "False"]], [["-23.25", "False"]]], "filtered_resps": [["-11.75", "False"], ["-14.5", "False"], ["-22.0", "False"], ["-23.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "bf85e1dbd4aba45d89708affcb8d2df1178d54092b4d12f1312e459caf2e9da2", "prompt_hash": "dfe7a51d7d25922d0098afdc3419430e54491b110344bd35eeb2621625cb5811", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 403, "doc": {"id": "7-732", "question_stem": "Coal-fire power stations heat coal to incredible temps in order to", "choices": {"text": ["produce energy", "use heat energy", "burn energy", "fuel the world"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Coal-fire power stations heat coal to incredible temps in order to", "arg_1": " produce energy"}, "gen_args_1": {"arg_0": "Coal-fire power stations heat coal to incredible temps in order to", "arg_1": " use heat energy"}, "gen_args_2": {"arg_0": "Coal-fire power stations heat coal to incredible temps in order to", "arg_1": " burn energy"}, "gen_args_3": {"arg_0": "Coal-fire power stations heat coal to incredible temps in order to", "arg_1": " fuel the world"}}, "resps": [[["-6.9375", "False"]], [["-17.25", "False"]], [["-14.125", "False"]], [["-13.6875", "False"]]], "filtered_resps": [["-6.9375", "False"], ["-17.25", "False"], ["-14.125", "False"], ["-13.6875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "20ff1dab6bb279904c36eaaa3901df70806ec17a0385dc343227b0a0abbdfa5f", "prompt_hash": "3429d1fac11433083fbb8e33b00615e67a26a2ecc1ca79210076c295e2a65be9", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 404, "doc": {"id": "7-658", "question_stem": "Creatures sometimes have barbs on their backs that they use to sting, all of these do, outside of the", "choices": {"text": ["wasp", "bee", "scorpion", "butterfly"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Creatures sometimes have barbs on their backs that they use to sting, all of these do, outside of the", "arg_1": " wasp"}, "gen_args_1": {"arg_0": "Creatures sometimes have barbs on their backs that they use to sting, all of these do, outside of the", "arg_1": " bee"}, "gen_args_2": {"arg_0": "Creatures sometimes have barbs on their backs that they use to sting, all of these do, outside of the", "arg_1": " scorpion"}, "gen_args_3": {"arg_0": "Creatures sometimes have barbs on their backs that they use to sting, all of these do, outside of the", "arg_1": " butterfly"}}, "resps": [[["-9.1875", "False"]], [["-9.5625", "False"]], [["-6.96875", "False"]], [["-9.1875", "False"]]], "filtered_resps": [["-9.1875", "False"], ["-9.5625", "False"], ["-6.96875", "False"], ["-9.1875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "ffea10313d56e605a494a0aee2fa8a984d1def73711aa24c7f345e13272642e3", "prompt_hash": "3ffcce63bdf0f0881c8ed0c27715eead23860bf8aeea116648920ce187be88f3", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 405, "doc": {"id": "1003", "question_stem": "A renewable resource is", "choices": {"text": ["fossil fuel", "turbine produced electricity", "copper", "coal lumps"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "A renewable resource is", "arg_1": " fossil fuel"}, "gen_args_1": {"arg_0": "A renewable resource is", "arg_1": " turbine produced electricity"}, "gen_args_2": {"arg_0": "A renewable resource is", "arg_1": " copper"}, "gen_args_3": {"arg_0": "A renewable resource is", "arg_1": " coal lumps"}}, "resps": [[["-14.25", "False"]], [["-33.75", "False"]], [["-13.0625", "False"]], [["-24.0", "False"]]], "filtered_resps": [["-14.25", "False"], ["-33.75", "False"], ["-13.0625", "False"], ["-24.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "fff59a882683b10fff21e3fe7d297928433ff3a0ad4e27f97b035d8887b1b84e", "prompt_hash": "d79e7917394a0287d1c2c271c29e108bf538249261820a8df2109e047d5f7b7e", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 406, "doc": {"id": "8-62", "question_stem": "In a hypothetical world, black bears decrease in numbers until there are zero black bears left on this world. The black bear species", "choices": {"text": ["would cease existing", "would be troubled", "would be thriving", "would be endangered"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "In a hypothetical world, black bears decrease in numbers until there are zero black bears left on this world. The black bear species", "arg_1": " would cease existing"}, "gen_args_1": {"arg_0": "In a hypothetical world, black bears decrease in numbers until there are zero black bears left on this world. The black bear species", "arg_1": " would be troubled"}, "gen_args_2": {"arg_0": "In a hypothetical world, black bears decrease in numbers until there are zero black bears left on this world. The black bear species", "arg_1": " would be thriving"}, "gen_args_3": {"arg_0": "In a hypothetical world, black bears decrease in numbers until there are zero black bears left on this world. The black bear species", "arg_1": " would be endangered"}}, "resps": [[["-19.25", "False"]], [["-25.25", "False"]], [["-15.375", "False"]], [["-15.75", "False"]]], "filtered_resps": [["-19.25", "False"], ["-25.25", "False"], ["-15.375", "False"], ["-15.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "6d8e5dd998e7b17339cfbbb3538c2956aac770c8161305d57473e2c91891050b", "prompt_hash": "9d28f51f90f57b32360d26c49a7704e6c0e84e3ce310766151e88f4d1c3c108e", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 407, "doc": {"id": "7-386", "question_stem": "Acid can be used to make a new", "choices": {"text": ["light", "substance", "electricity", "sound"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Acid can be used to make a new", "arg_1": " light"}, "gen_args_1": {"arg_0": "Acid can be used to make a new", "arg_1": " substance"}, "gen_args_2": {"arg_0": "Acid can be used to make a new", "arg_1": " electricity"}, "gen_args_3": {"arg_0": "Acid can be used to make a new", "arg_1": " sound"}}, "resps": [[["-7.375", "False"]], [["-1.9453125", "False"]], [["-11.1875", "False"]], [["-8.25", "False"]]], "filtered_resps": [["-7.375", "False"], ["-1.9453125", "False"], ["-11.1875", "False"], ["-8.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e5c4e948a78755d435b2787bc7d42677d03632f0034e5e7f542d575049da8c4e", "prompt_hash": "b73b1bb1ee9f3d8e2ebb897215b770ab33f733a664149a85ed2df9e02751ad32", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 408, "doc": {"id": "257", "question_stem": "Global warming is lowering the world's amount of", "choices": {"text": ["hurricanes", "ocean levels", "carbon dioxide", "ice"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Global warming is lowering the world's amount of", "arg_1": " hurricanes"}, "gen_args_1": {"arg_0": "Global warming is lowering the world's amount of", "arg_1": " ocean levels"}, "gen_args_2": {"arg_0": "Global warming is lowering the world's amount of", "arg_1": " carbon dioxide"}, "gen_args_3": {"arg_0": "Global warming is lowering the world's amount of", "arg_1": " ice"}}, "resps": [[["-13.0625", "False"]], [["-12.625", "False"]], [["-7.03125", "False"]], [["-0.7890625", "True"]]], "filtered_resps": [["-13.0625", "False"], ["-12.625", "False"], ["-7.03125", "False"], ["-0.7890625", "True"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "1bd966e1b63c273c6a81a1b52755961e289410b47cef52cac9ec0f75bcde3d9b", "prompt_hash": "dc983fe576b28207376acc0e61e4f0e7e4706c94109c535ac67b2d9774093e9c", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 409, "doc": {"id": "147", "question_stem": "Echolocation can't detect an object's", "choices": {"text": ["distance", "shape", "size", "temperature"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Echolocation can't detect an object's", "arg_1": " distance"}, "gen_args_1": {"arg_0": "Echolocation can't detect an object's", "arg_1": " shape"}, "gen_args_2": {"arg_0": "Echolocation can't detect an object's", "arg_1": " size"}, "gen_args_3": {"arg_0": "Echolocation can't detect an object's", "arg_1": " temperature"}}, "resps": [[["-4.625", "False"]], [["-1.4921875", "False"]], [["-1.4296875", "True"]], [["-6.6875", "False"]]], "filtered_resps": [["-4.625", "False"], ["-1.4921875", "False"], ["-1.4296875", "True"], ["-6.6875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "990ecfebe785a1252d3d346c5d7f66ed11b84ad2f0f804d0889a329376f0c47e", "prompt_hash": "a06440eb8cffeeef2d60e17b9b1bd2d7b84561365ad76fbb3d7e34327120f3fe", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 410, "doc": {"id": "7-599", "question_stem": "What material has already broken down?", "choices": {"text": ["wood", "glass", "boulders", "sand"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "What material has already broken down?", "arg_1": " wood"}, "gen_args_1": {"arg_0": "What material has already broken down?", "arg_1": " glass"}, "gen_args_2": {"arg_0": "What material has already broken down?", "arg_1": " boulders"}, "gen_args_3": {"arg_0": "What material has already broken down?", "arg_1": " sand"}}, "resps": [[["-13.25", "False"]], [["-14.25", "False"]], [["-21.875", "False"]], [["-15.25", "False"]]], "filtered_resps": [["-13.25", "False"], ["-14.25", "False"], ["-21.875", "False"], ["-15.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "47eee7094f6ee44e0566dc88defb78ec58e588ba6d4d1b6f08c32dcb7805a790", "prompt_hash": "8a80a19a798a93f0780adc61e44f412b44fca1025c11138431d2ede3524a7ea9", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 411, "doc": {"id": "8-92", "question_stem": "Which beverage would dissolve solids the best?", "choices": {"text": ["A glass of ice-cold water", "A boiling hot mug of tea", "A cup of warm milk", "A room temperature glass of water"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Which beverage would dissolve solids the best?", "arg_1": " A glass of ice-cold water"}, "gen_args_1": {"arg_0": "Which beverage would dissolve solids the best?", "arg_1": " A boiling hot mug of tea"}, "gen_args_2": {"arg_0": "Which beverage would dissolve solids the best?", "arg_1": " A cup of warm milk"}, "gen_args_3": {"arg_0": "Which beverage would dissolve solids the best?", "arg_1": " A room temperature glass of water"}}, "resps": [[["-25.0", "False"]], [["-33.75", "False"]], [["-21.25", "False"]], [["-27.375", "False"]]], "filtered_resps": [["-25.0", "False"], ["-33.75", "False"], ["-21.25", "False"], ["-27.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "11b2e3b88693eb7e173807232b91e84558428f49aeb6625e896a35d4c0c8a2c1", "prompt_hash": "b51fe9f7f2959a6a0600c4b0a61f4218ca38a99e9d1d8c5cc1e57017d47cb7e5", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 412, "doc": {"id": "354", "question_stem": "Which animal has live births?", "choices": {"text": ["poodle", "hummingbird", "crocodile", "trout"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Which animal has live births?", "arg_1": " poodle"}, "gen_args_1": {"arg_0": "Which animal has live births?", "arg_1": " hummingbird"}, "gen_args_2": {"arg_0": "Which animal has live births?", "arg_1": " crocodile"}, "gen_args_3": {"arg_0": "Which animal has live births?", "arg_1": " trout"}}, "resps": [[["-19.0", "False"]], [["-17.875", "False"]], [["-17.625", "False"]], [["-17.0", "False"]]], "filtered_resps": [["-19.0", "False"], ["-17.875", "False"], ["-17.625", "False"], ["-17.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "78ad7bd42c8a2792d604a4faef36edb6d7bcd90b2cd7892f3c4108de63fcf2a0", "prompt_hash": "7c040e6fdbcecb37eda55b29e11f59083c06693d0f93c32e2ad3be6f7dd9ae2a", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 413, "doc": {"id": "9-966", "question_stem": "Quartz crystals are made up of", "choices": {"text": ["majic", "hexagons", "octogons", "water"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Quartz crystals are made up of", "arg_1": " majic"}, "gen_args_1": {"arg_0": "Quartz crystals are made up of", "arg_1": " hexagons"}, "gen_args_2": {"arg_0": "Quartz crystals are made up of", "arg_1": " octogons"}, "gen_args_3": {"arg_0": "Quartz crystals are made up of", "arg_1": " water"}}, "resps": [[["-18.375", "False"]], [["-7.46875", "False"]], [["-30.875", "False"]], [["-7.84375", "False"]]], "filtered_resps": [["-18.375", "False"], ["-7.46875", "False"], ["-30.875", "False"], ["-7.84375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "1e25899c893ea269636e9c871835f209d280f28d5e8a4ae76873d95ffec3a8ca", "prompt_hash": "2b0ec3156968fdf7a2df18d4d426c425768981a88031207aa56c725aeff252ae", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 414, "doc": {"id": "9-612", "question_stem": "cellular respiration is when energy is produced in a cell by consumption of", "choices": {"text": ["water", "nutrients", "mitochondria", "gas"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "cellular respiration is when energy is produced in a cell by consumption of", "arg_1": " water"}, "gen_args_1": {"arg_0": "cellular respiration is when energy is produced in a cell by consumption of", "arg_1": " nutrients"}, "gen_args_2": {"arg_0": "cellular respiration is when energy is produced in a cell by consumption of", "arg_1": " mitochondria"}, "gen_args_3": {"arg_0": "cellular respiration is when energy is produced in a cell by consumption of", "arg_1": " gas"}}, "resps": [[["-10.0", "False"]], [["-3.640625", "False"]], [["-12.3125", "False"]], [["-13.5625", "False"]]], "filtered_resps": [["-10.0", "False"], ["-3.640625", "False"], ["-12.3125", "False"], ["-13.5625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "97234fd3efbada893f99e9df90edd43bf33ff5c4a3fbd388333b3fbc71c6e010", "prompt_hash": "c4a82ca7177664b03df9dc588db4387ca4c56a07f5061e6830ce81de9a265093", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 415, "doc": {"id": "9-548", "question_stem": "Did pasteurization get invented by Thomas Edison?", "choices": {"text": ["negative", "positive", "all of these", "maybe it was"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Did pasteurization get invented by Thomas Edison?", "arg_1": " negative"}, "gen_args_1": {"arg_0": "Did pasteurization get invented by Thomas Edison?", "arg_1": " positive"}, "gen_args_2": {"arg_0": "Did pasteurization get invented by Thomas Edison?", "arg_1": " all of these"}, "gen_args_3": {"arg_0": "Did pasteurization get invented by Thomas Edison?", "arg_1": " maybe it was"}}, "resps": [[["-18.875", "False"]], [["-22.75", "False"]], [["-22.25", "False"]], [["-21.125", "False"]]], "filtered_resps": [["-18.875", "False"], ["-22.75", "False"], ["-22.25", "False"], ["-21.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4b33dfba0d684b0b47a4d90ad966c2027c0dd75f1422c29f84010c2be81fa7a3", "prompt_hash": "9781071ebc3ad222fef4df092cc20d9c00da60492d0ea289f40ffa3600d88520", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 416, "doc": {"id": "9-429", "question_stem": "A glass of water can undergo a chemical change by adding", "choices": {"text": ["a cup of salt", "a cup of dirt", "a cup of water", "a cup of ice"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "A glass of water can undergo a chemical change by adding", "arg_1": " a cup of salt"}, "gen_args_1": {"arg_0": "A glass of water can undergo a chemical change by adding", "arg_1": " a cup of dirt"}, "gen_args_2": {"arg_0": "A glass of water can undergo a chemical change by adding", "arg_1": " a cup of water"}, "gen_args_3": {"arg_0": "A glass of water can undergo a chemical change by adding", "arg_1": " a cup of ice"}}, "resps": [[["-10.5", "False"]], [["-15.6875", "False"]], [["-16.25", "False"]], [["-11.875", "False"]]], "filtered_resps": [["-10.5", "False"], ["-15.6875", "False"], ["-16.25", "False"], ["-11.875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "dc8c3141c7cfd4d5b33da9ca708a931ad45006fc079572bb43a2a3bcbf1114cf", "prompt_hash": "4363ff916d6dfd0124676df9c04c7edc2816377c59a5839bec6a075aa6022f29", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 417, "doc": {"id": "7-95", "question_stem": "Water levels may decrease on cloudless days because", "choices": {"text": ["water is warmer than the air", "air is warmer than water", "moisture is pulled upwards", "moisture always tries to rise"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Water levels may decrease on cloudless days because", "arg_1": " water is warmer than the air"}, "gen_args_1": {"arg_0": "Water levels may decrease on cloudless days because", "arg_1": " air is warmer than water"}, "gen_args_2": {"arg_0": "Water levels may decrease on cloudless days because", "arg_1": " moisture is pulled upwards"}, "gen_args_3": {"arg_0": "Water levels may decrease on cloudless days because", "arg_1": " moisture always tries to rise"}}, "resps": [[["-19.875", "False"]], [["-14.0625", "False"]], [["-16.5", "False"]], [["-27.0", "False"]]], "filtered_resps": [["-19.875", "False"], ["-14.0625", "False"], ["-16.5", "False"], ["-27.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a2c37ec1e08367e65e74dc571b4f36374d74a7345a590d5162b5f095f87048de", "prompt_hash": "4d6ce7469ae842024c3b6a0ae68355c972c64c011c6b0c09189b2270b2667c45", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 418, "doc": {"id": "1560", "question_stem": "To change an object's shape", "choices": {"text": ["rip off a corner portion", "lay it flat on a table", "color the edges of it", "add a piece of tape to it"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "To change an object's shape", "arg_1": " rip off a corner portion"}, "gen_args_1": {"arg_0": "To change an object's shape", "arg_1": " lay it flat on a table"}, "gen_args_2": {"arg_0": "To change an object's shape", "arg_1": " color the edges of it"}, "gen_args_3": {"arg_0": "To change an object's shape", "arg_1": " add a piece of tape to it"}}, "resps": [[["-45.75", "False"]], [["-25.625", "False"]], [["-36.5", "False"]], [["-31.375", "False"]]], "filtered_resps": [["-45.75", "False"], ["-25.625", "False"], ["-36.5", "False"], ["-31.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4e45c7db742e15795695d05c3df459cb871e1c1a1256b5e26c4920b78356fd53", "prompt_hash": "8688d3eb89df5289caeed4ad8e15ffbcae84e1ea8197849d0fcd0f372987351a", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 419, "doc": {"id": "9-461", "question_stem": "Steve was driving on the highway when he rear-ended another car because he didn't see it until he was just a foot away. This could have happened because of", "choices": {"text": ["reports of tornadoes in the area", "a dog running across the highway behind Steve's car", "a sudden fog moving into the area", "ice forming on the road"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Steve was driving on the highway when he rear-ended another car because he didn't see it until he was just a foot away. This could have happened because of", "arg_1": " reports of tornadoes in the area"}, "gen_args_1": {"arg_0": "Steve was driving on the highway when he rear-ended another car because he didn't see it until he was just a foot away. This could have happened because of", "arg_1": " a dog running across the highway behind Steve's car"}, "gen_args_2": {"arg_0": "Steve was driving on the highway when he rear-ended another car because he didn't see it until he was just a foot away. This could have happened because of", "arg_1": " a sudden fog moving into the area"}, "gen_args_3": {"arg_0": "Steve was driving on the highway when he rear-ended another car because he didn't see it until he was just a foot away. This could have happened because of", "arg_1": " ice forming on the road"}}, "resps": [[["-32.0", "False"]], [["-38.0", "False"]], [["-29.0", "False"]], [["-22.25", "False"]]], "filtered_resps": [["-32.0", "False"], ["-38.0", "False"], ["-29.0", "False"], ["-22.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "32981cd1e6218d3ae4beee6f930a6f3d1dee52b77fd9b4bbc6ed29a652fef787", "prompt_hash": "1e4519e9e987c6c3f2a11cec17554b3f97d85cf9fca9208d471f7f4240a951a5", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 420, "doc": {"id": "9-490", "question_stem": "DNA is a vehicle for passing", "choices": {"text": ["clothes types", "school grades", "elbow size", "language and dialect"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "DNA is a vehicle for passing", "arg_1": " clothes types"}, "gen_args_1": {"arg_0": "DNA is a vehicle for passing", "arg_1": " school grades"}, "gen_args_2": {"arg_0": "DNA is a vehicle for passing", "arg_1": " elbow size"}, "gen_args_3": {"arg_0": "DNA is a vehicle for passing", "arg_1": " language and dialect"}}, "resps": [[["-26.5", "False"]], [["-22.0", "False"]], [["-28.75", "False"]], [["-25.25", "False"]]], "filtered_resps": [["-26.5", "False"], ["-22.0", "False"], ["-28.75", "False"], ["-25.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d9485e6d46662ca17ac1a16c271e1c2383ec9cb4aad2b31c3cee94bbb1b5c0fe", "prompt_hash": "a2859c41a716b05c8731e84af7ea28cce5875028c34c6aed4c5e81d4d7cb6569", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 421, "doc": {"id": "9-301", "question_stem": "A beach ball goes from flat to round once you put what inside of it?", "choices": {"text": ["food", "sunlight", "gas", "salt"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A beach ball goes from flat to round once you put what inside of it?", "arg_1": " food"}, "gen_args_1": {"arg_0": "A beach ball goes from flat to round once you put what inside of it?", "arg_1": " sunlight"}, "gen_args_2": {"arg_0": "A beach ball goes from flat to round once you put what inside of it?", "arg_1": " gas"}, "gen_args_3": {"arg_0": "A beach ball goes from flat to round once you put what inside of it?", "arg_1": " salt"}}, "resps": [[["-21.0", "False"]], [["-18.75", "False"]], [["-18.0", "False"]], [["-20.5", "False"]]], "filtered_resps": [["-21.0", "False"], ["-18.75", "False"], ["-18.0", "False"], ["-20.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "58c9740a1a2d2328afec203b3785a048a77092635d74740e03448d7853018bf2", "prompt_hash": "eda3054632689925236481b5239ee217b560347e7c00e27c67efd270f2c675a5", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 422, "doc": {"id": "60", "question_stem": "In general, how many times per month is there a full moon?", "choices": {"text": ["twice", "three times", "once", "four times"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "In general, how many times per month is there a full moon?", "arg_1": " twice"}, "gen_args_1": {"arg_0": "In general, how many times per month is there a full moon?", "arg_1": " three times"}, "gen_args_2": {"arg_0": "In general, how many times per month is there a full moon?", "arg_1": " once"}, "gen_args_3": {"arg_0": "In general, how many times per month is there a full moon?", "arg_1": " four times"}}, "resps": [[["-17.5", "False"]], [["-18.25", "False"]], [["-16.5", "False"]], [["-18.625", "False"]]], "filtered_resps": [["-17.5", "False"], ["-18.25", "False"], ["-16.5", "False"], ["-18.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "c84b8937d386ca51812c9c730980863bdb780a8f77d8dbfe573f9f409d0f4098", "prompt_hash": "432efee10fa59c0d8e20b200bf27dc5bcacfd249aa5a1033e847944d06c68c93", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 423, "doc": {"id": "9-894", "question_stem": "What kind of object does light bounce off of?", "choices": {"text": ["tadpole", "any object", "item that reflects", "black hole"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "What kind of object does light bounce off of?", "arg_1": " tadpole"}, "gen_args_1": {"arg_0": "What kind of object does light bounce off of?", "arg_1": " any object"}, "gen_args_2": {"arg_0": "What kind of object does light bounce off of?", "arg_1": " item that reflects"}, "gen_args_3": {"arg_0": "What kind of object does light bounce off of?", "arg_1": " black hole"}}, "resps": [[["-29.875", "False"]], [["-17.625", "False"]], [["-30.375", "False"]], [["-23.375", "False"]]], "filtered_resps": [["-29.875", "False"], ["-17.625", "False"], ["-30.375", "False"], ["-23.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "0352a3c75a95943b3c342fff8b3828fca20a4bf90df4e5038843a6fae173491d", "prompt_hash": "b78604e1ed0f895648f68655a8c3c70774bc57a02f2440fd6ac32393de45b8e8", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 424, "doc": {"id": "9-895", "question_stem": "The amount of brush in a park has been decreasing. What could be a cause?", "choices": {"text": ["the season has been quite dry", "There has been a lot of rain", "snakes shelter under the brush", "People have been walking by the brush on the trails"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "The amount of brush in a park has been decreasing. What could be a cause?", "arg_1": " the season has been quite dry"}, "gen_args_1": {"arg_0": "The amount of brush in a park has been decreasing. What could be a cause?", "arg_1": " There has been a lot of rain"}, "gen_args_2": {"arg_0": "The amount of brush in a park has been decreasing. What could be a cause?", "arg_1": " snakes shelter under the brush"}, "gen_args_3": {"arg_0": "The amount of brush in a park has been decreasing. What could be a cause?", "arg_1": " People have been walking by the brush on the trails"}}, "resps": [[["-35.0", "False"]], [["-19.875", "False"]], [["-42.25", "False"]], [["-46.75", "False"]]], "filtered_resps": [["-35.0", "False"], ["-19.875", "False"], ["-42.25", "False"], ["-46.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2457b330213e7fd6373a834816d3b00535347fad9c11deed3be0850911cd8487", "prompt_hash": "cf3e241385be7b8b7011aac477e4eb694a2cdc59f98f795982571248b446200e", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 425, "doc": {"id": "9-281", "question_stem": "if a pot on the stove is described as hot, what does this mean?", "choices": {"text": ["the body of the pot is of high temperature", "the body of the pot is cold", "all of these", "the body of the pot is wet"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "if a pot on the stove is described as hot, what does this mean?", "arg_1": " the body of the pot is of high temperature"}, "gen_args_1": {"arg_0": "if a pot on the stove is described as hot, what does this mean?", "arg_1": " the body of the pot is cold"}, "gen_args_2": {"arg_0": "if a pot on the stove is described as hot, what does this mean?", "arg_1": " all of these"}, "gen_args_3": {"arg_0": "if a pot on the stove is described as hot, what does this mean?", "arg_1": " the body of the pot is wet"}}, "resps": [[["-34.25", "False"]], [["-32.0", "False"]], [["-22.625", "False"]], [["-31.25", "False"]]], "filtered_resps": [["-34.25", "False"], ["-32.0", "False"], ["-22.625", "False"], ["-31.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "9ef1c80d50e232eaa60f2a2cc8c4850dc09908a7f1d659d8ac2977e285952a44", "prompt_hash": "c801645e0045c0e5ab1f4fd13a170b06b233dfbd184c668909d3043a23a0dd06", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 426, "doc": {"id": "202", "question_stem": "What animal is more difficult for predators to see in water?", "choices": {"text": ["a fish", "a duck", "an octopus", "a crab"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "What animal is more difficult for predators to see in water?", "arg_1": " a fish"}, "gen_args_1": {"arg_0": "What animal is more difficult for predators to see in water?", "arg_1": " a duck"}, "gen_args_2": {"arg_0": "What animal is more difficult for predators to see in water?", "arg_1": " an octopus"}, "gen_args_3": {"arg_0": "What animal is more difficult for predators to see in water?", "arg_1": " a crab"}}, "resps": [[["-23.125", "False"]], [["-26.75", "False"]], [["-14.625", "False"]], [["-25.5", "False"]]], "filtered_resps": [["-23.125", "False"], ["-26.75", "False"], ["-14.625", "False"], ["-25.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4f63bf45f86f9194959629367d87b129de8248ea979c5c28aec140a034357c26", "prompt_hash": "402206c918989c926698c27e93cba12186b1020edf6a2bb6f5982198a58516d8", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 427, "doc": {"id": "1937", "question_stem": "A wedge requires", "choices": {"text": ["electrical energy", "chemical energy", "mechanical energy", "heat energy"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A wedge requires", "arg_1": " electrical energy"}, "gen_args_1": {"arg_0": "A wedge requires", "arg_1": " chemical energy"}, "gen_args_2": {"arg_0": "A wedge requires", "arg_1": " mechanical energy"}, "gen_args_3": {"arg_0": "A wedge requires", "arg_1": " heat energy"}}, "resps": [[["-16.75", "False"]], [["-19.875", "False"]], [["-16.0", "False"]], [["-17.625", "False"]]], "filtered_resps": [["-16.75", "False"], ["-19.875", "False"], ["-16.0", "False"], ["-17.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2b9653038bb6e0511c3f2185989b7a1a3c9e9844461e8637c938a2b545acf428", "prompt_hash": "2e301f16efa951ba118aa09c148051044f6ea03ed1868654bb0f4fc86fb32abb", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 428, "doc": {"id": "620", "question_stem": "In solid phase matter has a/an", "choices": {"text": ["concrete configuration", "ambiguous form", "shapeless form", "radioactive glow"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "In solid phase matter has a/an", "arg_1": " concrete configuration"}, "gen_args_1": {"arg_0": "In solid phase matter has a/an", "arg_1": " ambiguous form"}, "gen_args_2": {"arg_0": "In solid phase matter has a/an", "arg_1": " shapeless form"}, "gen_args_3": {"arg_0": "In solid phase matter has a/an", "arg_1": " radioactive glow"}}, "resps": [[["-24.5", "False"]], [["-18.875", "False"]], [["-14.875", "False"]], [["-22.75", "False"]]], "filtered_resps": [["-24.5", "False"], ["-18.875", "False"], ["-14.875", "False"], ["-22.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "7fc272fff86f45f0921229743f71aac755f9e2ac436497e5ebde1fa814b03c9b", "prompt_hash": "5d0100db0d684887812647b1430e128095b9f44757446e12270cc37eb720e69c", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 429, "doc": {"id": "8-142", "question_stem": "A school trip is going to study the coral reef for a class. They want to see how strong coral is, and what species of life live in and around it. Therefore, the class", "choices": {"text": ["takes a trip to the desert", "climbs a tall mountain", "travels to the seaside", "visits a remote jungle"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A school trip is going to study the coral reef for a class. They want to see how strong coral is, and what species of life live in and around it. Therefore, the class", "arg_1": " takes a trip to the desert"}, "gen_args_1": {"arg_0": "A school trip is going to study the coral reef for a class. They want to see how strong coral is, and what species of life live in and around it. Therefore, the class", "arg_1": " climbs a tall mountain"}, "gen_args_2": {"arg_0": "A school trip is going to study the coral reef for a class. They want to see how strong coral is, and what species of life live in and around it. Therefore, the class", "arg_1": " travels to the seaside"}, "gen_args_3": {"arg_0": "A school trip is going to study the coral reef for a class. They want to see how strong coral is, and what species of life live in and around it. Therefore, the class", "arg_1": " visits a remote jungle"}}, "resps": [[["-22.375", "False"]], [["-27.75", "False"]], [["-20.125", "False"]], [["-27.25", "False"]]], "filtered_resps": [["-22.375", "False"], ["-27.75", "False"], ["-20.125", "False"], ["-27.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "b75353c79a38aa9d5cacb5f1a27132c38003bdc6cc70cf6c9b32c922230323c4", "prompt_hash": "9bbdd7bc8bb2059dff077e6e3ba9d1ddfd1a80ac20c8cfbe9b0d0ea46409c40b", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 430, "doc": {"id": "7-1138", "question_stem": "When ice buildup is on a sidewalk, the ice may be reduced by", "choices": {"text": ["adding salt", "adding litter", "adding sand", "adding water"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "When ice buildup is on a sidewalk, the ice may be reduced by", "arg_1": " adding salt"}, "gen_args_1": {"arg_0": "When ice buildup is on a sidewalk, the ice may be reduced by", "arg_1": " adding litter"}, "gen_args_2": {"arg_0": "When ice buildup is on a sidewalk, the ice may be reduced by", "arg_1": " adding sand"}, "gen_args_3": {"arg_0": "When ice buildup is on a sidewalk, the ice may be reduced by", "arg_1": " adding water"}}, "resps": [[["-5.03125", "False"]], [["-13.25", "False"]], [["-6.75", "False"]], [["-3.71875", "False"]]], "filtered_resps": [["-5.03125", "False"], ["-13.25", "False"], ["-6.75", "False"], ["-3.71875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "fd3089ea8126924b937fe808ac78602a2bad79880287fe97a73cb4c3998d6469", "prompt_hash": "4808b2ca29af71a6c8d8171f472faf4043163e3e048600977039d2beac78a9ac", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 431, "doc": {"id": "8-471", "question_stem": "The appropriate place to put this item is the recycling bin", "choices": {"text": ["used motor oil", "used soda can", "used Styrofoam plates", "left over medicine"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "The appropriate place to put this item is the recycling bin", "arg_1": " used motor oil"}, "gen_args_1": {"arg_0": "The appropriate place to put this item is the recycling bin", "arg_1": " used soda can"}, "gen_args_2": {"arg_0": "The appropriate place to put this item is the recycling bin", "arg_1": " used Styrofoam plates"}, "gen_args_3": {"arg_0": "The appropriate place to put this item is the recycling bin", "arg_1": " left over medicine"}}, "resps": [[["-35.25", "False"]], [["-36.5", "False"]], [["-39.5", "False"]], [["-37.5", "False"]]], "filtered_resps": [["-35.25", "False"], ["-36.5", "False"], ["-39.5", "False"], ["-37.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "6218defc7b1dfffa5b147e8dff9ebe8ab773dcdea6bbb505131f7993d784fca9", "prompt_hash": "37f5efa9bc9aaae19628ffdfa7f487375163b0fd9bebf6bee3e02e2882c5db83", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 432, "doc": {"id": "9-433", "question_stem": "How many times would someone change the page of a calendar in a year?", "choices": {"text": ["13", "12", "15", "14"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "How many times would someone change the page of a calendar in a year?", "arg_1": " 13"}, "gen_args_1": {"arg_0": "How many times would someone change the page of a calendar in a year?", "arg_1": " 12"}, "gen_args_2": {"arg_0": "How many times would someone change the page of a calendar in a year?", "arg_1": " 15"}, "gen_args_3": {"arg_0": "How many times would someone change the page of a calendar in a year?", "arg_1": " 14"}}, "resps": [[["-20.25", "False"]], [["-18.75", "False"]], [["-20.75", "False"]], [["-20.25", "False"]]], "filtered_resps": [["-20.25", "False"], ["-18.75", "False"], ["-20.75", "False"], ["-20.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "46bc38c56f710def4fec52114d8c4984c9664a2c54bb02392c9c3a17b7509d5f", "prompt_hash": "61fe28ba94a2c8b169e6062c1746eed326911062e4c89ed363cbd8832abe2bbd", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 433, "doc": {"id": "1458", "question_stem": "A car has the least speed if it", "choices": {"text": ["is heavy", "is large", "is turned off", "is small"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A car has the least speed if it", "arg_1": " is heavy"}, "gen_args_1": {"arg_0": "A car has the least speed if it", "arg_1": " is large"}, "gen_args_2": {"arg_0": "A car has the least speed if it", "arg_1": " is turned off"}, "gen_args_3": {"arg_0": "A car has the least speed if it", "arg_1": " is small"}}, "resps": [[["-8.9375", "False"]], [["-9.4375", "False"]], [["-7.6875", "False"]], [["-7.0625", "False"]]], "filtered_resps": [["-8.9375", "False"], ["-9.4375", "False"], ["-7.6875", "False"], ["-7.0625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "5fb92b807afed85f98138c07b34a9cbc68511d0e7a0378badb3a5cba4b98e997", "prompt_hash": "937020811c6b993d67cdf60fe0ad47d17df195a2a8196a7a8e9eb159991c2f39", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 434, "doc": {"id": "57", "question_stem": "Which musical instrument is the same type as a guitar?", "choices": {"text": ["flute", "cello", "drum", "trumpet"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Which musical instrument is the same type as a guitar?", "arg_1": " flute"}, "gen_args_1": {"arg_0": "Which musical instrument is the same type as a guitar?", "arg_1": " cello"}, "gen_args_2": {"arg_0": "Which musical instrument is the same type as a guitar?", "arg_1": " drum"}, "gen_args_3": {"arg_0": "Which musical instrument is the same type as a guitar?", "arg_1": " trumpet"}}, "resps": [[["-11.5625", "False"]], [["-11.0625", "False"]], [["-12.3125", "False"]], [["-12.0625", "False"]]], "filtered_resps": [["-11.5625", "False"], ["-11.0625", "False"], ["-12.3125", "False"], ["-12.0625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "3736cf025511bc1e87f603253263c44cf7a77eb60701ff5646214136decdeefd", "prompt_hash": "c7f91eef938c54d541ee8748be51e14a94630a4a8960a30e2ee51c790314dd93", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 435, "doc": {"id": "605", "question_stem": "The balance result will be number of", "choices": {"text": ["kilowatts", "kilobytes", "kilograms", "kilometers"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "The balance result will be number of", "arg_1": " kilowatts"}, "gen_args_1": {"arg_0": "The balance result will be number of", "arg_1": " kilobytes"}, "gen_args_2": {"arg_0": "The balance result will be number of", "arg_1": " kilograms"}, "gen_args_3": {"arg_0": "The balance result will be number of", "arg_1": " kilometers"}}, "resps": [[["-12.5625", "False"]], [["-13.625", "False"]], [["-10.1875", "False"]], [["-10.6875", "False"]]], "filtered_resps": [["-12.5625", "False"], ["-13.625", "False"], ["-10.1875", "False"], ["-10.6875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "b3d671a326a87ae3155bdee68bf1bf9db28e3a4a11ef53710795dce989c8312f", "prompt_hash": "bdd03f547edb0f308683a543d8f632a463f67b6ad966ba81c125781119059508", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 436, "doc": {"id": "9-889", "question_stem": "Which of these is a place where a human might live?", "choices": {"text": ["igloo", "cloud", "Mars", "the Moon"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Which of these is a place where a human might live?", "arg_1": " igloo"}, "gen_args_1": {"arg_0": "Which of these is a place where a human might live?", "arg_1": " cloud"}, "gen_args_2": {"arg_0": "Which of these is a place where a human might live?", "arg_1": " Mars"}, "gen_args_3": {"arg_0": "Which of these is a place where a human might live?", "arg_1": " the Moon"}}, "resps": [[["-24.0", "False"]], [["-23.25", "False"]], [["-19.25", "False"]], [["-24.75", "False"]]], "filtered_resps": [["-24.0", "False"], ["-23.25", "False"], ["-19.25", "False"], ["-24.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "70c54eb9b6ab077ee998046bc82643bc2796e67a28e4c89aeff841e65202c53e", "prompt_hash": "bc039395a8ce8a359a3883240386ac2e9c426074c9293fc80ab94ca3973fe559", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 437, "doc": {"id": "1890", "question_stem": "Camouflage is when an organism does what?", "choices": {"text": ["reconfigure appearance to blend in", "hides its young to avoid prey", "changes its shape to appear larger", "buries itself to disappear momentarily"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Camouflage is when an organism does what?", "arg_1": " reconfigure appearance to blend in"}, "gen_args_1": {"arg_0": "Camouflage is when an organism does what?", "arg_1": " hides its young to avoid prey"}, "gen_args_2": {"arg_0": "Camouflage is when an organism does what?", "arg_1": " changes its shape to appear larger"}, "gen_args_3": {"arg_0": "Camouflage is when an organism does what?", "arg_1": " buries itself to disappear momentarily"}}, "resps": [[["-39.0", "False"]], [["-41.25", "False"]], [["-36.25", "False"]], [["-40.75", "False"]]], "filtered_resps": [["-39.0", "False"], ["-41.25", "False"], ["-36.25", "False"], ["-40.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "fab7abaf69fc418504f92ee9add031c37f5645121b04fb571bff0d1322269126", "prompt_hash": "7ce5fcdb45214ee3bfd15ef1e6e7782a7ad0774a91d6f2b32c57f678428169b2", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 438, "doc": {"id": "9-618", "question_stem": "Which of these is required for a plant to enjoy the product of a rain storm?", "choices": {"text": ["xylem", "luck", "magic", "dirt"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Which of these is required for a plant to enjoy the product of a rain storm?", "arg_1": " xylem"}, "gen_args_1": {"arg_0": "Which of these is required for a plant to enjoy the product of a rain storm?", "arg_1": " luck"}, "gen_args_2": {"arg_0": "Which of these is required for a plant to enjoy the product of a rain storm?", "arg_1": " magic"}, "gen_args_3": {"arg_0": "Which of these is required for a plant to enjoy the product of a rain storm?", "arg_1": " dirt"}}, "resps": [[["-17.75", "False"]], [["-21.5", "False"]], [["-19.875", "False"]], [["-18.625", "False"]]], "filtered_resps": [["-17.75", "False"], ["-21.5", "False"], ["-19.875", "False"], ["-18.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "630048582efa1a66d9abc0d006fcdc97afd3d5b0b5d7acaa067f00dc2515ecec", "prompt_hash": "e0e838b3a8de5c7a772ccf856e11e033a9e291167ab074263937cdcbb290aa36", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 439, "doc": {"id": "9-523", "question_stem": "How does a microscope make small things appear?", "choices": {"text": ["humongous", "transparent", "discolored", "distorted"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "How does a microscope make small things appear?", "arg_1": " humongous"}, "gen_args_1": {"arg_0": "How does a microscope make small things appear?", "arg_1": " transparent"}, "gen_args_2": {"arg_0": "How does a microscope make small things appear?", "arg_1": " discolored"}, "gen_args_3": {"arg_0": "How does a microscope make small things appear?", "arg_1": " distorted"}}, "resps": [[["-26.0", "False"]], [["-18.5", "False"]], [["-34.25", "False"]], [["-18.625", "False"]]], "filtered_resps": [["-26.0", "False"], ["-18.5", "False"], ["-34.25", "False"], ["-18.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f456855d233ce0fac8380eb99c9f0cd5d87029a0ddfab092c1c7e1ead809ae70", "prompt_hash": "5b08237c3887337524c17a45fc697894c3965e2250c49abc2b6590df67c89704", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 440, "doc": {"id": "1126", "question_stem": "Water can turn to vapor", "choices": {"text": ["when a pot of water is placed on an off stove burner", "when placing water in a freezer", "when boiling eggs on a stove top", "when placed in a room temperature setting"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Water can turn to vapor", "arg_1": " when a pot of water is placed on an off stove burner"}, "gen_args_1": {"arg_0": "Water can turn to vapor", "arg_1": " when placing water in a freezer"}, "gen_args_2": {"arg_0": "Water can turn to vapor", "arg_1": " when boiling eggs on a stove top"}, "gen_args_3": {"arg_0": "Water can turn to vapor", "arg_1": " when placed in a room temperature setting"}}, "resps": [[["-50.75", "False"]], [["-28.25", "False"]], [["-42.0", "False"]], [["-31.375", "False"]]], "filtered_resps": [["-50.75", "False"], ["-28.25", "False"], ["-42.0", "False"], ["-31.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "bcf4826f4367a766ae452c9aaa9014d91a4f0168730dd5724789373e40ef8e23", "prompt_hash": "c06609f1aa951cab1c04ad5ccb2a4d089a46a68ce83fc398bcdb1d3fa9526f55", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 441, "doc": {"id": "644", "question_stem": "An incandescent bulb's filament produces similar light as an LED bulb, but more", "choices": {"text": ["white light", "conversion", "heat", "sound"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "An incandescent bulb's filament produces similar light as an LED bulb, but more", "arg_1": " white light"}, "gen_args_1": {"arg_0": "An incandescent bulb's filament produces similar light as an LED bulb, but more", "arg_1": " conversion"}, "gen_args_2": {"arg_0": "An incandescent bulb's filament produces similar light as an LED bulb, but more", "arg_1": " heat"}, "gen_args_3": {"arg_0": "An incandescent bulb's filament produces similar light as an LED bulb, but more", "arg_1": " sound"}}, "resps": [[["-10.8125", "False"]], [["-15.1875", "False"]], [["-3.9375", "False"]], [["-15.4375", "False"]]], "filtered_resps": [["-10.8125", "False"], ["-15.1875", "False"], ["-3.9375", "False"], ["-15.4375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "fa787ceb906ad192d3e41ce6bd1b573c0a3310a8dcf596218f76d3f4371bc927", "prompt_hash": "7533cfcb7f52f5c0b8b1ebf506e1ccd1e9faa8b488f1ff2e9346ace5b688b357", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 442, "doc": {"id": "8-365", "question_stem": "A boy at school is waiting desperately for the school day to be over so that he can go home and play video games. He watches the time count down on the clock at the head of the class, counting the", "choices": {"text": ["seconds", "days", "weeks", "years"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "A boy at school is waiting desperately for the school day to be over so that he can go home and play video games. He watches the time count down on the clock at the head of the class, counting the", "arg_1": " seconds"}, "gen_args_1": {"arg_0": "A boy at school is waiting desperately for the school day to be over so that he can go home and play video games. He watches the time count down on the clock at the head of the class, counting the", "arg_1": " days"}, "gen_args_2": {"arg_0": "A boy at school is waiting desperately for the school day to be over so that he can go home and play video games. He watches the time count down on the clock at the head of the class, counting the", "arg_1": " weeks"}, "gen_args_3": {"arg_0": "A boy at school is waiting desperately for the school day to be over so that he can go home and play video games. He watches the time count down on the clock at the head of the class, counting the", "arg_1": " years"}}, "resps": [[["-0.2294921875", "True"]], [["-10.75", "False"]], [["-13.3125", "False"]], [["-12.1875", "False"]]], "filtered_resps": [["-0.2294921875", "True"], ["-10.75", "False"], ["-13.3125", "False"], ["-12.1875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "16dc20d33bedd5f9dc1a60137683b66f5ec8c25559d0644113ade7364e171a2e", "prompt_hash": "04125fcd1335b9262dec7559cd257e449039acf808987108fa178d7a62b6e297", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 443, "doc": {"id": "9-727", "question_stem": "Camouflage can be used by animals for hunting", "choices": {"text": ["water", "trees", "air", "meals"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Camouflage can be used by animals for hunting", "arg_1": " water"}, "gen_args_1": {"arg_0": "Camouflage can be used by animals for hunting", "arg_1": " trees"}, "gen_args_2": {"arg_0": "Camouflage can be used by animals for hunting", "arg_1": " air"}, "gen_args_3": {"arg_0": "Camouflage can be used by animals for hunting", "arg_1": " meals"}}, "resps": [[["-15.25", "False"]], [["-15.0", "False"]], [["-17.625", "False"]], [["-13.25", "False"]]], "filtered_resps": [["-15.25", "False"], ["-15.0", "False"], ["-17.625", "False"], ["-13.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "5b32ab274017cf32df39fbac4a8a414cd1886221b80c1224a5f16ab9d74292f3", "prompt_hash": "578305b94174cba2913923efa9ae21a40bc55adad1ded1b27ed51e0e4db45237", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 444, "doc": {"id": "7-461", "question_stem": "Carbohydrates are made of sugar, which means that a diabetic would need to exhibit care in consuming", "choices": {"text": ["broccoli", "meat", "celery", "toast"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Carbohydrates are made of sugar, which means that a diabetic would need to exhibit care in consuming", "arg_1": " broccoli"}, "gen_args_1": {"arg_0": "Carbohydrates are made of sugar, which means that a diabetic would need to exhibit care in consuming", "arg_1": " meat"}, "gen_args_2": {"arg_0": "Carbohydrates are made of sugar, which means that a diabetic would need to exhibit care in consuming", "arg_1": " celery"}, "gen_args_3": {"arg_0": "Carbohydrates are made of sugar, which means that a diabetic would need to exhibit care in consuming", "arg_1": " toast"}}, "resps": [[["-13.4375", "False"]], [["-12.5625", "False"]], [["-16.375", "False"]], [["-14.1875", "False"]]], "filtered_resps": [["-13.4375", "False"], ["-12.5625", "False"], ["-16.375", "False"], ["-14.1875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4bdc210f1d4f69655c6ff4ffe2a924c618938912c6fa0b2813242c309f6e186d", "prompt_hash": "315c400304e1b0f9029cfd4c9376aa0199923f51a454c40dd59de4f0691eb2e7", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 445, "doc": {"id": "9-1071", "question_stem": "Hand dryers can also be used to", "choices": {"text": ["keep cold drinks cool", "dry out clothes after coming in from the rain", "hydrate your face and hands", "make a damp rag damper"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Hand dryers can also be used to", "arg_1": " keep cold drinks cool"}, "gen_args_1": {"arg_0": "Hand dryers can also be used to", "arg_1": " dry out clothes after coming in from the rain"}, "gen_args_2": {"arg_0": "Hand dryers can also be used to", "arg_1": " hydrate your face and hands"}, "gen_args_3": {"arg_0": "Hand dryers can also be used to", "arg_1": " make a damp rag damper"}}, "resps": [[["-24.0", "False"]], [["-30.625", "False"]], [["-21.5", "False"]], [["-35.0", "False"]]], "filtered_resps": [["-24.0", "False"], ["-30.625", "False"], ["-21.5", "False"], ["-35.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "7458918bc7b93e931e432cd04ad6e88cbb8f8d6ef85c3dadd0dfcad2d5ffb750", "prompt_hash": "47ee3858b37529744ebd1a3ff95a7a59e836ff5cfcda86160eed4f5b41838e49", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 446, "doc": {"id": "1918", "question_stem": "Polar bears require", "choices": {"text": ["a tropical environment", "a frigid environment", "a tepid environment", "a warm environment"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Polar bears require", "arg_1": " a tropical environment"}, "gen_args_1": {"arg_0": "Polar bears require", "arg_1": " a frigid environment"}, "gen_args_2": {"arg_0": "Polar bears require", "arg_1": " a tepid environment"}, "gen_args_3": {"arg_0": "Polar bears require", "arg_1": " a warm environment"}}, "resps": [[["-13.5", "False"]], [["-9.5625", "False"]], [["-22.0", "False"]], [["-8.75", "False"]]], "filtered_resps": [["-13.5", "False"], ["-9.5625", "False"], ["-22.0", "False"], ["-8.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "1705f5b5d047aefd4d22a1ba7b0e8c24e71b40b5c0b16e0f1cd0fe6ebd279412", "prompt_hash": "e7b9dfe8fdbf0b783ee8ac3b5fa455a2cd63bf6f032201c4178a8524f0d0bfa2", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 447, "doc": {"id": "1038", "question_stem": "A measurement of time that is less than a minute is a", "choices": {"text": ["day", "minute", "hour", "second"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "A measurement of time that is less than a minute is a", "arg_1": " day"}, "gen_args_1": {"arg_0": "A measurement of time that is less than a minute is a", "arg_1": " minute"}, "gen_args_2": {"arg_0": "A measurement of time that is less than a minute is a", "arg_1": " hour"}, "gen_args_3": {"arg_0": "A measurement of time that is less than a minute is a", "arg_1": " second"}}, "resps": [[["-7.25", "False"]], [["-1.8984375", "False"]], [["-11.125", "False"]], [["-5.96875", "False"]]], "filtered_resps": [["-7.25", "False"], ["-1.8984375", "False"], ["-11.125", "False"], ["-5.96875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "227221c4c589976933e57127c7106b9b3eefd8d921932cc1d317af782594a18c", "prompt_hash": "03f94eaddc9a5a28030395fa91b929e5310be079666acdf3c0dbf48122a508b2", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 448, "doc": {"id": "9-197", "question_stem": "What will be more available in an area when rainfall increases?", "choices": {"text": ["fire", "air", "dirt", "H2O"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "What will be more available in an area when rainfall increases?", "arg_1": " fire"}, "gen_args_1": {"arg_0": "What will be more available in an area when rainfall increases?", "arg_1": " air"}, "gen_args_2": {"arg_0": "What will be more available in an area when rainfall increases?", "arg_1": " dirt"}, "gen_args_3": {"arg_0": "What will be more available in an area when rainfall increases?", "arg_1": " H2O"}}, "resps": [[["-25.0", "False"]], [["-25.5", "False"]], [["-26.0", "False"]], [["-18.375", "False"]]], "filtered_resps": [["-25.0", "False"], ["-25.5", "False"], ["-26.0", "False"], ["-18.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4b88cbffeea7d9f6999d16fb1c6f2052f984d168b1eba7f7dbaf1b864554110c", "prompt_hash": "f09325f2803f6eb6a218d32bc5753e8a102b0dd654f34555d2a632f1ae8b8b7c", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 449, "doc": {"id": "1393", "question_stem": "When does the first quarter phase of the moon occur?", "choices": {"text": ["when you cannot see the moon in the sky at night", "after the first phase of the lunar month", "after a blue moon", "during the full moon"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "When does the first quarter phase of the moon occur?", "arg_1": " when you cannot see the moon in the sky at night"}, "gen_args_1": {"arg_0": "When does the first quarter phase of the moon occur?", "arg_1": " after the first phase of the lunar month"}, "gen_args_2": {"arg_0": "When does the first quarter phase of the moon occur?", "arg_1": " after a blue moon"}, "gen_args_3": {"arg_0": "When does the first quarter phase of the moon occur?", "arg_1": " during the full moon"}}, "resps": [[["-36.5", "False"]], [["-33.25", "False"]], [["-26.625", "False"]], [["-15.9375", "False"]]], "filtered_resps": [["-36.5", "False"], ["-33.25", "False"], ["-26.625", "False"], ["-15.9375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "25ec23cc6d617dd4df935509c7ed00b566b7752639bb7127c2e8f0ed3e5e5c82", "prompt_hash": "244b73ceae4b3fc30f8e82c37e2e759203798224dfebb5080ade38e0a37dc88e", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 450, "doc": {"id": "7-244", "question_stem": "An ice cube placed in sunlight will", "choices": {"text": ["shrink", "change color", "grow", "freeze"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "An ice cube placed in sunlight will", "arg_1": " shrink"}, "gen_args_1": {"arg_0": "An ice cube placed in sunlight will", "arg_1": " change color"}, "gen_args_2": {"arg_0": "An ice cube placed in sunlight will", "arg_1": " grow"}, "gen_args_3": {"arg_0": "An ice cube placed in sunlight will", "arg_1": " freeze"}}, "resps": [[["-5.4375", "False"]], [["-6.4375", "False"]], [["-9.3125", "False"]], [["-7.4375", "False"]]], "filtered_resps": [["-5.4375", "False"], ["-6.4375", "False"], ["-9.3125", "False"], ["-7.4375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "5c980db69961c2ccdec772407811718859bd9e9f9ce504cbff39569bea182750", "prompt_hash": "207697fc6a15dc189282232761aab8bb93137d3c9262c366a1f5fde1bcb46f77", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 451, "doc": {"id": "9-916", "question_stem": "If a person loses his job and is low on money, he will have to start cutting back on how much food he consumes or he'd run out, otherwise known as", "choices": {"text": ["destroying", "conserving", "losing", "squandering"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "If a person loses his job and is low on money, he will have to start cutting back on how much food he consumes or he'd run out, otherwise known as", "arg_1": " destroying"}, "gen_args_1": {"arg_0": "If a person loses his job and is low on money, he will have to start cutting back on how much food he consumes or he'd run out, otherwise known as", "arg_1": " conserving"}, "gen_args_2": {"arg_0": "If a person loses his job and is low on money, he will have to start cutting back on how much food he consumes or he'd run out, otherwise known as", "arg_1": " losing"}, "gen_args_3": {"arg_0": "If a person loses his job and is low on money, he will have to start cutting back on how much food he consumes or he'd run out, otherwise known as", "arg_1": " squandering"}}, "resps": [[["-14.0", "False"]], [["-10.625", "False"]], [["-9.0625", "False"]], [["-11.5", "False"]]], "filtered_resps": [["-14.0", "False"], ["-10.625", "False"], ["-9.0625", "False"], ["-11.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d9052a770b7303f6d0bf269778dabccc0d4da48104258d9fb744237b2c5a0e28", "prompt_hash": "1e8cb64f4e108dc260f2e1c0d9d28bfd4c53cbe056a0eb8f2c6e46b23e1bb7ec", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 452, "doc": {"id": "9-1046", "question_stem": "The skeletal system protects which of these?", "choices": {"text": ["liver", "eyelashes", "finger nails", "blood vessels"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "The skeletal system protects which of these?", "arg_1": " liver"}, "gen_args_1": {"arg_0": "The skeletal system protects which of these?", "arg_1": " eyelashes"}, "gen_args_2": {"arg_0": "The skeletal system protects which of these?", "arg_1": " finger nails"}, "gen_args_3": {"arg_0": "The skeletal system protects which of these?", "arg_1": " blood vessels"}}, "resps": [[["-23.625", "False"]], [["-30.125", "False"]], [["-33.25", "False"]], [["-19.25", "False"]]], "filtered_resps": [["-23.625", "False"], ["-30.125", "False"], ["-33.25", "False"], ["-19.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "6a3607f4508553f86dec686dad25249d468b3c3c170d72e7970e02fb6ca40f3c", "prompt_hash": "7894b3f003508c73165579d3ff146d942a56e7aa0a631ea6f7697671db9deb7a", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 453, "doc": {"id": "167", "question_stem": "What has more gravity force than Earth but less than the sun?", "choices": {"text": ["Jupiter", "the moon", "a space station", "a comet"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "What has more gravity force than Earth but less than the sun?", "arg_1": " Jupiter"}, "gen_args_1": {"arg_0": "What has more gravity force than Earth but less than the sun?", "arg_1": " the moon"}, "gen_args_2": {"arg_0": "What has more gravity force than Earth but less than the sun?", "arg_1": " a space station"}, "gen_args_3": {"arg_0": "What has more gravity force than Earth but less than the sun?", "arg_1": " a comet"}}, "resps": [[["-12.0", "False"]], [["-20.25", "False"]], [["-26.5", "False"]], [["-28.625", "False"]]], "filtered_resps": [["-12.0", "False"], ["-20.25", "False"], ["-26.5", "False"], ["-28.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "6d02f201dc84341abc604f7c4cdcaabdac97967c672bca5b15881bb18b82f331", "prompt_hash": "3d05e0dd69e2a9815112e450a2ad133d7125e2178e70a7bc7f076fa32daab948", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 454, "doc": {"id": "9-566", "question_stem": "The dam was put under much more stress after the", "choices": {"text": ["party", "huge rain storm", "drought", "breakup."], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "The dam was put under much more stress after the", "arg_1": " party"}, "gen_args_1": {"arg_0": "The dam was put under much more stress after the", "arg_1": " huge rain storm"}, "gen_args_2": {"arg_0": "The dam was put under much more stress after the", "arg_1": " drought"}, "gen_args_3": {"arg_0": "The dam was put under much more stress after the", "arg_1": " breakup."}}, "resps": [[["-10.4375", "False"]], [["-15.375", "False"]], [["-5.40625", "False"]], [["-12.625", "False"]]], "filtered_resps": [["-10.4375", "False"], ["-15.375", "False"], ["-5.40625", "False"], ["-12.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "bd3a1d47d64a449749a2c5cd1814adf697ffd619fd7f518fbb98910c2ee0296a", "prompt_hash": "3a425f23e8c270839530abae54a700a1ece3c6bb1e501cdd6771d7745cc5faf7", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 455, "doc": {"id": "8-28", "question_stem": "If photosynthesis was a recipe it would require these ingredients", "choices": {"text": ["CO2, water, and argon", "sunlight, oxygen, and fertilizer", "CO2, H20, and cloudy skies", "CO2, H20, and sun rays"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "If photosynthesis was a recipe it would require these ingredients", "arg_1": " CO2, water, and argon"}, "gen_args_1": {"arg_0": "If photosynthesis was a recipe it would require these ingredients", "arg_1": " sunlight, oxygen, and fertilizer"}, "gen_args_2": {"arg_0": "If photosynthesis was a recipe it would require these ingredients", "arg_1": " CO2, H20, and cloudy skies"}, "gen_args_3": {"arg_0": "If photosynthesis was a recipe it would require these ingredients", "arg_1": " CO2, H20, and sun rays"}}, "resps": [[["-40.0", "False"]], [["-39.5", "False"]], [["-43.25", "False"]], [["-36.0", "False"]]], "filtered_resps": [["-40.0", "False"], ["-39.5", "False"], ["-43.25", "False"], ["-36.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "3deb5b1064817143dadab155f366af2981085a5e012c35d820c401b6cd49aafe", "prompt_hash": "23bdb763b31a11ebc680326dba9ddbf4ba1e77caa76866e3611ac234b032bb83", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 456, "doc": {"id": "7-179", "question_stem": "If a nail is Fe, that nail is", "choices": {"text": ["foreign", "atomic 26", "nickel", "atomic 12"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "If a nail is Fe, that nail is", "arg_1": " foreign"}, "gen_args_1": {"arg_0": "If a nail is Fe, that nail is", "arg_1": " atomic 26"}, "gen_args_2": {"arg_0": "If a nail is Fe, that nail is", "arg_1": " nickel"}, "gen_args_3": {"arg_0": "If a nail is Fe, that nail is", "arg_1": " atomic 12"}}, "resps": [[["-11.625", "False"]], [["-18.5", "False"]], [["-9.0625", "False"]], [["-20.625", "False"]]], "filtered_resps": [["-11.625", "False"], ["-18.5", "False"], ["-9.0625", "False"], ["-20.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "382784a16361ebcf7546cfa157c66017ab52765d351fb605130b2ee4ad90adb8", "prompt_hash": "fdedac765185dff8949e21d8cf576b7731dd7566f3afc354235cba409f879c2a", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 457, "doc": {"id": "389", "question_stem": "when a circle is torn it is", "choices": {"text": ["doubled", "changed", "a smaller circle", "a square"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "when a circle is torn it is", "arg_1": " doubled"}, "gen_args_1": {"arg_0": "when a circle is torn it is", "arg_1": " changed"}, "gen_args_2": {"arg_0": "when a circle is torn it is", "arg_1": " a smaller circle"}, "gen_args_3": {"arg_0": "when a circle is torn it is", "arg_1": " a square"}}, "resps": [[["-10.875", "False"]], [["-8.875", "False"]], [["-10.4375", "False"]], [["-10.1875", "False"]]], "filtered_resps": [["-10.875", "False"], ["-8.875", "False"], ["-10.4375", "False"], ["-10.1875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "bbdf8a489eb2aa34f2484a39d8fd2d45490d756646eb77d3a30290271cbad13c", "prompt_hash": "14473c48429089cd6fb3cea7964312d79558c11f84510089a67eafb5479da872", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 458, "doc": {"id": "1528", "question_stem": "Wind can cause", "choices": {"text": ["leaves to remain on branches", "trees to stand perfectly still", "dunes at the beach to be depleted", "still waters on the ocean"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Wind can cause", "arg_1": " leaves to remain on branches"}, "gen_args_1": {"arg_0": "Wind can cause", "arg_1": " trees to stand perfectly still"}, "gen_args_2": {"arg_0": "Wind can cause", "arg_1": " dunes at the beach to be depleted"}, "gen_args_3": {"arg_0": "Wind can cause", "arg_1": " still waters on the ocean"}}, "resps": [[["-20.25", "False"]], [["-24.125", "False"]], [["-38.75", "False"]], [["-27.625", "False"]]], "filtered_resps": [["-20.25", "False"], ["-24.125", "False"], ["-38.75", "False"], ["-27.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "56221ee134ce5f17f47c2e3675feccbaaa3f7885f7e298fbb6756927e99e156b", "prompt_hash": "38214f9c0a6eb1c2378a1f8f31ece909c9057ac1d6e0b010853628650785d4ac", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 459, "doc": {"id": "1457", "question_stem": "What happens as water levels rise?", "choices": {"text": ["fish swim more", "homes are built", "land is taller", "beaches shrink"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "What happens as water levels rise?", "arg_1": " fish swim more"}, "gen_args_1": {"arg_0": "What happens as water levels rise?", "arg_1": " homes are built"}, "gen_args_2": {"arg_0": "What happens as water levels rise?", "arg_1": " land is taller"}, "gen_args_3": {"arg_0": "What happens as water levels rise?", "arg_1": " beaches shrink"}}, "resps": [[["-29.25", "False"]], [["-26.125", "False"]], [["-31.25", "False"]], [["-27.0", "False"]]], "filtered_resps": [["-29.25", "False"], ["-26.125", "False"], ["-31.25", "False"], ["-27.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "c62dd703a9ebc5a9acc432aa4b8d6d52cf97a5c65da2b70d6f7a92b0ff6f7cfe", "prompt_hash": "919e59524860ecf072c93eeccd09e9413f14b6b517eee4d5f3ef517f05986b3d", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 460, "doc": {"id": "1208", "question_stem": "An increase in an object's temperature occurs when", "choices": {"text": ["an orange is placed in a refrigerator", "a steak is removed from the freezer to defrost", "a glass of water is moved from counter top to dinner table", "an ice tray is placed in a freezer"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "An increase in an object's temperature occurs when", "arg_1": " an orange is placed in a refrigerator"}, "gen_args_1": {"arg_0": "An increase in an object's temperature occurs when", "arg_1": " a steak is removed from the freezer to defrost"}, "gen_args_2": {"arg_0": "An increase in an object's temperature occurs when", "arg_1": " a glass of water is moved from counter top to dinner table"}, "gen_args_3": {"arg_0": "An increase in an object's temperature occurs when", "arg_1": " an ice tray is placed in a freezer"}}, "resps": [[["-22.625", "False"]], [["-42.0", "False"]], [["-44.75", "False"]], [["-37.25", "False"]]], "filtered_resps": [["-22.625", "False"], ["-42.0", "False"], ["-44.75", "False"], ["-37.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4466aa365d990a5494baa30c195a9e9251339de98a819045b2b60ca7a4641134", "prompt_hash": "100e9d4a707d2431df71540712f500bcaf0deb9126129f8375ac23d797b96e37", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 461, "doc": {"id": "1170", "question_stem": "A sousaphone", "choices": {"text": ["is ancient", "is a frog", "makes deep noises", "is a smartphone"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A sousaphone", "arg_1": " is ancient"}, "gen_args_1": {"arg_0": "A sousaphone", "arg_1": " is a frog"}, "gen_args_2": {"arg_0": "A sousaphone", "arg_1": " makes deep noises"}, "gen_args_3": {"arg_0": "A sousaphone", "arg_1": " is a smartphone"}}, "resps": [[["-17.25", "False"]], [["-15.875", "False"]], [["-32.75", "False"]], [["-13.625", "False"]]], "filtered_resps": [["-17.25", "False"], ["-15.875", "False"], ["-32.75", "False"], ["-13.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "6cc713ddb8737bc019187f3d1e9f47b133cf485652159d7a21f6aa0a63013997", "prompt_hash": "1f8188166f8d8bb63dff914696cbdfb9e7e28b99da2753edb6913d03cc3166c2", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 462, "doc": {"id": "8-409", "question_stem": "A cooked lobster is", "choices": {"text": ["inedible", "cold", "dead", "green"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A cooked lobster is", "arg_1": " inedible"}, "gen_args_1": {"arg_0": "A cooked lobster is", "arg_1": " cold"}, "gen_args_2": {"arg_0": "A cooked lobster is", "arg_1": " dead"}, "gen_args_3": {"arg_0": "A cooked lobster is", "arg_1": " green"}}, "resps": [[["-15.25", "False"]], [["-9.75", "False"]], [["-14.0625", "False"]], [["-12.625", "False"]]], "filtered_resps": [["-15.25", "False"], ["-9.75", "False"], ["-14.0625", "False"], ["-12.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "81f54ad6715945e7260a38cd6b494228f01b08a1a8b6cc8d7dfbc43db9304bce", "prompt_hash": "4acfd47d912d81b41090581c298751153430f7bf91120889f1d479d1210b11cf", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 463, "doc": {"id": "8-307", "question_stem": "An animal might pant", "choices": {"text": ["on a sunny day", "during a rain storm", "when it is snowing", "during the night time"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "An animal might pant", "arg_1": " on a sunny day"}, "gen_args_1": {"arg_0": "An animal might pant", "arg_1": " during a rain storm"}, "gen_args_2": {"arg_0": "An animal might pant", "arg_1": " when it is snowing"}, "gen_args_3": {"arg_0": "An animal might pant", "arg_1": " during the night time"}}, "resps": [[["-12.4375", "False"]], [["-16.875", "False"]], [["-18.625", "False"]], [["-23.75", "False"]]], "filtered_resps": [["-12.4375", "False"], ["-16.875", "False"], ["-18.625", "False"], ["-23.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a4f166154aa5fc7cbae83ea8d675b92eef414abbbf0982f89c52933a82ff0419", "prompt_hash": "624227b3ad8e58180b5d5d720ed1b4ec57bc2ad191002c56cfb5785dcf08741d", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 464, "doc": {"id": "1948", "question_stem": "Carnivores", "choices": {"text": ["eat foliage and vegetables exclusively", "are the bottom of the food chain", "require prey to survive", "require carbon dioxide to survive"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Carnivores", "arg_1": " eat foliage and vegetables exclusively"}, "gen_args_1": {"arg_0": "Carnivores", "arg_1": " are the bottom of the food chain"}, "gen_args_2": {"arg_0": "Carnivores", "arg_1": " require prey to survive"}, "gen_args_3": {"arg_0": "Carnivores", "arg_1": " require carbon dioxide to survive"}}, "resps": [[["-41.0", "False"]], [["-13.0", "False"]], [["-17.25", "False"]], [["-25.0", "False"]]], "filtered_resps": [["-41.0", "False"], ["-13.0", "False"], ["-17.25", "False"], ["-25.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "c3bea78f0f1c0146c311058723768fa0eedf6b74bf51f75ea70cefe26d6d9dca", "prompt_hash": "4b65cbeaae537c106c33039778f83df2b1233f287f325f1ce864ac2cb25ef319", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 465, "doc": {"id": "661", "question_stem": "The light that appears dimmest is", "choices": {"text": ["the light in the hall", "a light in the room", "a star outside the window", "a streetlight outside the window"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "The light that appears dimmest is", "arg_1": " the light in the hall"}, "gen_args_1": {"arg_0": "The light that appears dimmest is", "arg_1": " a light in the room"}, "gen_args_2": {"arg_0": "The light that appears dimmest is", "arg_1": " a star outside the window"}, "gen_args_3": {"arg_0": "The light that appears dimmest is", "arg_1": " a streetlight outside the window"}}, "resps": [[["-17.75", "False"]], [["-19.625", "False"]], [["-29.25", "False"]], [["-24.75", "False"]]], "filtered_resps": [["-17.75", "False"], ["-19.625", "False"], ["-29.25", "False"], ["-24.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "c14772e2e3e54ed636bb7057c11cf8f8a481ebda52715f2a814eb92484a1ae3f", "prompt_hash": "8651b0511fe3f6601e5da1a421fff716edebc2dd595c1aa0f7f20981b3a61c1c", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 466, "doc": {"id": "7-435", "question_stem": "A plant will grow strong if it has", "choices": {"text": ["love", "heat", "earth", "sand"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A plant will grow strong if it has", "arg_1": " love"}, "gen_args_1": {"arg_0": "A plant will grow strong if it has", "arg_1": " heat"}, "gen_args_2": {"arg_0": "A plant will grow strong if it has", "arg_1": " earth"}, "gen_args_3": {"arg_0": "A plant will grow strong if it has", "arg_1": " sand"}}, "resps": [[["-14.75", "False"]], [["-14.1875", "False"]], [["-16.125", "False"]], [["-14.6875", "False"]]], "filtered_resps": [["-14.75", "False"], ["-14.1875", "False"], ["-16.125", "False"], ["-14.6875", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "acb04c77e8cd4dd20a402e05d9b0363f1ffbb8256599e16615ab6834cc3b3622", "prompt_hash": "a8229ed1647c3386e618a563ed179f578c7d8f71a6ec0987b16a68527486eabe", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 467, "doc": {"id": "8-332", "question_stem": "loose soil can be caused by one of these", "choices": {"text": ["a koala sitting on a tree", "none of these", "a worm burrowing through the earth", "a bird flying through the air"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "loose soil can be caused by one of these", "arg_1": " a koala sitting on a tree"}, "gen_args_1": {"arg_0": "loose soil can be caused by one of these", "arg_1": " none of these"}, "gen_args_2": {"arg_0": "loose soil can be caused by one of these", "arg_1": " a worm burrowing through the earth"}, "gen_args_3": {"arg_0": "loose soil can be caused by one of these", "arg_1": " a bird flying through the air"}}, "resps": [[["-36.0", "False"]], [["-17.0", "False"]], [["-29.25", "False"]], [["-34.25", "False"]]], "filtered_resps": [["-36.0", "False"], ["-17.0", "False"], ["-29.25", "False"], ["-34.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "0fb2f79b35d8b399f3bbc50c2b03b455223c23155787d0215b3792d251aa94c6", "prompt_hash": "b0653d77cb1e31483bea61f1db41b180c4504d664c8260ede78dd05b9571efd8", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 468, "doc": {"id": "948", "question_stem": "An instinctual behavior is", "choices": {"text": ["dogs rolling over on command", "frogs returning to the ponds were they hatched to lay eggs", "birds mimicking human speech", "seals clapping for treats from trainers"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "An instinctual behavior is", "arg_1": " dogs rolling over on command"}, "gen_args_1": {"arg_0": "An instinctual behavior is", "arg_1": " frogs returning to the ponds were they hatched to lay eggs"}, "gen_args_2": {"arg_0": "An instinctual behavior is", "arg_1": " birds mimicking human speech"}, "gen_args_3": {"arg_0": "An instinctual behavior is", "arg_1": " seals clapping for treats from trainers"}}, "resps": [[["-36.75", "False"]], [["-60.0", "False"]], [["-28.75", "False"]], [["-62.75", "False"]]], "filtered_resps": [["-36.75", "False"], ["-60.0", "False"], ["-28.75", "False"], ["-62.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "8b5d40206001d50a29a7613ba949b779c622d008f8dc386da0a801a3391a0e5c", "prompt_hash": "911f44ab4f961c7b738102cb75c23dfa6e6ccd92204864128a9046213c19605b", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 469, "doc": {"id": "381", "question_stem": "How do plants reproduce?", "choices": {"text": ["seeds", "stem", "flowers", "leaves"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "How do plants reproduce?", "arg_1": " seeds"}, "gen_args_1": {"arg_0": "How do plants reproduce?", "arg_1": " stem"}, "gen_args_2": {"arg_0": "How do plants reproduce?", "arg_1": " flowers"}, "gen_args_3": {"arg_0": "How do plants reproduce?", "arg_1": " leaves"}}, "resps": [[["-13.125", "False"]], [["-19.0", "False"]], [["-15.4375", "False"]], [["-19.5", "False"]]], "filtered_resps": [["-13.125", "False"], ["-19.0", "False"], ["-15.4375", "False"], ["-19.5", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "dce50828f5711fed4e30bd5b5329b89b9632ba624d97b9c5a4102ed26573ffbb", "prompt_hash": "4f6c41999d35b7ef4ba077aea9cab1eda8fec98638d4b61ce03e8d9be8d83e7a", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 470, "doc": {"id": "9-759", "question_stem": "What produce pollen and seeds?", "choices": {"text": ["lakes that are frozen over", "things you give a loved one in a bouquet", "various types of animals", "a person that is healthy"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "What produce pollen and seeds?", "arg_1": " lakes that are frozen over"}, "gen_args_1": {"arg_0": "What produce pollen and seeds?", "arg_1": " things you give a loved one in a bouquet"}, "gen_args_2": {"arg_0": "What produce pollen and seeds?", "arg_1": " various types of animals"}, "gen_args_3": {"arg_0": "What produce pollen and seeds?", "arg_1": " a person that is healthy"}}, "resps": [[["-38.0", "False"]], [["-46.0", "False"]], [["-24.125", "False"]], [["-29.0", "False"]]], "filtered_resps": [["-38.0", "False"], ["-46.0", "False"], ["-24.125", "False"], ["-29.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "94fd8a60e22bfa48b1c890bd19c021df15ad74be5ef4f662570f97c052697bc3", "prompt_hash": "0eba91530446746894fde6e59340a2a835852746017e3185907d5aa20ee27459", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 471, "doc": {"id": "8-350", "question_stem": "which of these would be most ideal for plant root growth?", "choices": {"text": ["a sticky clay soil", "soil with worms burrowing around", "an arid soil with little looseness", "all of these"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "which of these would be most ideal for plant root growth?", "arg_1": " a sticky clay soil"}, "gen_args_1": {"arg_0": "which of these would be most ideal for plant root growth?", "arg_1": " soil with worms burrowing around"}, "gen_args_2": {"arg_0": "which of these would be most ideal for plant root growth?", "arg_1": " an arid soil with little looseness"}, "gen_args_3": {"arg_0": "which of these would be most ideal for plant root growth?", "arg_1": " all of these"}}, "resps": [[["-26.125", "False"]], [["-36.25", "False"]], [["-43.5", "False"]], [["-13.0", "False"]]], "filtered_resps": [["-26.125", "False"], ["-36.25", "False"], ["-43.5", "False"], ["-13.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "ae8850f8c2aef2eff1e49a425cb1b8f8951d54757889dd60918f8769ea0b6a6f", "prompt_hash": "b93fec2249b1ee6f435d7be69a4974f92c53b2b6ab1d829067fd1b9d3ae3fe16", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 472, "doc": {"id": "7-727", "question_stem": "Having a sense of touch means", "choices": {"text": ["I am the water", "I am a tree", "I am an Ant", "I am the Air"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Having a sense of touch means", "arg_1": " I am the water"}, "gen_args_1": {"arg_0": "Having a sense of touch means", "arg_1": " I am a tree"}, "gen_args_2": {"arg_0": "Having a sense of touch means", "arg_1": " I am an Ant"}, "gen_args_3": {"arg_0": "Having a sense of touch means", "arg_1": " I am the Air"}}, "resps": [[["-23.875", "False"]], [["-23.875", "False"]], [["-30.75", "False"]], [["-29.125", "False"]]], "filtered_resps": [["-23.875", "False"], ["-23.875", "False"], ["-30.75", "False"], ["-29.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "5691103914bf8dae3da73f30f578fa6788d1a2325e310df184819c94b3471226", "prompt_hash": "ceb82befdb060519d91f6a5e84ced4d0117ddf007b1b9e04b0fc0bdcd1e8e642", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 473, "doc": {"id": "850", "question_stem": "Live birth is exemplified in", "choices": {"text": ["snakes slithering out of eggs", "a calf emerging from a mother giraffe", "owlets pecking out of their encasement", "sea turtles emerging from their shells"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Live birth is exemplified in", "arg_1": " snakes slithering out of eggs"}, "gen_args_1": {"arg_0": "Live birth is exemplified in", "arg_1": " a calf emerging from a mother giraffe"}, "gen_args_2": {"arg_0": "Live birth is exemplified in", "arg_1": " owlets pecking out of their encasement"}, "gen_args_3": {"arg_0": "Live birth is exemplified in", "arg_1": " sea turtles emerging from their shells"}}, "resps": [[["-29.25", "False"]], [["-35.5", "False"]], [["-67.0", "False"]], [["-26.75", "False"]]], "filtered_resps": [["-29.25", "False"], ["-35.5", "False"], ["-67.0", "False"], ["-26.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "34c64e8a981ca47cfa5024090178b5e0d15918f3b869e9b977afab049a5b7e81", "prompt_hash": "f8280e11cb7576d3ea997b87ecf34f2e46d8c77c8176f8867b0929e29a3c7a42", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 474, "doc": {"id": "970", "question_stem": "Cooking peas requires", "choices": {"text": ["fresh briny sea water", "an unheated stove top", "salt and cayenne pepper", "turning on a stove top"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Cooking peas requires", "arg_1": " fresh briny sea water"}, "gen_args_1": {"arg_0": "Cooking peas requires", "arg_1": " an unheated stove top"}, "gen_args_2": {"arg_0": "Cooking peas requires", "arg_1": " salt and cayenne pepper"}, "gen_args_3": {"arg_0": "Cooking peas requires", "arg_1": " turning on a stove top"}}, "resps": [[["-29.125", "False"]], [["-24.0", "False"]], [["-20.375", "False"]], [["-21.625", "False"]]], "filtered_resps": [["-29.125", "False"], ["-24.0", "False"], ["-20.375", "False"], ["-21.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "3172bf318a02d6a3a2cfc572b94154b204dd548a7158704ecaebbdb66b41b1ea", "prompt_hash": "a1d3cccf7073e958ed4f2ba8d3d2a3ad157c02161e39dae17ba9d6bf80057fe5", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 475, "doc": {"id": "7-381", "question_stem": "Earth revolves around", "choices": {"text": ["the moon", "outer space", "another planet", "an energy source"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "Earth revolves around", "arg_1": " the moon"}, "gen_args_1": {"arg_0": "Earth revolves around", "arg_1": " outer space"}, "gen_args_2": {"arg_0": "Earth revolves around", "arg_1": " another planet"}, "gen_args_3": {"arg_0": "Earth revolves around", "arg_1": " an energy source"}}, "resps": [[["-14.125", "False"]], [["-18.25", "False"]], [["-12.875", "False"]], [["-20.0", "False"]]], "filtered_resps": [["-14.125", "False"], ["-18.25", "False"], ["-12.875", "False"], ["-20.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "18729ebddd81350b0cb050bdaab8b607abc0e4485f6ed6d746c7d28e531beeb6", "prompt_hash": "379e3ce7d9bd2b147ece4dba6f766da3f4dc18324c06591cbf0756daa870a793", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 476, "doc": {"id": "9-436", "question_stem": "A satellite orbits a", "choices": {"text": ["empty space", "ocean", "terrestrial body", "air pocket"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A satellite orbits a", "arg_1": " empty space"}, "gen_args_1": {"arg_0": "A satellite orbits a", "arg_1": " ocean"}, "gen_args_2": {"arg_0": "A satellite orbits a", "arg_1": " terrestrial body"}, "gen_args_3": {"arg_0": "A satellite orbits a", "arg_1": " air pocket"}}, "resps": [[["-24.375", "False"]], [["-24.875", "False"]], [["-8.625", "False"]], [["-35.0", "False"]]], "filtered_resps": [["-24.375", "False"], ["-24.875", "False"], ["-8.625", "False"], ["-35.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "4767b81b681a6ffcf22ea676481ec41b6973b9fa21aafbc65073ade608735b6c", "prompt_hash": "ad0ea827d664e127a1b57914941e07fa1cfefde7c0e49375c2b3c72d3aed8a60", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 477, "doc": {"id": "9-411", "question_stem": "Will happen to the number of islands if the planet's temperature rises?", "choices": {"text": ["they will increase", "nothing will happen", "they will shrink", "they will double"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Will happen to the number of islands if the planet's temperature rises?", "arg_1": " they will increase"}, "gen_args_1": {"arg_0": "Will happen to the number of islands if the planet's temperature rises?", "arg_1": " nothing will happen"}, "gen_args_2": {"arg_0": "Will happen to the number of islands if the planet's temperature rises?", "arg_1": " they will shrink"}, "gen_args_3": {"arg_0": "Will happen to the number of islands if the planet's temperature rises?", "arg_1": " they will double"}}, "resps": [[["-23.125", "False"]], [["-23.625", "False"]], [["-22.125", "False"]], [["-28.375", "False"]]], "filtered_resps": [["-23.125", "False"], ["-23.625", "False"], ["-22.125", "False"], ["-28.375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f2d711a8d4b40fe6c3f26ed65e6a7a9dd4e96dc97ef44bd6434c0d90222ae2b3", "prompt_hash": "6f04ab3b8fb776b82a5eebb66305574d7fca99d0259a4d58bcb23b02f85f3b17", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 478, "doc": {"id": "9-692", "question_stem": "There is a heightened threat of landslide in", "choices": {"text": ["the desert", "The Andes", "the ocean", "Indiana"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "There is a heightened threat of landslide in", "arg_1": " the desert"}, "gen_args_1": {"arg_0": "There is a heightened threat of landslide in", "arg_1": " The Andes"}, "gen_args_2": {"arg_0": "There is a heightened threat of landslide in", "arg_1": " the ocean"}, "gen_args_3": {"arg_0": "There is a heightened threat of landslide in", "arg_1": " Indiana"}}, "resps": [[["-10.9375", "False"]], [["-16.75", "False"]], [["-16.375", "False"]], [["-11.3125", "False"]]], "filtered_resps": [["-10.9375", "False"], ["-16.75", "False"], ["-16.375", "False"], ["-11.3125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "46db2c26e927453a53d1639d42a3af7795b53ab3bb4552e594e885fe87f90f18", "prompt_hash": "ada1ed54905412563f047de32e31a6e7db021254839716da26a17eecc97befea", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 479, "doc": {"id": "1334", "question_stem": "An example of water being an electrical conductor would be what?", "choices": {"text": ["lightening hitting water and organisms inside dying", "standing in a puddle and avoiding being struck by lightening", "standing in a field and getting struck by lightening", "grabbing a fence and being shocked"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "An example of water being an electrical conductor would be what?", "arg_1": " lightening hitting water and organisms inside dying"}, "gen_args_1": {"arg_0": "An example of water being an electrical conductor would be what?", "arg_1": " standing in a puddle and avoiding being struck by lightening"}, "gen_args_2": {"arg_0": "An example of water being an electrical conductor would be what?", "arg_1": " standing in a field and getting struck by lightening"}, "gen_args_3": {"arg_0": "An example of water being an electrical conductor would be what?", "arg_1": " grabbing a fence and being shocked"}}, "resps": [[["-73.0", "False"]], [["-56.0", "False"]], [["-51.0", "False"]], [["-47.0", "False"]]], "filtered_resps": [["-73.0", "False"], ["-56.0", "False"], ["-51.0", "False"], ["-47.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e6a4ee5ddb14ae516f841267ff6f96d568745021c775dc040b72e57f0743d275", "prompt_hash": "27f9fb639c0e1788f27d93710b66dbdfd1138c84cb55dca8eb8ac779e4ed7901", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 480, "doc": {"id": "9-1160", "question_stem": "What would Occur once between January 1st and December 31st", "choices": {"text": ["The moons orbit around the year", "One rotation on mercury", "The distance between earth and Jupiter when traveling at light speed", "A Solar Year on earth"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "What would Occur once between January 1st and December 31st", "arg_1": " The moons orbit around the year"}, "gen_args_1": {"arg_0": "What would Occur once between January 1st and December 31st", "arg_1": " One rotation on mercury"}, "gen_args_2": {"arg_0": "What would Occur once between January 1st and December 31st", "arg_1": " The distance between earth and Jupiter when traveling at light speed"}, "gen_args_3": {"arg_0": "What would Occur once between January 1st and December 31st", "arg_1": " A Solar Year on earth"}}, "resps": [[["-38.25", "False"]], [["-45.75", "False"]], [["-62.25", "False"]], [["-47.0", "False"]]], "filtered_resps": [["-38.25", "False"], ["-45.75", "False"], ["-62.25", "False"], ["-47.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "ea92ee69a1b97563bbfd31e6fea9df1f8b63d07dce7f00e1d6430ea9fdced06d", "prompt_hash": "5e208dd166ef187f80c2f3c975c9aa99753d3f5cb5c08a725fb122309d646664", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 481, "doc": {"id": "9-89", "question_stem": "Burning something that reproduces usually will:", "choices": {"text": ["impair its well being in some way", "weed out weaker members of the species", "speed up its biological functions", "increase its population growth"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "Burning something that reproduces usually will:", "arg_1": " impair its well being in some way"}, "gen_args_1": {"arg_0": "Burning something that reproduces usually will:", "arg_1": " weed out weaker members of the species"}, "gen_args_2": {"arg_0": "Burning something that reproduces usually will:", "arg_1": " speed up its biological functions"}, "gen_args_3": {"arg_0": "Burning something that reproduces usually will:", "arg_1": " increase its population growth"}}, "resps": [[["-40.0", "False"]], [["-39.75", "False"]], [["-33.0", "False"]], [["-25.25", "False"]]], "filtered_resps": [["-40.0", "False"], ["-39.75", "False"], ["-33.0", "False"], ["-25.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "a8b70d60c5a6f6578c3a0fb98b525ac7cb88524d746d61bdee3d72a3ade4d767", "prompt_hash": "91e672bfa7e192e4331a4e92f838201c24a2952cad0e1ea83a7a49413ecb56aa", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 482, "doc": {"id": "9-1034", "question_stem": "what does a chipmunk do with acorns", "choices": {"text": ["throw them at other chipmunks", "leave them where they're found", "use them to build shelter", "transfer them to the stomach"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "what does a chipmunk do with acorns", "arg_1": " throw them at other chipmunks"}, "gen_args_1": {"arg_0": "what does a chipmunk do with acorns", "arg_1": " leave them where they're found"}, "gen_args_2": {"arg_0": "what does a chipmunk do with acorns", "arg_1": " use them to build shelter"}, "gen_args_3": {"arg_0": "what does a chipmunk do with acorns", "arg_1": " transfer them to the stomach"}}, "resps": [[["-30.75", "False"]], [["-34.0", "False"]], [["-28.0", "False"]], [["-32.75", "False"]]], "filtered_resps": [["-30.75", "False"], ["-34.0", "False"], ["-28.0", "False"], ["-32.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "2668b8930ab7de50e941a74b87b6ed0aba1c9e642bfdde2719292b10f8ea9e77", "prompt_hash": "cd9f1a59761e84d12d67049f3c058f27fbf7726f8531e125a434de2ff00c18e8", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 483, "doc": {"id": "8-293", "question_stem": "A pot of pasta is boiling on the stove, and the lid on top of the pot is shaking as the water boils more rapidly. A person goes to the stove and removes the pot, releasing steam into the air above, and so the steam is", "choices": {"text": ["cold air", "water vapor", "very dry", "boiling water"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "A pot of pasta is boiling on the stove, and the lid on top of the pot is shaking as the water boils more rapidly. A person goes to the stove and removes the pot, releasing steam into the air above, and so the steam is", "arg_1": " cold air"}, "gen_args_1": {"arg_0": "A pot of pasta is boiling on the stove, and the lid on top of the pot is shaking as the water boils more rapidly. A person goes to the stove and removes the pot, releasing steam into the air above, and so the steam is", "arg_1": " water vapor"}, "gen_args_2": {"arg_0": "A pot of pasta is boiling on the stove, and the lid on top of the pot is shaking as the water boils more rapidly. A person goes to the stove and removes the pot, releasing steam into the air above, and so the steam is", "arg_1": " very dry"}, "gen_args_3": {"arg_0": "A pot of pasta is boiling on the stove, and the lid on top of the pot is shaking as the water boils more rapidly. A person goes to the stove and removes the pot, releasing steam into the air above, and so the steam is", "arg_1": " boiling water"}}, "resps": [[["-12.5", "False"]], [["-10.0625", "False"]], [["-13.8125", "False"]], [["-15.125", "False"]]], "filtered_resps": [["-12.5", "False"], ["-10.0625", "False"], ["-13.8125", "False"], ["-15.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "f8e711d0f4d2ad8a10bf71872d476392978ca0034cbd7a4f60d2de10d4059682", "prompt_hash": "fb6c34ddf46bb02d672e6c7948ba0ee4fd3e9abd98689cdc33c3e920be659da5", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 484, "doc": {"id": "9-652", "question_stem": "A plant that gets extra minerals such as zinc are probably", "choices": {"text": ["planted in zinc pills", "plated in the sea", "placed in good soil", "made out of soil"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A plant that gets extra minerals such as zinc are probably", "arg_1": " planted in zinc pills"}, "gen_args_1": {"arg_0": "A plant that gets extra minerals such as zinc are probably", "arg_1": " plated in the sea"}, "gen_args_2": {"arg_0": "A plant that gets extra minerals such as zinc are probably", "arg_1": " placed in good soil"}, "gen_args_3": {"arg_0": "A plant that gets extra minerals such as zinc are probably", "arg_1": " made out of soil"}}, "resps": [[["-35.5", "False"]], [["-22.875", "False"]], [["-21.25", "False"]], [["-20.75", "False"]]], "filtered_resps": [["-35.5", "False"], ["-22.875", "False"], ["-21.25", "False"], ["-20.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "9e4e6aa52ce1be0ee58a3ceeafc19440e448332d426948d7ecb3be22dd358114", "prompt_hash": "3b5d54df33d6a45f536279c2932f06e63831ea9e2fb9ec37a689ac6b0fd929e1", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 485, "doc": {"id": "1391", "question_stem": "Which item has a higher altitude?", "choices": {"text": ["Tile Floor", "Cars", "A 6'' Man", "A Picture Book"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Which item has a higher altitude?", "arg_1": " Tile Floor"}, "gen_args_1": {"arg_0": "Which item has a higher altitude?", "arg_1": " Cars"}, "gen_args_2": {"arg_0": "Which item has a higher altitude?", "arg_1": " A 6'' Man"}, "gen_args_3": {"arg_0": "Which item has a higher altitude?", "arg_1": " A Picture Book"}}, "resps": [[["-23.75", "False"]], [["-15.3125", "False"]], [["-44.0", "False"]], [["-26.75", "False"]]], "filtered_resps": [["-23.75", "False"], ["-15.3125", "False"], ["-44.0", "False"], ["-26.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "dff789a12393a9e4af0fd6d96600d137233d9ed462551d8acc56532e086d2b10", "prompt_hash": "5f2a080a626f29a12c9ac91930133ca33baa3a26e355f530447ce91e21c89745", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 486, "doc": {"id": "9-948", "question_stem": "if a student wants an orange, he would have to get it from which of these?", "choices": {"text": ["from a live cow", "from a live plant", "from a volcano cave", "from a wild dog"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "if a student wants an orange, he would have to get it from which of these?", "arg_1": " from a live cow"}, "gen_args_1": {"arg_0": "if a student wants an orange, he would have to get it from which of these?", "arg_1": " from a live plant"}, "gen_args_2": {"arg_0": "if a student wants an orange, he would have to get it from which of these?", "arg_1": " from a volcano cave"}, "gen_args_3": {"arg_0": "if a student wants an orange, he would have to get it from which of these?", "arg_1": " from a wild dog"}}, "resps": [[["-34.5", "False"]], [["-33.0", "False"]], [["-39.75", "False"]], [["-32.0", "False"]]], "filtered_resps": [["-34.5", "False"], ["-33.0", "False"], ["-39.75", "False"], ["-32.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "1fb52084684b62a1c513c76f315cc39acef85b424c3cce5395c0d717932313fa", "prompt_hash": "29371f301e1943a82906e266f72f94dbac4aeac9281f1ba4baeee9328e302997", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 487, "doc": {"id": "8-213", "question_stem": "What could have covered an organism in order to create a trilobite?", "choices": {"text": ["Grass", "Water", "Snow", "Sand"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}, "target": "3", "arguments": {"gen_args_0": {"arg_0": "What could have covered an organism in order to create a trilobite?", "arg_1": " Grass"}, "gen_args_1": {"arg_0": "What could have covered an organism in order to create a trilobite?", "arg_1": " Water"}, "gen_args_2": {"arg_0": "What could have covered an organism in order to create a trilobite?", "arg_1": " Snow"}, "gen_args_3": {"arg_0": "What could have covered an organism in order to create a trilobite?", "arg_1": " Sand"}}, "resps": [[["-18.0", "False"]], [["-16.375", "False"]], [["-18.75", "False"]], [["-16.625", "False"]]], "filtered_resps": [["-18.0", "False"], ["-16.375", "False"], ["-18.75", "False"], ["-16.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "68a33ca453ca02b6935e6d56f5c496dd63c8c7bd879046a6ab099869a1433ec8", "prompt_hash": "7a7869a98712000b261afd243193ddf7a7837dccb79ac1cb3290439e8fc4a06d", "target_hash": "4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 488, "doc": {"id": "162", "question_stem": "A dog is more likely to shiver at", "choices": {"text": ["1 pm", "5 am", "9 am", "6 pm"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "A dog is more likely to shiver at", "arg_1": " 1 pm"}, "gen_args_1": {"arg_0": "A dog is more likely to shiver at", "arg_1": " 5 am"}, "gen_args_2": {"arg_0": "A dog is more likely to shiver at", "arg_1": " 9 am"}, "gen_args_3": {"arg_0": "A dog is more likely to shiver at", "arg_1": " 6 pm"}}, "resps": [[["-16.5", "False"]], [["-11.8125", "False"]], [["-13.4375", "False"]], [["-10.25", "False"]]], "filtered_resps": [["-16.5", "False"], ["-11.8125", "False"], ["-13.4375", "False"], ["-10.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "9ea269a2e42db917611c8fc7e62bccecbacb7ab6ceb5c42b07d7125ab6e14587", "prompt_hash": "ea4c41865e79f47210f2bdc60816945e094f8f9852ee362e44d98d9ae42bf3b8", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 489, "doc": {"id": "1359", "question_stem": "Photosynthesis does what by converting carbon dioxide, water, and sunlight into carbohydrates?", "choices": {"text": ["nourishes small protein bits that need to eat with tiny shakes", "providing nourishment which enables some growth to vegetation", "mixes carbs into soluble plant matter", "makes good vegetable protein"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "Photosynthesis does what by converting carbon dioxide, water, and sunlight into carbohydrates?", "arg_1": " nourishes small protein bits that need to eat with tiny shakes"}, "gen_args_1": {"arg_0": "Photosynthesis does what by converting carbon dioxide, water, and sunlight into carbohydrates?", "arg_1": " providing nourishment which enables some growth to vegetation"}, "gen_args_2": {"arg_0": "Photosynthesis does what by converting carbon dioxide, water, and sunlight into carbohydrates?", "arg_1": " mixes carbs into soluble plant matter"}, "gen_args_3": {"arg_0": "Photosynthesis does what by converting carbon dioxide, water, and sunlight into carbohydrates?", "arg_1": " makes good vegetable protein"}}, "resps": [[["-98.5", "False"]], [["-71.0", "False"]], [["-52.75", "False"]], [["-46.75", "False"]]], "filtered_resps": [["-98.5", "False"], ["-71.0", "False"], ["-52.75", "False"], ["-46.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "1a95b70b70c0943604b9b4a7434acef538d417c512d8f19cd729e4b1a03259e4", "prompt_hash": "b22af5c546c17437d645f03a9a9edfdb88c28e749b8f7a2309fd5f62fe444531", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 490, "doc": {"id": "9-743", "question_stem": "where might a bunny live?", "choices": {"text": ["a thicket", "atop palm trees", "a sewer system", "a deserted island"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "where might a bunny live?", "arg_1": " a thicket"}, "gen_args_1": {"arg_0": "where might a bunny live?", "arg_1": " atop palm trees"}, "gen_args_2": {"arg_0": "where might a bunny live?", "arg_1": " a sewer system"}, "gen_args_3": {"arg_0": "where might a bunny live?", "arg_1": " a deserted island"}}, "resps": [[["-18.5", "False"]], [["-32.5", "False"]], [["-23.375", "False"]], [["-23.125", "False"]]], "filtered_resps": [["-18.5", "False"], ["-32.5", "False"], ["-23.375", "False"], ["-23.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "e041f628f7574bd5b4710c31392e24f16cd2fcee146727233e089bdfb783f3b7", "prompt_hash": "a280ea86f9faff4ec04d1ee4faf62eed571cec78bbc33be59f580e40abd8ba01", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 491, "doc": {"id": "9-645", "question_stem": "A shark will be unable to survive on eating algae and moss, because", "choices": {"text": ["it is a predator", "it is a vegetarian", "it is a freshwater fish", "it is a producer"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "A shark will be unable to survive on eating algae and moss, because", "arg_1": " it is a predator"}, "gen_args_1": {"arg_0": "A shark will be unable to survive on eating algae and moss, because", "arg_1": " it is a vegetarian"}, "gen_args_2": {"arg_0": "A shark will be unable to survive on eating algae and moss, because", "arg_1": " it is a freshwater fish"}, "gen_args_3": {"arg_0": "A shark will be unable to survive on eating algae and moss, because", "arg_1": " it is a producer"}}, "resps": [[["-4.71875", "False"]], [["-12.0", "False"]], [["-16.875", "False"]], [["-13.625", "False"]]], "filtered_resps": [["-4.71875", "False"], ["-12.0", "False"], ["-16.875", "False"], ["-13.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "156002bde1daec832fa0d3a32938490a31d42996b25bfc9a813c91f6fe0cf3f1", "prompt_hash": "26972a8d8595f1c295abaae61d3e45ce48fcfc938f2d697d7a5851f7724dedbe", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 492, "doc": {"id": "8-250", "question_stem": "A meadow vole just gave birth, and needs to feed herself so that she can produce milk for her babies. She searches for food in a field, and happily munches down on some", "choices": {"text": ["oil", "deer", "bugs", "recycled plastic fruit"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A meadow vole just gave birth, and needs to feed herself so that she can produce milk for her babies. She searches for food in a field, and happily munches down on some", "arg_1": " oil"}, "gen_args_1": {"arg_0": "A meadow vole just gave birth, and needs to feed herself so that she can produce milk for her babies. She searches for food in a field, and happily munches down on some", "arg_1": " deer"}, "gen_args_2": {"arg_0": "A meadow vole just gave birth, and needs to feed herself so that she can produce milk for her babies. She searches for food in a field, and happily munches down on some", "arg_1": " bugs"}, "gen_args_3": {"arg_0": "A meadow vole just gave birth, and needs to feed herself so that she can produce milk for her babies. She searches for food in a field, and happily munches down on some", "arg_1": " recycled plastic fruit"}}, "resps": [[["-17.25", "False"]], [["-11.8125", "False"]], [["-10.25", "False"]], [["-31.625", "False"]]], "filtered_resps": [["-17.25", "False"], ["-11.8125", "False"], ["-10.25", "False"], ["-31.625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "04508fc4d5eb64c6436336aa283a5a7b0e91121726d0787a84a498b251072337", "prompt_hash": "8db6184fb8b16285d9281314b28f212d7c0dda12e4d40de348bc6390155814e5", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 1.0, "acc_norm": 0.0}
{"doc_id": 493, "doc": {"id": "283", "question_stem": "The Grand Canyon was formed by", "choices": {"text": ["a volcano erupting in 1782", "a river named after the 20th state to join the union flowing over time", "a river named after the 38th state to join the union flowing over time", "the Great Lakes drying up"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "The Grand Canyon was formed by", "arg_1": " a volcano erupting in 1782"}, "gen_args_1": {"arg_0": "The Grand Canyon was formed by", "arg_1": " a river named after the 20th state to join the union flowing over time"}, "gen_args_2": {"arg_0": "The Grand Canyon was formed by", "arg_1": " a river named after the 38th state to join the union flowing over time"}, "gen_args_3": {"arg_0": "The Grand Canyon was formed by", "arg_1": " the Great Lakes drying up"}}, "resps": [[["-48.25", "False"]], [["-96.5", "False"]], [["-89.0", "False"]], [["-37.0", "False"]]], "filtered_resps": [["-48.25", "False"], ["-96.5", "False"], ["-89.0", "False"], ["-37.0", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d3ba01e04ee24acc746ccd879742813d4e4ac41a6d6e9b1316164a13fb7597ca", "prompt_hash": "4f25add5eb3d5232821653b7c7ad0a05224926a8430b1f9878089c3845a38801", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 1.0}
{"doc_id": 494, "doc": {"id": "8-183", "question_stem": "A woman, with a pale complexion, wants to spend the bright, sunny day at the beach. She makes sure that she stops at the store to pick up some sunblock before she begins to enjoy her day filled with sand and surf. She applies the sunblock carefully and thoroughly, because she knows that", "choices": {"text": ["UV rays are harmful", "sunlight will be fun", "the sun is close", "the sun is in space"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "A woman, with a pale complexion, wants to spend the bright, sunny day at the beach. She makes sure that she stops at the store to pick up some sunblock before she begins to enjoy her day filled with sand and surf. She applies the sunblock carefully and thoroughly, because she knows that", "arg_1": " UV rays are harmful"}, "gen_args_1": {"arg_0": "A woman, with a pale complexion, wants to spend the bright, sunny day at the beach. She makes sure that she stops at the store to pick up some sunblock before she begins to enjoy her day filled with sand and surf. She applies the sunblock carefully and thoroughly, because she knows that", "arg_1": " sunlight will be fun"}, "gen_args_2": {"arg_0": "A woman, with a pale complexion, wants to spend the bright, sunny day at the beach. She makes sure that she stops at the store to pick up some sunblock before she begins to enjoy her day filled with sand and surf. She applies the sunblock carefully and thoroughly, because she knows that", "arg_1": " the sun is close"}, "gen_args_3": {"arg_0": "A woman, with a pale complexion, wants to spend the bright, sunny day at the beach. She makes sure that she stops at the store to pick up some sunblock before she begins to enjoy her day filled with sand and surf. She applies the sunblock carefully and thoroughly, because she knows that", "arg_1": " the sun is in space"}}, "resps": [[["-10.375", "False"]], [["-21.875", "False"]], [["-12.625", "False"]], [["-25.125", "False"]]], "filtered_resps": [["-10.375", "False"], ["-21.875", "False"], ["-12.625", "False"], ["-25.125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "913a532e140929c4d0ccecdaba6139da588ad65941c04bd7cfc07b3eacd35e14", "prompt_hash": "56c8e94e50205ea2e6d6eebb9a5fcaaac96b081ffb3e7fecdaadc92efe86d3b1", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 1.0, "acc_norm": 1.0}
{"doc_id": 495, "doc": {"id": "9-284", "question_stem": "A person is heating water in order to cook pasta. He spills the pot of water on his leg and finds that the water", "choices": {"text": ["scalds", "cools", "toasts", "freezes"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}, "target": "0", "arguments": {"gen_args_0": {"arg_0": "A person is heating water in order to cook pasta. He spills the pot of water on his leg and finds that the water", "arg_1": " scalds"}, "gen_args_1": {"arg_0": "A person is heating water in order to cook pasta. He spills the pot of water on his leg and finds that the water", "arg_1": " cools"}, "gen_args_2": {"arg_0": "A person is heating water in order to cook pasta. He spills the pot of water on his leg and finds that the water", "arg_1": " toasts"}, "gen_args_3": {"arg_0": "A person is heating water in order to cook pasta. He spills the pot of water on his leg and finds that the water", "arg_1": " freezes"}}, "resps": [[["-11.3125", "False"]], [["-6.6875", "False"]], [["-14.5625", "False"]], [["-7.9375", "False"]]], "filtered_resps": [["-11.3125", "False"], ["-6.6875", "False"], ["-14.5625", "False"], ["-7.9375", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "287e28d81e1c0839d071c70338dd7ec2f1c01f160556b3e6a146a9611429067e", "prompt_hash": "3d0bb71f6f9d810c37d93dbda5ff51c20a68be8ad11857491783d1b2a8d6a242", "target_hash": "5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 496, "doc": {"id": "7-1186", "question_stem": "Pasta may be cooked in water when", "choices": {"text": ["the water is warm", "the water is on the stove", "water is bubbling from applied warmth", "the pasta is very fresh"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Pasta may be cooked in water when", "arg_1": " the water is warm"}, "gen_args_1": {"arg_0": "Pasta may be cooked in water when", "arg_1": " the water is on the stove"}, "gen_args_2": {"arg_0": "Pasta may be cooked in water when", "arg_1": " water is bubbling from applied warmth"}, "gen_args_3": {"arg_0": "Pasta may be cooked in water when", "arg_1": " the pasta is very fresh"}}, "resps": [[["-9.0", "False"]], [["-15.1875", "False"]], [["-46.75", "False"]], [["-9.5625", "False"]]], "filtered_resps": [["-9.0", "False"], ["-15.1875", "False"], ["-46.75", "False"], ["-9.5625", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "d9a8b9628587e07fff4bd33caf330f76ad443d4df6bda1442812cb2039177ff6", "prompt_hash": "8920761350f9359ff5197dd457ae9ef0fed0745be83cb3803ff51265810e29e3", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 497, "doc": {"id": "926", "question_stem": "A decrease in diseases", "choices": {"text": ["has no impact on a population", "leads to more sick people", "leads to less sick people", "leads to an uptick in emergency room visits"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "A decrease in diseases", "arg_1": " has no impact on a population"}, "gen_args_1": {"arg_0": "A decrease in diseases", "arg_1": " leads to more sick people"}, "gen_args_2": {"arg_0": "A decrease in diseases", "arg_1": " leads to less sick people"}, "gen_args_3": {"arg_0": "A decrease in diseases", "arg_1": " leads to an uptick in emergency room visits"}}, "resps": [[["-21.125", "False"]], [["-21.375", "False"]], [["-22.5", "False"]], [["-26.75", "False"]]], "filtered_resps": [["-21.125", "False"], ["-21.375", "False"], ["-22.5", "False"], ["-26.75", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "6915c85a8fe376d02837166941648f4ef65fc7ed0348d91c92a35f9e73edfcb4", "prompt_hash": "a2a2ce4fc5e7db7f0b67aae68f8c5c511d0880a895bcea8afc5f72644926b748", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 498, "doc": {"id": "7-519", "question_stem": "When soil is viewed in a scientific way, what is seen and viewed is actually", "choices": {"text": ["insects like big beetles", "tiny lifeforms in dirt", "small mammals living there", "a lot of tiny pebbles"], "label": ["A", "B", "C", "D"]}, "answerKey": "B"}, "target": "1", "arguments": {"gen_args_0": {"arg_0": "When soil is viewed in a scientific way, what is seen and viewed is actually", "arg_1": " insects like big beetles"}, "gen_args_1": {"arg_0": "When soil is viewed in a scientific way, what is seen and viewed is actually", "arg_1": " tiny lifeforms in dirt"}, "gen_args_2": {"arg_0": "When soil is viewed in a scientific way, what is seen and viewed is actually", "arg_1": " small mammals living there"}, "gen_args_3": {"arg_0": "When soil is viewed in a scientific way, what is seen and viewed is actually", "arg_1": " a lot of tiny pebbles"}}, "resps": [[["-29.875", "False"]], [["-36.5", "False"]], [["-34.0", "False"]], [["-17.25", "False"]]], "filtered_resps": [["-29.875", "False"], ["-36.5", "False"], ["-34.0", "False"], ["-17.25", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "c3b1534801626ebc9fb03b75abde3038cbb673a70f1c3926f5302c8ee2e39f66", "prompt_hash": "3724492406bd35f6206b66fcf8249d95146066989a310d4a49e407372886a2a1", "target_hash": "6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b", "acc": 0.0, "acc_norm": 0.0}
{"doc_id": 499, "doc": {"id": "7-7", "question_stem": "Some animals use a liquid coming from their skin to adjust to", "choices": {"text": ["cold", "water", "heat", "humidity"], "label": ["A", "B", "C", "D"]}, "answerKey": "C"}, "target": "2", "arguments": {"gen_args_0": {"arg_0": "Some animals use a liquid coming from their skin to adjust to", "arg_1": " cold"}, "gen_args_1": {"arg_0": "Some animals use a liquid coming from their skin to adjust to", "arg_1": " water"}, "gen_args_2": {"arg_0": "Some animals use a liquid coming from their skin to adjust to", "arg_1": " heat"}, "gen_args_3": {"arg_0": "Some animals use a liquid coming from their skin to adjust to", "arg_1": " humidity"}}, "resps": [[["-5.28125", "False"]], [["-3.09375", "False"]], [["-5.21875", "False"]], [["-5.78125", "False"]]], "filtered_resps": [["-5.28125", "False"], ["-3.09375", "False"], ["-5.21875", "False"], ["-5.78125", "False"]], "filter": "none", "metrics": ["acc", "acc_norm"], "doc_hash": "dc84cadd9e3b95e801f09943ec02b24041e2e6617c35c98cf89c4a3948894269", "prompt_hash": "78200e062f207d770dec1695e9ba2c5eb35dbb458d59e706a6a8e76d7a14b21d", "target_hash": "d4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35", "acc": 0.0, "acc_norm": 0.0}
